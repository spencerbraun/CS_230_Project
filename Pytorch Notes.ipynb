{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import open\n",
    "import glob\n",
    "\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch General - Defining Model and Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full training\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing \n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model \n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFiles(path): \n",
    "    return glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Spanish.txt', 'data/names/German.txt', 'data/names/Polish.txt', 'data/names/Russian.txt', 'data/names/Chinese.txt', 'data/names/Portuguese.txt', 'data/names/Japanese.txt', 'data/names/French.txt', 'data/names/English.txt', 'data/names/Korean.txt', 'data/names/Irish.txt', 'data/names/Arabic.txt', 'data/names/Vietnamese.txt', 'data/names/Dutch.txt', 'data/names/Italian.txt', 'data/names/Scottish.txt', 'data/names/Czech.txt', 'data/names/Greek.txt']\n"
     ]
    }
   ],
   "source": [
    "print(findFiles('data/names/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0191, -2.8241, -3.0121, -2.9176, -2.8639, -2.9306, -2.9030, -2.8033,\n",
      "         -2.9599, -2.9411, -2.8120, -2.8065, -2.8642, -2.8686, -2.8052, -2.9574,\n",
      "         -2.9036, -2.8744]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "     2 linear layers which operate on an input and \n",
    "     hidden state, with a LogSoftmax layer after the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "input = lineToTensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('French', 7)\n",
      "category = Russian / line = Elachich\n",
      "category = Scottish / line = Whyte\n",
      "category = Russian / line = Haritonov\n",
      "category = Polish / line = Chlebek\n",
      "category = Spanish / line = Belmonte\n",
      "category = Spanish / line = Garcia\n",
      "category = Vietnamese / line = Ma\n",
      "category = English / line = Jaffray\n",
      "category = Portuguese / line = Costa\n",
      "category = Greek / line = Sklavenitis\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))\n",
    "\n",
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2ec1946169c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running on user input \n",
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Text Generation - Character Level RNN\n",
    "This network extends the last tutorial’s RNN with an extra argument for the category tensor, which is concatenated along with the others. The category tensor is a one-hot vector just like the letter input.\n",
    "\n",
    "We will interpret the output as the probability of the next letter. When sampling, the most likely output letter is used as the next input letter.\n",
    "\n",
    "I added a second linear layer o2o (after combining hidden and output) to give it more muscle to work with. There’s also a dropout layer, which randomly zeros parts of its input with a given probability (here 0.1) and is usually used to fuzz inputs to prevent overfitting. Here we’re using it towards the end of the network to purposely add some chaos and increase sampling variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, input, hidden):\n",
    "        input_combined = torch.cat((category, input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random category and random line from that category\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot vector for category\n",
    "#The category tensor is a one-hot tensor of size <1 x n_categories>. \n",
    "#When training we feed it to the network at every timestep - \n",
    "#this is a design choice, it could have been \n",
    "#included as part of initial hidden state or some other strategy.\n",
    "def categoryTensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make category, input, and target tensors from a random category, line pair\n",
    "def randomTrainingExample():\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = inputTensor(line)\n",
    "    target_line_tensor = targetTensor(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Network\n",
    "\n",
    "In contrast to classification, where only the last output is used, we are making a prediction at every step, so we are calculating loss at every step.\n",
    "\n",
    "The magic of autograd allows you to simply sum these losses at each step and call backward at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
    "    target_line_tensor.unsqueeze_(-1)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "        l = criterion(output, target_line_tensor[i])\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item() / input_line_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    output, loss = train(*randomTrainingExample())\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categoryTensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-36b6e5d9313c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_letter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0msamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Russian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RUS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'German'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GER'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-36b6e5d9313c>\u001b[0m in \u001b[0;36msamples\u001b[0;34m(category, start_letters)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_letters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ABC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstart_letter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstart_letters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_letter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Russian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RUS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-36b6e5d9313c>\u001b[0m in \u001b[0;36msample\u001b[0;34m(category, start_letter)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_letter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# no need to track history in sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mcategory_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategoryTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_letter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categoryTensor' is not defined"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "#Rather than having to give it a starting letter, \n",
    "#another strategy would have been to include a “start of string” \n",
    "#token in training and have the network choose its own starting letter.\n",
    "def sample(category, start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter))\n",
    "\n",
    "samples('Russian', 'RUS')\n",
    "\n",
    "samples('German', 'GER')\n",
    "\n",
    "samples('Spanish', 'SPA')\n",
    "\n",
    "samples('Chinese', 'CHI')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation with Seq 2 Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10662 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4435\n",
      "eng 2830\n",
      "['vous etes extravertie .', 'you re extroverted .']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'j': 2,\n",
       " 'ai': 3,\n",
       " 'ans': 4,\n",
       " '.': 5,\n",
       " 'je': 6,\n",
       " 'vais': 7,\n",
       " 'bien': 8,\n",
       " 'ca': 9,\n",
       " 'va': 10,\n",
       " 'suis': 11,\n",
       " 'gras': 12,\n",
       " 'gros': 13,\n",
       " 'en': 14,\n",
       " 'forme': 15,\n",
       " 'touche': 16,\n",
       " '': 17,\n",
       " 'touchee': 18,\n",
       " 'malade': 19,\n",
       " 'triste': 20,\n",
       " 'timide': 21,\n",
       " 'mouille': 22,\n",
       " 'mouillee': 23,\n",
       " 'il': 24,\n",
       " 'est': 25,\n",
       " 'revenu': 26,\n",
       " 'me': 27,\n",
       " 'revoila': 28,\n",
       " 'chauve': 29,\n",
       " 'occupe': 30,\n",
       " 'occupee': 31,\n",
       " 'calme': 32,\n",
       " 'froid': 33,\n",
       " 'fini': 34,\n",
       " 'tout': 35,\n",
       " 'libre': 36,\n",
       " 'disponible': 37,\n",
       " 'repu': 38,\n",
       " 'rassasie': 39,\n",
       " 'content': 40,\n",
       " 'chez': 41,\n",
       " 'moi': 42,\n",
       " 'retard': 43,\n",
       " 'paresseux': 44,\n",
       " 'faineant': 45,\n",
       " 'paresseuse': 46,\n",
       " 'faineante': 47,\n",
       " 'porte': 48,\n",
       " 'securite': 49,\n",
       " 'certain': 50,\n",
       " 'sur': 51,\n",
       " 'sure': 52,\n",
       " 'grande': 53,\n",
       " 'mince': 54,\n",
       " 'ordonne': 55,\n",
       " 'ordonnee': 56,\n",
       " 'laid': 57,\n",
       " 'laide': 58,\n",
       " 'faible': 59,\n",
       " 'vieux': 60,\n",
       " 'dj': 61,\n",
       " 'bon': 62,\n",
       " 'riche': 63,\n",
       " 'ici': 64,\n",
       " 'flic': 65,\n",
       " 'un': 66,\n",
       " 'homme': 67,\n",
       " 'seule': 68,\n",
       " 'seul': 69,\n",
       " 'arme': 70,\n",
       " 'armee': 71,\n",
       " 'reveille': 72,\n",
       " 'aveugle': 73,\n",
       " 'fauche': 74,\n",
       " 'fou': 75,\n",
       " 'folle': 76,\n",
       " 'gueri': 77,\n",
       " 'guerie': 78,\n",
       " 'saoul': 79,\n",
       " 'soul': 80,\n",
       " 'ivre': 81,\n",
       " 'meurs': 82,\n",
       " 'avance': 83,\n",
       " 'premier': 84,\n",
       " 'difficile': 85,\n",
       " 'tatillon': 86,\n",
       " 'tatillonne': 87,\n",
       " 'pars': 88,\n",
       " 'maintenant': 89,\n",
       " 'tire': 90,\n",
       " 'jy': 91,\n",
       " 'loyal': 92,\n",
       " 'loyale': 93,\n",
       " 'veinard': 94,\n",
       " 'veinarde': 95,\n",
       " 'du': 96,\n",
       " 'pot': 97,\n",
       " 'chanceux': 98,\n",
       " 'chanceuse': 99,\n",
       " 'train': 100,\n",
       " 'de': 101,\n",
       " 'mentir': 102,\n",
       " 'tranquille': 103,\n",
       " 'prete': 104,\n",
       " 'pret': 105,\n",
       " 'raison': 106,\n",
       " 'sobre': 107,\n",
       " 'excusemoi': 108,\n",
       " 'desole': 109,\n",
       " 'desolee': 110,\n",
       " 'coincee': 111,\n",
       " 'fatigue': 112,\n",
       " 'dur': 113,\n",
       " 'dure': 114,\n",
       " 'a': 115,\n",
       " 'cuire': 116,\n",
       " 'toi': 117,\n",
       " 'vous': 118,\n",
       " 'elle': 119,\n",
       " 'chaude': 120,\n",
       " 'tres': 121,\n",
       " 'attirante': 122,\n",
       " 'nous': 123,\n",
       " 'avons': 124,\n",
       " 'chaud': 125,\n",
       " 'sommes': 126,\n",
       " 'tristes': 127,\n",
       " 'timides': 128,\n",
       " 'faire': 129,\n",
       " 'gentil': 130,\n",
       " 'pauvre': 131,\n",
       " 'suisse': 132,\n",
       " 'helvete': 133,\n",
       " 'ruine': 134,\n",
       " 'intelligent': 135,\n",
       " 'humain': 136,\n",
       " 'creve': 137,\n",
       " 'francais': 138,\n",
       " 'coreen': 139,\n",
       " 'heros': 140,\n",
       " 'menteur': 141,\n",
       " 'cuis': 142,\n",
       " 'mieux': 143,\n",
       " 'paie': 144,\n",
       " 'c': 145,\n",
       " 'qui': 146,\n",
       " 'grassouillet': 147,\n",
       " 'grassouillette': 148,\n",
       " 'mange': 149,\n",
       " 'connu': 150,\n",
       " 'connue': 151,\n",
       " 'plus': 152,\n",
       " 'rapide': 153,\n",
       " 'flasque': 154,\n",
       " 'cupide': 155,\n",
       " 'gourmand': 156,\n",
       " 'gourmande': 157,\n",
       " 'cache': 158,\n",
       " 'honnete': 159,\n",
       " 'humble': 160,\n",
       " 'faim': 161,\n",
       " 'immunise': 162,\n",
       " 'immunisee': 163,\n",
       " 'au': 164,\n",
       " 'lit': 165,\n",
       " 'alite': 166,\n",
       " 'alitee': 167,\n",
       " 'rigole': 168,\n",
       " 'sens': 169,\n",
       " 'perdre': 170,\n",
       " 'demenager': 171,\n",
       " 'normal': 172,\n",
       " 'normale': 173,\n",
       " 'payer': 174,\n",
       " 'repose': 175,\n",
       " 'reposee': 176,\n",
       " 'ruinee': 177,\n",
       " 'remue': 178,\n",
       " 'remuee': 179,\n",
       " 'celibataire': 180,\n",
       " 'maigrichon': 181,\n",
       " 'maigrichonne': 182,\n",
       " 'sommeil': 183,\n",
       " 'sournois': 184,\n",
       " 'sournoise': 185,\n",
       " 'strict': 186,\n",
       " 'stricte': 187,\n",
       " 'fort': 188,\n",
       " 'forte': 189,\n",
       " 'trente': 190,\n",
       " 'affaiblie': 191,\n",
       " 'vieille': 192,\n",
       " 'gentille': 193,\n",
       " 'des': 194,\n",
       " 'hommes': 195,\n",
       " 'retour': 196,\n",
       " 'occupes': 197,\n",
       " 'occupees': 198,\n",
       " 'termine': 199,\n",
       " 'quittes': 200,\n",
       " 'allons': 201,\n",
       " 'la': 202,\n",
       " 'perdus': 203,\n",
       " 'perdues': 204,\n",
       " 'riches': 205,\n",
       " 'on': 206,\n",
       " 'foutu': 207,\n",
       " 'foutus': 208,\n",
       " 'faibles': 209,\n",
       " 'tu': 210,\n",
       " 'es': 211,\n",
       " 'vilain': 212,\n",
       " 'grand': 213,\n",
       " 'etes': 214,\n",
       " 'grands': 215,\n",
       " 'grandes': 216,\n",
       " 't': 217,\n",
       " 'marrante': 218,\n",
       " 'marrant': 219,\n",
       " 'vieilles': 220,\n",
       " 'huit': 221,\n",
       " 'hai': 222,\n",
       " 'mechant': 223,\n",
       " 'jeune': 224,\n",
       " 'beau': 225,\n",
       " 'mec': 226,\n",
       " 'gosse': 227,\n",
       " 'type': 228,\n",
       " 'binoclard': 229,\n",
       " 'flemmard': 230,\n",
       " 'endormi': 231,\n",
       " 'arrive': 232,\n",
       " 'd': 233,\n",
       " 'arriver': 234,\n",
       " 'pleurer': 235,\n",
       " 'simule': 236,\n",
       " 'blinde': 237,\n",
       " 'pete': 238,\n",
       " 'thune': 239,\n",
       " 'plein': 240,\n",
       " 'aux': 241,\n",
       " 'as': 242,\n",
       " 'mon': 243,\n",
       " 'age': 244,\n",
       " 'n': 245,\n",
       " 'pas': 246,\n",
       " 'lui': 247,\n",
       " 'l': 248,\n",
       " 'interieur': 249,\n",
       " 'cuisinier': 250,\n",
       " 'moine': 251,\n",
       " 'plaisante': 252,\n",
       " 'egalement': 253,\n",
       " 'dixsept': 254,\n",
       " 'finlandais': 255,\n",
       " 'finlandaise': 256,\n",
       " 'italien': 257,\n",
       " 'boulanger': 258,\n",
       " 'boulangere': 259,\n",
       " 'fait': 260,\n",
       " 'honte': 261,\n",
       " 'maison': 262,\n",
       " 'dans': 263,\n",
       " 'perplexe': 264,\n",
       " 'beni': 265,\n",
       " 'benie': 266,\n",
       " 'prudent': 267,\n",
       " 'prudente': 268,\n",
       " 'les': 269,\n",
       " 'foies': 270,\n",
       " 'chocottes': 271,\n",
       " 'curieux': 272,\n",
       " 'curieuse': 273,\n",
       " 'danser': 274,\n",
       " 'regime': 275,\n",
       " 'conduire': 276,\n",
       " 'conduis': 277,\n",
       " 'fiance': 278,\n",
       " 'fiancee': 279,\n",
       " 'excite': 280,\n",
       " 'excitee': 281,\n",
       " 'fais': 282,\n",
       " 'diete': 283,\n",
       " 'meticuleux': 284,\n",
       " 'meticuleuse': 285,\n",
       " 'affole': 286,\n",
       " 'affolee': 287,\n",
       " 'furieux': 288,\n",
       " 'bonne': 289,\n",
       " 'sante': 290,\n",
       " 'fredonne': 291,\n",
       " 'jalouse': 292,\n",
       " 'frousse': 293,\n",
       " 'marie': 294,\n",
       " 'mariee': 295,\n",
       " 'ne': 296,\n",
       " 'une': 297,\n",
       " 'idiote': 298,\n",
       " 'service': 299,\n",
       " 'patiente': 300,\n",
       " 'patient': 301,\n",
       " 'populaire': 302,\n",
       " 'remonte': 303,\n",
       " 'voyant': 304,\n",
       " 'voyante': 305,\n",
       " 'lis': 306,\n",
       " 'detendu': 307,\n",
       " 'detendue': 308,\n",
       " 'retraite': 309,\n",
       " 'retraitee': 310,\n",
       " 'pensionne': 311,\n",
       " 'pensionnee': 312,\n",
       " 'egoiste': 313,\n",
       " 'serieux': 314,\n",
       " 'choque': 315,\n",
       " 'choquee': 316,\n",
       " 'sincere': 317,\n",
       " 'bourre': 318,\n",
       " 'bourree': 319,\n",
       " 'tellement': 320,\n",
       " 'estomac': 321,\n",
       " 'talons': 322,\n",
       " 'dalle': 323,\n",
       " 'crocs': 324,\n",
       " 'reste': 325,\n",
       " 'gave': 326,\n",
       " 'gavee': 327,\n",
       " 'sidere': 328,\n",
       " 'sideree': 329,\n",
       " 'discuter': 330,\n",
       " 'taquine': 331,\n",
       " 'soif': 332,\n",
       " 'trop': 333,\n",
       " 'mecontent': 334,\n",
       " 'mecontente': 335,\n",
       " 'schcoumoune': 336,\n",
       " 'fortune': 337,\n",
       " 'fortunee': 338,\n",
       " 'gagne': 339,\n",
       " 'emporte': 340,\n",
       " 'travailler': 341,\n",
       " 'souci': 342,\n",
       " 'charme': 343,\n",
       " 'morte': 344,\n",
       " 'enfoiree': 345,\n",
       " 'renard': 346,\n",
       " 'ils': 347,\n",
       " 'sont': 348,\n",
       " 'mauvais': 349,\n",
       " 'elles': 350,\n",
       " 'mauvaises': 351,\n",
       " 'egalite': 352,\n",
       " 'y': 353,\n",
       " 'seuls': 354,\n",
       " 'seules': 355,\n",
       " 'colere': 356,\n",
       " 'armes': 357,\n",
       " 'armees': 358,\n",
       " 'ennuyons': 359,\n",
       " 's': 360,\n",
       " 'emmerde': 361,\n",
       " 'fauches': 362,\n",
       " 'fauchees': 363,\n",
       " 'mourir': 364,\n",
       " 'heureux': 365,\n",
       " 'pretes': 366,\n",
       " 'sauves': 367,\n",
       " 'sauvees': 368,\n",
       " 'intelligents': 369,\n",
       " 'intelligentes': 370,\n",
       " 'desoles': 371,\n",
       " 'desolees': 372,\n",
       " 'coinces': 373,\n",
       " 'coincees': 374,\n",
       " 'fatigues': 375,\n",
       " 'fatiguees': 376,\n",
       " 'jumeaux': 377,\n",
       " 'jumelles': 378,\n",
       " 'sympa': 379,\n",
       " 'decontracte': 380,\n",
       " 'juste': 381,\n",
       " 'justes': 382,\n",
       " 'vas': 383,\n",
       " 'libres': 384,\n",
       " 'bons': 385,\n",
       " 'bonnes': 386,\n",
       " 'paresseuses': 387,\n",
       " 'perdu': 388,\n",
       " 'perdue': 389,\n",
       " 'sympas': 390,\n",
       " 'dingue': 391,\n",
       " 'dingues': 392,\n",
       " 'givre': 393,\n",
       " 'givree': 394,\n",
       " 'givres': 395,\n",
       " 'givrees': 396,\n",
       " 'grossier': 397,\n",
       " 'grossiers': 398,\n",
       " 'grossiere': 399,\n",
       " 'grossieres': 400,\n",
       " 'sauf': 401,\n",
       " 'saufs': 402,\n",
       " 'sauve': 403,\n",
       " 'minces': 404,\n",
       " 'poete': 405,\n",
       " 'excentrique': 406,\n",
       " 'manger': 407,\n",
       " 'heroique': 408,\n",
       " 'anglais': 409,\n",
       " 'bigot': 410,\n",
       " 'sectaire': 411,\n",
       " 'fanatique': 412,\n",
       " 'illumine': 413,\n",
       " 'souffre': 414,\n",
       " 'actuellement': 415,\n",
       " 'exterieur': 416,\n",
       " 'si': 417,\n",
       " 'mignon': 418,\n",
       " 'couard': 419,\n",
       " 'lache': 420,\n",
       " 'medecin': 421,\n",
       " 'toubib': 422,\n",
       " 'fermier': 423,\n",
       " 'agriculteur': 424,\n",
       " 'travaille': 425,\n",
       " 'ma': 426,\n",
       " 'ferme': 427,\n",
       " 'puriste': 428,\n",
       " 'drogue': 429,\n",
       " 'accro': 430,\n",
       " 'assuetude': 431,\n",
       " 'ouie': 432,\n",
       " 'adulte': 433,\n",
       " 'agent': 434,\n",
       " 'saigner': 435,\n",
       " 'melange': 436,\n",
       " 'pinceaux': 437,\n",
       " 'creatif': 438,\n",
       " 'creative': 439,\n",
       " 'cultive': 440,\n",
       " 'cultivee': 441,\n",
       " 'divorce': 442,\n",
       " 'divorcee': 443,\n",
       " 'noyer': 444,\n",
       " 'dixhuit': 445,\n",
       " 'fidele': 446,\n",
       " 'affame': 447,\n",
       " 'intrepide': 448,\n",
       " 'bats': 449,\n",
       " 'gele': 450,\n",
       " 'cloue': 451,\n",
       " 'clouee': 452,\n",
       " 'credule': 453,\n",
       " 'le': 454,\n",
       " 'mal': 455,\n",
       " 'pays': 456,\n",
       " 'gueule': 457,\n",
       " 'bois': 458,\n",
       " 'paris': 459,\n",
       " 'innocent': 460,\n",
       " 'innocente': 461,\n",
       " 'ingenu': 462,\n",
       " 'ingenue': 463,\n",
       " 'implique': 464,\n",
       " 'impliquee': 465,\n",
       " 'm': 466,\n",
       " 'sors': 467,\n",
       " 'nouveau': 468,\n",
       " 'rebelle': 469,\n",
       " 'saint': 470,\n",
       " 'sainte': 471,\n",
       " 'sourd': 472,\n",
       " 'sourde': 473,\n",
       " 'abruti': 474,\n",
       " 'abrutie': 475,\n",
       " 'blesse': 476,\n",
       " 'blessee': 477,\n",
       " 'mechante': 478,\n",
       " 'certaine': 479,\n",
       " 'offense': 480,\n",
       " 'offensee': 481,\n",
       " 'indigne': 482,\n",
       " 'indignee': 483,\n",
       " 'puissant': 484,\n",
       " 'puissante': 485,\n",
       " 'preparee': 486,\n",
       " 'prepare': 487,\n",
       " 'ponctuel': 488,\n",
       " 'ponctuelle': 489,\n",
       " 'rationnel': 490,\n",
       " 'rationnelle': 491,\n",
       " 'amende': 492,\n",
       " 'amendee': 493,\n",
       " 'fiable': 494,\n",
       " 'tiens': 495,\n",
       " 'place': 496,\n",
       " 'impitoyable': 497,\n",
       " 'dormir': 498,\n",
       " 'las': 499,\n",
       " 'parler': 500,\n",
       " 'fringale': 501,\n",
       " 'tetu': 502,\n",
       " 'tetue': 503,\n",
       " 'obstine': 504,\n",
       " 'obstinee': 505,\n",
       " 'patron': 506,\n",
       " 'patronne': 507,\n",
       " 'reflechis': 508,\n",
       " 'minutieux': 509,\n",
       " 'minutieuse': 510,\n",
       " 'consciencieux': 511,\n",
       " 'consciencieuse': 512,\n",
       " 'ravi': 513,\n",
       " 'ravie': 514,\n",
       " 'chatouilleux': 515,\n",
       " 'chatouilleuse': 516,\n",
       " 'affaire': 517,\n",
       " 'affairee': 518,\n",
       " 'dis': 519,\n",
       " 'verite': 520,\n",
       " 'impartial': 521,\n",
       " 'impartiale': 522,\n",
       " 'audessus': 523,\n",
       " 'epuise': 524,\n",
       " 'chancarde': 525,\n",
       " 'tranchante': 526,\n",
       " 'affutee': 527,\n",
       " 'tort': 528,\n",
       " 'ce': 529,\n",
       " 'garcons': 530,\n",
       " 'froids': 531,\n",
       " 'froides': 532,\n",
       " 'flics': 533,\n",
       " 'mignons': 534,\n",
       " 'mignonnes': 535,\n",
       " 'decedes': 536,\n",
       " 'decedees': 537,\n",
       " 'ont': 538,\n",
       " 'partis': 539,\n",
       " 'parties': 540,\n",
       " 'miens': 541,\n",
       " 'miennes': 542,\n",
       " 'arabes': 543,\n",
       " 'equipe': 544,\n",
       " 'adultes': 545,\n",
       " 'tous': 546,\n",
       " 'guerre': 547,\n",
       " 'notre': 548,\n",
       " 'point': 549,\n",
       " 'vue': 550,\n",
       " 'biaise': 551,\n",
       " 'prejuges': 552,\n",
       " 'achetons': 553,\n",
       " 'fermes': 554,\n",
       " 'venons': 555,\n",
       " 'sortons': 556,\n",
       " 'ensemble': 557,\n",
       " 'condamnes': 558,\n",
       " 'cachons': 559,\n",
       " 'plaisantons': 560,\n",
       " 'perdons': 561,\n",
       " 'bouger': 562,\n",
       " 'normaux': 563,\n",
       " 'normales': 564,\n",
       " 'creves': 565,\n",
       " 'crevees': 566,\n",
       " 'ruines': 567,\n",
       " 'ruinees': 568,\n",
       " 'remues': 569,\n",
       " 'remuees': 570,\n",
       " 'sournoises': 571,\n",
       " 'forts': 572,\n",
       " 'fortes': 573,\n",
       " 'essayons': 574,\n",
       " 'joues': 575,\n",
       " 'chef': 576,\n",
       " 'cruelle': 577,\n",
       " 'cruel': 578,\n",
       " 'cruelles': 579,\n",
       " 'cruels': 580,\n",
       " 'viens': 581,\n",
       " 'tot': 582,\n",
       " 'matinal': 583,\n",
       " 'matinale': 584,\n",
       " 'vire': 585,\n",
       " 'licencie': 586,\n",
       " 'passes': 587,\n",
       " 'marrants': 588,\n",
       " 'marrantes': 589,\n",
       " 'manieres': 590,\n",
       " 'faites': 591,\n",
       " 'malpoli': 592,\n",
       " 'mens': 593,\n",
       " 'mentez': 594,\n",
       " 'lunatique': 595,\n",
       " 'naif': 596,\n",
       " 'naive': 597,\n",
       " 'naifs': 598,\n",
       " 'naives': 599,\n",
       " 'idiot': 600,\n",
       " 'idiots': 601,\n",
       " 'idiotes': 602,\n",
       " 'plante': 603,\n",
       " 'plantee': 604,\n",
       " 'plantes': 605,\n",
       " 'plantees': 606,\n",
       " 'durs': 607,\n",
       " 'dures': 608,\n",
       " 'contrariee': 609,\n",
       " 'contrarie': 610,\n",
       " 'bizarre': 611,\n",
       " 'bizarres': 612,\n",
       " 'erreur': 613,\n",
       " 'jeunes': 614,\n",
       " 'britannique': 615,\n",
       " 'voleur': 616,\n",
       " 'genre': 617,\n",
       " 'sorti': 618,\n",
       " 'lire': 619,\n",
       " 'court': 620,\n",
       " 'patin': 621,\n",
       " 'patine': 622,\n",
       " 'rouspeteur': 623,\n",
       " 'jesuite': 624,\n",
       " 'aine': 625,\n",
       " 'ancien': 626,\n",
       " 'magnat': 627,\n",
       " 'adorable': 628,\n",
       " 'derriere': 629,\n",
       " 'apres': 630,\n",
       " 'mes': 631,\n",
       " 'fesses': 632,\n",
       " 'poursuit': 633,\n",
       " 'embetant': 634,\n",
       " 'tokyo': 635,\n",
       " 'peu': 636,\n",
       " 'manque': 637,\n",
       " 'confiance': 638,\n",
       " 'sans': 639,\n",
       " 'pitie': 640,\n",
       " 'etudie': 641,\n",
       " 'etudier': 642,\n",
       " 'lent': 643,\n",
       " 'ton': 644,\n",
       " 'fils': 645,\n",
       " 'votre': 646,\n",
       " 'americain': 647,\n",
       " 'americaine': 648,\n",
       " 'japonais': 649,\n",
       " 'musulman': 650,\n",
       " 'musulmane': 651,\n",
       " 'coureur': 652,\n",
       " 'diabetique': 653,\n",
       " 'etudiant': 654,\n",
       " 'professeur': 655,\n",
       " 'enseignante': 656,\n",
       " 'adapte': 657,\n",
       " 'peur': 658,\n",
       " 'toute': 659,\n",
       " 'ambitieuse': 660,\n",
       " 'ambitieux': 661,\n",
       " 'artiste': 662,\n",
       " 'orphelin': 663,\n",
       " 'attentive': 664,\n",
       " 'attentif': 665,\n",
       " 'belle': 666,\n",
       " 'preoccupe': 667,\n",
       " 'preoccupee': 668,\n",
       " 'voila': 669,\n",
       " 'satisfait': 670,\n",
       " 'satisfaite': 671,\n",
       " 'convaincu': 672,\n",
       " 'deprime': 673,\n",
       " 'desespere': 674,\n",
       " 'desesperee': 675,\n",
       " 'differente': 676,\n",
       " 'degoute': 677,\n",
       " 'degoutee': 678,\n",
       " 'facile': 679,\n",
       " 'vivre': 680,\n",
       " 'epuisee': 681,\n",
       " 'vanne': 682,\n",
       " 'vannee': 683,\n",
       " 'fourbu': 684,\n",
       " 'crevee': 685,\n",
       " 'distrait': 686,\n",
       " 'distraite': 687,\n",
       " 'etourdi': 688,\n",
       " 'etourdie': 689,\n",
       " 'tom': 690,\n",
       " 'cur': 691,\n",
       " 'impatient': 692,\n",
       " 'impatiente': 693,\n",
       " 'important': 694,\n",
       " 'impressionnee': 695,\n",
       " 'impulsif': 696,\n",
       " 'impulsive': 697,\n",
       " 'boston': 698,\n",
       " 'danger': 699,\n",
       " 'intrigue': 700,\n",
       " 'intriguee': 701,\n",
       " 'cela': 702,\n",
       " 'ecoute': 703,\n",
       " 'motivee': 704,\n",
       " 'motive': 705,\n",
       " 'expert': 706,\n",
       " 'partie': 707,\n",
       " 'ses': 708,\n",
       " 'admirateurs': 709,\n",
       " 'leurs': 710,\n",
       " 'rends': 711,\n",
       " 'heureuse': 712,\n",
       " 'contente': 713,\n",
       " 'obese': 714,\n",
       " 'petite': 715,\n",
       " 'fatiguee': 716,\n",
       " 'observateur': 717,\n",
       " 'observatrice': 718,\n",
       " 'respectueux': 719,\n",
       " 'respectueuse': 720,\n",
       " 'chemin': 721,\n",
       " 'route': 722,\n",
       " 'desarme': 723,\n",
       " 'desarmee': 724,\n",
       " 'realiste': 725,\n",
       " 'eprouve': 726,\n",
       " 'ressentiment': 727,\n",
       " 'endurant': 728,\n",
       " 'endurante': 729,\n",
       " 'tenace': 730,\n",
       " 'non': 731,\n",
       " 'sensible': 732,\n",
       " 'surpris': 733,\n",
       " 'surprise': 734,\n",
       " 'survis': 735,\n",
       " 'terrifie': 736,\n",
       " 'terrifiee': 737,\n",
       " 'assure': 738,\n",
       " 'assuree': 739,\n",
       " 'vote': 740,\n",
       " 'francaise': 741,\n",
       " 'jumelle': 742,\n",
       " 'active': 743,\n",
       " 'pleure': 744,\n",
       " 'beaute': 745,\n",
       " 'mannequin': 746,\n",
       " 'super': 747,\n",
       " 'asiatiques': 748,\n",
       " 'vie': 749,\n",
       " 'eveilles': 750,\n",
       " 'eveillees': 751,\n",
       " 'courageux': 752,\n",
       " 'courageuses': 753,\n",
       " 'yeux': 754,\n",
       " 'propres': 755,\n",
       " 'fous': 756,\n",
       " 'folles': 757,\n",
       " 'saouls': 758,\n",
       " 'saoules': 759,\n",
       " 'heureuses': 760,\n",
       " 'mentent': 761,\n",
       " 'petits': 762,\n",
       " 'petites': 763,\n",
       " 'espions': 764,\n",
       " 'espionnes': 765,\n",
       " 'formons': 766,\n",
       " 'groupe': 767,\n",
       " 'anxieux': 768,\n",
       " 'perplexes': 769,\n",
       " 'prudents': 770,\n",
       " 'prudentes': 771,\n",
       " 'certains': 772,\n",
       " 'certaines': 773,\n",
       " 'cousines': 774,\n",
       " 'fiances': 775,\n",
       " 'faisons': 776,\n",
       " 'amis': 777,\n",
       " 'amies': 778,\n",
       " 'aimons': 779,\n",
       " 'amoureux': 780,\n",
       " 'amoureuses': 781,\n",
       " 'veinards': 782,\n",
       " 'veinardes': 783,\n",
       " 'souffrons': 784,\n",
       " 'synchronises': 785,\n",
       " 'synchronisees': 786,\n",
       " 'ville': 787,\n",
       " 'jalouses': 788,\n",
       " 'maries': 789,\n",
       " 'detendus': 790,\n",
       " 'detendues': 791,\n",
       " 'serieuses': 792,\n",
       " 'choques': 793,\n",
       " 'choquees': 794,\n",
       " 'sinceres': 795,\n",
       " 'coulons': 796,\n",
       " 'couler': 797,\n",
       " 'bourres': 798,\n",
       " 'bourrees': 799,\n",
       " 'speciaux': 800,\n",
       " 'arret': 801,\n",
       " 'affames': 802,\n",
       " 'affamees': 803,\n",
       " 'restons': 804,\n",
       " 'gaves': 805,\n",
       " 'gavees': 806,\n",
       " 'sideres': 807,\n",
       " 'siderees': 808,\n",
       " 'touches': 809,\n",
       " 'touchees': 810,\n",
       " 'pieges': 811,\n",
       " 'piegees': 812,\n",
       " 'satisfaits': 813,\n",
       " 'satisfaites': 814,\n",
       " 'malchanceux': 815,\n",
       " 'malchanceuses': 816,\n",
       " 'inutiles': 817,\n",
       " 'attendre': 818,\n",
       " 'gagneurs': 819,\n",
       " 'gagner': 820,\n",
       " 'malin': 821,\n",
       " 'avez': 822,\n",
       " 'amour': 823,\n",
       " 'menteuse': 824,\n",
       " 'snob': 825,\n",
       " 'partiale': 826,\n",
       " 'partial': 827,\n",
       " 'ennuyez': 828,\n",
       " 'ennuies': 829,\n",
       " 'brillant': 830,\n",
       " 'brillante': 831,\n",
       " 'brillants': 832,\n",
       " 'brillantes': 833,\n",
       " 'maline': 834,\n",
       " 'malins': 835,\n",
       " 'malines': 836,\n",
       " 'ruse': 837,\n",
       " 'rusee': 838,\n",
       " 'ruses': 839,\n",
       " 'rusees': 840,\n",
       " 'astucieux': 841,\n",
       " 'astucieuse': 842,\n",
       " 'sinistre': 843,\n",
       " 'sinistres': 844,\n",
       " 'connus': 845,\n",
       " 'connues': 846,\n",
       " 'avide': 847,\n",
       " 'avides': 848,\n",
       " 'bougon': 849,\n",
       " 'bougonne': 850,\n",
       " 'grognon': 851,\n",
       " 'plaisantes': 852,\n",
       " 'plaisantez': 853,\n",
       " 'fric': 854,\n",
       " 'pleine': 855,\n",
       " 'drole': 856,\n",
       " 'jolie': 857,\n",
       " 'maigrichonnes': 858,\n",
       " 'maigrichons': 859,\n",
       " 'endormie': 860,\n",
       " 'endormis': 861,\n",
       " 'endormies': 862,\n",
       " 'injuste': 863,\n",
       " 'canadien': 864,\n",
       " 'genie': 865,\n",
       " 'ecrivain': 866,\n",
       " 'acteur': 867,\n",
       " 'faillite': 868,\n",
       " 'oncle': 869,\n",
       " 'ouvert': 870,\n",
       " 'sociable': 871,\n",
       " 'extraverti': 872,\n",
       " 'quelque': 873,\n",
       " 'joueur': 874,\n",
       " 'desormais': 875,\n",
       " 'planque': 876,\n",
       " 'auteur': 877,\n",
       " 'detenu': 878,\n",
       " 'horslaloi': 879,\n",
       " 'harcele': 880,\n",
       " 'se': 881,\n",
       " 'trouve': 882,\n",
       " 'prison': 883,\n",
       " 'ami': 884,\n",
       " 'cest': 885,\n",
       " 'rend': 886,\n",
       " 'hongrois': 887,\n",
       " 'garcon': 888,\n",
       " 'touriste': 889,\n",
       " 'londres': 890,\n",
       " 'petrin': 891,\n",
       " 'endroit': 892,\n",
       " 'conge': 893,\n",
       " 'aujourd': 894,\n",
       " 'hui': 895,\n",
       " 'petit': 896,\n",
       " 'musicien': 897,\n",
       " 'musicienne': 898,\n",
       " 'quelqu': 899,\n",
       " 'vrai': 900,\n",
       " 'vendeur': 901,\n",
       " 'que': 902,\n",
       " 'crains': 903,\n",
       " 'contre': 904,\n",
       " 'maladroit': 905,\n",
       " 'vieil': 906,\n",
       " 'stupefait': 907,\n",
       " 'stupefaite': 908,\n",
       " 'utilise': 909,\n",
       " 'contagieux': 910,\n",
       " 'contagieuse': 911,\n",
       " 'mort': 912,\n",
       " 'deshydrate': 913,\n",
       " 'deshydratee': 914,\n",
       " 'peut': 915,\n",
       " 'compter': 916,\n",
       " 'aneanti': 917,\n",
       " 'aneantie': 918,\n",
       " 'bas': 919,\n",
       " 'entrainer': 920,\n",
       " 'vois': 921,\n",
       " 'loin': 922,\n",
       " 'fascine': 923,\n",
       " 'fascinee': 924,\n",
       " 'te': 925,\n",
       " 'kyoto': 926,\n",
       " 'retourne': 927,\n",
       " 'deviens': 928,\n",
       " 'aussi': 929,\n",
       " 'amuse': 930,\n",
       " 'analphabete': 931,\n",
       " 'voiture': 932,\n",
       " 'interesse': 933,\n",
       " 'interessee': 934,\n",
       " 'seulement': 935,\n",
       " 'simplement': 936,\n",
       " 'the': 937,\n",
       " 'mediter': 938,\n",
       " 'methodique': 939,\n",
       " 'baisse': 940,\n",
       " 'facilement': 941,\n",
       " 'bras': 942,\n",
       " 'abandonne': 943,\n",
       " 'amer': 944,\n",
       " 'amere': 945,\n",
       " 'grincheux': 946,\n",
       " 'grincheuse': 947,\n",
       " 'coupable': 948,\n",
       " 'blague': 949,\n",
       " 'raciste': 950,\n",
       " 'suffisamment': 951,\n",
       " 'vacances': 952,\n",
       " 'conges': 953,\n",
       " 'entre': 954,\n",
       " 'optimiste': 955,\n",
       " 'essence': 956,\n",
       " 'vraiment': 957,\n",
       " 'raisonnable': 958,\n",
       " 'reamenager': 959,\n",
       " 'quelle': 960,\n",
       " 'poisse': 961,\n",
       " 'toujours': 962,\n",
       " 'encore': 963,\n",
       " 'succes': 964,\n",
       " 'tueur': 965,\n",
       " 'deshabille': 966,\n",
       " 'devets': 967,\n",
       " 'chomage': 968,\n",
       " 'talent': 969,\n",
       " 'depourvu': 970,\n",
       " 'depourvue': 971,\n",
       " 'habitue': 972,\n",
       " 'accoutume': 973,\n",
       " 'habituee': 974,\n",
       " 'vegetarienne': 975,\n",
       " 'eveille': 976,\n",
       " 'eveillee': 977,\n",
       " 'infirmiere': 978,\n",
       " 'maladroite': 979,\n",
       " 'exprimer': 980,\n",
       " 'fille': 981,\n",
       " 'bombe': 982,\n",
       " 'canon': 983,\n",
       " 'ange': 984,\n",
       " 'enceinte': 985,\n",
       " 'attend': 986,\n",
       " 'evenement': 987,\n",
       " 'famille': 988,\n",
       " 'bruyante': 989,\n",
       " 'parle': 990,\n",
       " 'ennuient': 991,\n",
       " 'bebes': 992,\n",
       " 'ennuyeux': 993,\n",
       " 'ennuyeuses': 994,\n",
       " 'arrivent': 995,\n",
       " 'prets': 996,\n",
       " 'toutes': 997,\n",
       " 'couple': 998,\n",
       " 'completement': 999,\n",
       " 'freres': 1000,\n",
       " 'divorces': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state.\n",
    "\n",
    "“Teacher forcing” is the concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input. Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - intuitively it has learned to represent the output grammar and can “pick up” the meaning once the teacher tells it the first few words, but it has not properly learned how to create the sentence from the translation in the first place.\n",
    "\n",
    "Because of the freedom PyTorch’s autograd gives us, we can randomly choose to use teacher forcing or not with a simple if statement. Turn teacher_forcing_ratio up to use more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation is mostly the same as training, but there are no targets so we simply feed the decoder’s predictions back to itself for each step. Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. We also store the decoder’s attention outputs for display later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification with TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import text_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ag_news_csv.tar.gz: 11.8MB [00:00, 77.3MB/s]\n",
      "120000lines [00:09, 13160.90lines/s]\n",
      "120000lines [00:19, 6256.57lines/s]\n",
      "7600lines [00:01, 6347.36lines/s]\n"
     ]
    }
   ],
   "source": [
    "NGRAMS = 2\n",
    "if not os.path.isdir('./.data'):\n",
    "    os.mkdir('./.data')\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-ab47838d58a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0meval_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-ab47838d58a7>\u001b[0m in \u001b[0;36mbatchify\u001b[0;34m(data, bsz)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Evenly divide the data across the bsz batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36] *",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
