{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./chen_crossaligned\")\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import ipdb\n",
    "import random\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from vocab import Vocabulary, build_vocab\n",
    "from accumulator import Accumulator\n",
    "from options import load_arguments\n",
    "from file_io import load_sent, write_sent\n",
    "from utils import *\n",
    "from nn import *\n",
    "import beam_search, greedy_decoding\n",
    "# %load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "{   'batch_size': 64,\n",
      "    'beam': 1,\n",
      "    'dev': '',\n",
      "    'dim_emb': 100,\n",
      "    'dim_y': 200,\n",
      "    'dim_z': 500,\n",
      "    'dropout_keep_prob': 0.5,\n",
      "    'embedding': '',\n",
      "    'filter_sizes': '1,2,3,4,5',\n",
      "    'gamma_decay': 1,\n",
      "    'gamma_init': 0.1,\n",
      "    'gamma_min': 0.1,\n",
      "    'learning_rate': 0.0005,\n",
      "    'load_model': False,\n",
      "    'max_epochs': 20,\n",
      "    'max_seq_length': 20,\n",
      "    'max_train_size': -1,\n",
      "    'model': '',\n",
      "    'n_filters': 128,\n",
      "    'n_layers': 1,\n",
      "    'online_testing': False,\n",
      "    'output': '',\n",
      "    'rho': 1,\n",
      "    'steps_per_checkpoint': 1000,\n",
      "    'test': '',\n",
      "    'train': '',\n",
      "    'vocab': ''}\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "args = load_arguments([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, args, vocab):\n",
    "        dim_y = args.dim_y\n",
    "        dim_z = args.dim_z\n",
    "        dim_h = dim_y + dim_z\n",
    "        dim_emb = args.dim_emb\n",
    "        n_layers = args.n_layers\n",
    "        max_len = args.max_seq_length\n",
    "        filter_sizes = [int(x) for x in args.filter_sizes.split(',')]\n",
    "        n_filters = args.n_filters\n",
    "        beta1, beta2 = 0.5, 0.999\n",
    "        grad_clip = 30.0\n",
    "\n",
    "        self.dropout = tf.placeholder(tf.float32,\n",
    "            name='dropout')\n",
    "        self.learning_rate = tf.placeholder(tf.float32,\n",
    "            name='learning_rate')\n",
    "        self.rho = tf.placeholder(tf.float32,\n",
    "            name='rho')\n",
    "        self.gamma = tf.placeholder(tf.float32,\n",
    "            name='gamma')\n",
    "\n",
    "        self.batch_len = tf.placeholder(tf.int32,\n",
    "            name='batch_len')\n",
    "        self.batch_size = tf.placeholder(tf.int32,\n",
    "            name='batch_size')\n",
    "        self.enc_inputs = tf.placeholder(tf.int32, [None, None],    #size * len\n",
    "            name='enc_inputs')\n",
    "        self.dec_inputs = tf.placeholder(tf.int32, [None, None],\n",
    "            name='dec_inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, [None, None],\n",
    "            name='targets')\n",
    "        self.weights = tf.placeholder(tf.float32, [None, None],\n",
    "            name='weights')\n",
    "        self.labels = tf.placeholder(tf.float32, [None],\n",
    "            name='labels')\n",
    "\n",
    "        labels = tf.reshape(self.labels, [-1, 1])\n",
    "\n",
    "        embedding = tf.get_variable('embedding',\n",
    "            initializer=vocab.embedding.astype(np.float32))\n",
    "        with tf.variable_scope('projection'):\n",
    "            proj_W = tf.get_variable('W', [dim_h, vocab.size])\n",
    "            proj_b = tf.get_variable('b', [vocab.size])\n",
    "\n",
    "        enc_inputs = tf.nn.embedding_lookup(embedding, self.enc_inputs)\n",
    "        dec_inputs = tf.nn.embedding_lookup(embedding, self.dec_inputs)\n",
    "\n",
    "        #####   auto-encoder   #####\n",
    "        init_state = tf.concat([linear(labels, dim_y, scope='encoder'),\n",
    "            tf.zeros([self.batch_size, dim_z])], 1)\n",
    "        cell_e = create_cell(dim_h, n_layers, self.dropout)\n",
    "        _, z = tf.nn.dynamic_rnn(cell_e, enc_inputs,\n",
    "            initial_state=init_state, scope='encoder')\n",
    "        z = z[:, dim_y:]\n",
    "\n",
    "        #cell_e = create_cell(dim_z, n_layers, self.dropout)\n",
    "        #_, z = tf.nn.dynamic_rnn(cell_e, enc_inputs,\n",
    "        #    dtype=tf.float32, scope='encoder')\n",
    "\n",
    "        self.h_ori = tf.concat([linear(labels, dim_y,\n",
    "            scope='generator'), z], 1)\n",
    "        self.h_tsf = tf.concat([linear(1-labels, dim_y,\n",
    "            scope='generator', reuse=True), z], 1)\n",
    "\n",
    "        cell_g = create_cell(dim_h, n_layers, self.dropout)\n",
    "        g_outputs, _ = tf.nn.dynamic_rnn(cell_g, dec_inputs,\n",
    "            initial_state=self.h_ori, scope='generator')\n",
    "\n",
    "        # attach h0 in the front\n",
    "        teach_h = tf.concat([tf.expand_dims(self.h_ori, 1), g_outputs], 1)\n",
    "\n",
    "        g_outputs = tf.nn.dropout(g_outputs, self.dropout)\n",
    "        g_outputs = tf.reshape(g_outputs, [-1, dim_h])\n",
    "        g_logits = tf.matmul(g_outputs, proj_W) + proj_b\n",
    "\n",
    "        loss_rec = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.reshape(self.targets, [-1]), logits=g_logits)\n",
    "        loss_rec *= tf.reshape(self.weights, [-1])\n",
    "        self.loss_rec = tf.reduce_sum(loss_rec) / tf.to_float(self.batch_size)\n",
    "\n",
    "        #####   feed-previous decoding   #####\n",
    "        go = dec_inputs[:,0,:]\n",
    "        soft_func = softsample_word(self.dropout, proj_W, proj_b, embedding,\n",
    "            self.gamma)\n",
    "        hard_func = argmax_word(self.dropout, proj_W, proj_b, embedding)\n",
    "\n",
    "        soft_h_ori, soft_logits_ori = rnn_decode(self.h_ori, go, max_len,\n",
    "            cell_g, soft_func, scope='generator')\n",
    "        soft_h_tsf, soft_logits_tsf = rnn_decode(self.h_tsf, go, max_len,\n",
    "            cell_g, soft_func, scope='generator')\n",
    "\n",
    "        hard_h_ori, self.hard_logits_ori = rnn_decode(self.h_ori, go, max_len,\n",
    "            cell_g, hard_func, scope='generator')\n",
    "        hard_h_tsf, self.hard_logits_tsf = rnn_decode(self.h_tsf, go, max_len,\n",
    "            cell_g, hard_func, scope='generator')\n",
    "\n",
    "        #####   discriminator   #####\n",
    "        # a batch's first half consists of sentences of one style,\n",
    "        # and second half of the other\n",
    "        half = self.batch_size / 2\n",
    "        zeros, ones = self.labels[:half], self.labels[half:]\n",
    "        soft_h_tsf = soft_h_tsf[:, :1+self.batch_len, :]\n",
    "\n",
    "        self.loss_d0, loss_g0 = discriminator(teach_h[:half], soft_h_tsf[half:],\n",
    "            ones, zeros, filter_sizes, n_filters, self.dropout,\n",
    "            scope='discriminator0')\n",
    "        self.loss_d1, loss_g1 = discriminator(teach_h[half:], soft_h_tsf[:half],\n",
    "            ones, zeros, filter_sizes, n_filters, self.dropout,\n",
    "            scope='discriminator1')\n",
    "\n",
    "        #####   optimizer   #####\n",
    "        self.loss_adv = loss_g0 + loss_g1\n",
    "        self.loss = self.loss_rec + self.rho * self.loss_adv\n",
    "\n",
    "        theta_eg = retrive_var(['encoder', 'generator',\n",
    "            'embedding', 'projection'])\n",
    "        theta_d0 = retrive_var(['discriminator0'])\n",
    "        theta_d1 = retrive_var(['discriminator1'])\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(self.learning_rate, beta1, beta2)\n",
    "\n",
    "        grad_rec, _ = zip(*opt.compute_gradients(self.loss_rec, theta_eg))\n",
    "        grad_adv, _ = zip(*opt.compute_gradients(self.loss_adv, theta_eg))\n",
    "        grad, _ = zip(*opt.compute_gradients(self.loss, theta_eg))\n",
    "        grad, _ = tf.clip_by_global_norm(grad, grad_clip)\n",
    "\n",
    "        self.grad_rec_norm = tf.global_norm(grad_rec)\n",
    "        self.grad_adv_norm = tf.global_norm(grad_adv)\n",
    "        self.grad_norm = tf.global_norm(grad)\n",
    "\n",
    "        self.optimize_tot = opt.apply_gradients(zip(grad, theta_eg))\n",
    "        self.optimize_rec = opt.minimize(self.loss_rec, var_list=theta_eg)\n",
    "        self.optimize_d0 = opt.minimize(self.loss_d0, var_list=theta_d0)\n",
    "        self.optimize_d1 = opt.minimize(self.loss_d1, var_list=theta_d1)\n",
    "\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(model, decoder, sess, args, vocab, data0, data1, out_path):\n",
    "    batches, order0, order1 = get_batches(data0, data1,\n",
    "        vocab.word2id, args.batch_size)\n",
    "\n",
    "    #data0_rec, data1_rec = [], []\n",
    "    data0_tsf, data1_tsf = [], []\n",
    "    losses = Accumulator(len(batches), ['loss', 'rec', 'adv', 'd0', 'd1'])\n",
    "    for batch in batches:\n",
    "        rec, tsf = decoder.rewrite(batch)\n",
    "        half = batch['size'] / 2\n",
    "        #data0_rec += rec[:half]\n",
    "        #data1_rec += rec[half:]\n",
    "        data0_tsf += tsf[:half]\n",
    "        data1_tsf += tsf[half:]\n",
    "\n",
    "        loss, loss_rec, loss_adv, loss_d0, loss_d1 = sess.run([model.loss,\n",
    "            model.loss_rec, model.loss_adv, model.loss_d0, model.loss_d1],\n",
    "            feed_dict=feed_dictionary(model, batch, args.rho, args.gamma_min))\n",
    "        losses.add([loss, loss_rec, loss_adv, loss_d0, loss_d1])\n",
    "\n",
    "    n0, n1 = len(data0), len(data1)\n",
    "    #data0_rec = reorder(order0, data0_rec)[:n0]\n",
    "    #data1_rec = reorder(order1, data1_rec)[:n1]\n",
    "    data0_tsf = reorder(order0, data0_tsf)[:n0]\n",
    "    data1_tsf = reorder(order1, data1_tsf)[:n1]\n",
    "\n",
    "    if out_path:\n",
    "        #write_sent(data0_rec, out_path+'.0'+'.rec')\n",
    "        #write_sent(data1_rec, out_path+'.1'+'.rec')\n",
    "        write_sent(data0_tsf, out_path+'.0'+'.tsf')\n",
    "        write_sent(data1_tsf, out_path+'.1'+'.tsf')\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sess, args, vocab):\n",
    "    model = Model(args, vocab)\n",
    "    if args.load_model:\n",
    "        print( 'Loading model from', args.model)\n",
    "        model.saver.restore(sess, args.model)\n",
    "    else:\n",
    "        print( 'Creating model with fresh parameters.')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = load_sent(args.train + '.0', args.max_train_size)\n",
    "train1 = load_sent(args.train + '.1', args.max_train_size)\n",
    "print( '#sents of training file 0:', len(train0))\n",
    "print( '#sents of training file 1:', len(train1))\n",
    "\n",
    "\n",
    "build_vocab(train0 + train1, args.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train0 = load_sent(args.train + '.0', args.max_train_size)\n",
    "train1 = load_sent(args.train + '.1', args.max_train_size)\n",
    "print( '#sents of training file 0:', len(train0))\n",
    "print( '#sents of training file 1:', len(train1))\n",
    "\n",
    "\n",
    "build_vocab(train0 + train1, args.vocab)\n",
    "\n",
    "vocab = Vocabulary(args.vocab, args.embedding, args.dim_emb)\n",
    "print( 'vocabulary size:', vocab.size)\n",
    "\n",
    "if args.dev:\n",
    "    dev0 = load_sent(args.dev + '.0')\n",
    "    dev1 = load_sent(args.dev + '.1')\n",
    "\n",
    "if args.test:\n",
    "    test0 = load_sent(args.test + '.0')\n",
    "    test1 = load_sent(args.test + '.1')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    model = create_model(sess, args, vocab)\n",
    "\n",
    "    if args.beam > 1:\n",
    "        decoder = beam_search.Decoder(sess, args, vocab, model)\n",
    "    else:\n",
    "        decoder = greedy_decoding.Decoder(sess, args, vocab, model)\n",
    "\n",
    "    if args.train:\n",
    "        batches, _, _ = get_batches(train0, train1, vocab.word2id,\n",
    "            args.batch_size, noisy=True)\n",
    "        random.shuffle(batches)\n",
    "\n",
    "        start_time = time.time()\n",
    "        step = 0\n",
    "        losses = Accumulator(args.steps_per_checkpoint,\n",
    "            ['loss', 'rec', 'adv', 'd0', 'd1'])\n",
    "        best_dev = float('inf')\n",
    "        learning_rate = args.learning_rate\n",
    "        rho = args.rho\n",
    "        gamma = args.gamma_init\n",
    "        dropout = args.dropout_keep_prob\n",
    "\n",
    "        #gradients = Accumulator(args.steps_per_checkpoint,\n",
    "        #    ['|grad_rec|', '|grad_adv|', '|grad|'])\n",
    "\n",
    "        for epoch in range(1, 1+args.max_epochs):\n",
    "            print( '--------------------epoch %d--------------------' % epoch)\n",
    "            print( 'learning_rate:', learning_rate, '  gamma:', gamma)\n",
    "\n",
    "            for batch in batches:\n",
    "                feed_dict = feed_dictionary(model, batch, rho, gamma,\n",
    "                    dropout, learning_rate)\n",
    "\n",
    "                loss_d0, _ = sess.run([model.loss_d0, model.optimize_d0],\n",
    "                    feed_dict=feed_dict)\n",
    "                loss_d1, _ = sess.run([model.loss_d1, model.optimize_d1],\n",
    "                    feed_dict=feed_dict)\n",
    "\n",
    "                # do not back-propagate from the discriminator\n",
    "                # when it is too poor\n",
    "                if loss_d0 < 1.2 and loss_d1 < 1.2:\n",
    "                    optimize = model.optimize_tot\n",
    "                else:\n",
    "                    optimize = model.optimize_rec\n",
    "\n",
    "                loss, loss_rec, loss_adv, _ = sess.run([model.loss,\n",
    "                    model.loss_rec, model.loss_adv, optimize],\n",
    "                    feed_dict=feed_dict)\n",
    "                losses.add([loss, loss_rec, loss_adv, loss_d0, loss_d1])\n",
    "\n",
    "                #grad_rec, grad_adv, grad = sess.run([model.grad_rec_norm,\n",
    "                #    model.grad_adv_norm, model.grad_norm],\n",
    "                #    feed_dict=feed_dict)\n",
    "                #gradients.add([grad_rec, grad_adv, grad])\n",
    "\n",
    "                step += 1\n",
    "                if step % args.steps_per_checkpoint == 0:\n",
    "                    losses.output('step %d, time %.0fs,'\n",
    "                        % (step, time.time() - start_time))\n",
    "                    losses.clear()\n",
    "\n",
    "                    #gradients.output()\n",
    "                    #gradients.clear()\n",
    "\n",
    "            if args.dev:\n",
    "                dev_losses = transfer(model, decoder, sess, args, vocab,\n",
    "                    dev0, dev1, args.output + '.epoch%d' % epoch)\n",
    "                dev_losses.output('dev')\n",
    "                if dev_losses.values[0] < best_dev:\n",
    "                    best_dev = dev_losses.values[0]\n",
    "                    print( 'saving model...')\n",
    "                    model.saver.save(sess, args.model)\n",
    "\n",
    "            gamma = max(args.gamma_min, gamma * args.gamma_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
