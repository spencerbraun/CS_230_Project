{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./chen_crossaligned\")\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import ipdb\n",
    "import random\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from vocab import Vocabulary#, build_vocab\n",
    "from accumulator import Accumulator\n",
    "from options import load_arguments\n",
    "from file_io import load_sent, write_sent\n",
    "from utils import *\n",
    "from nn import *\n",
    "import beam_search, greedy_decoding\n",
    "\n",
    "from numpy import linalg as LA\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# %load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "{   'batch_size': 64,\n",
      "    'beam': 1,\n",
      "    'dev': '',\n",
      "    'dim_emb': 100,\n",
      "    'dim_y': 200,\n",
      "    'dim_z': 500,\n",
      "    'dropout_keep_prob': 0.5,\n",
      "    'embedding': '',\n",
      "    'filter_sizes': '1,2,3,4,5',\n",
      "    'gamma_decay': 1,\n",
      "    'gamma_init': 0.1,\n",
      "    'gamma_min': 0.1,\n",
      "    'learning_rate': 0.0005,\n",
      "    'load_model': False,\n",
      "    'max_epochs': 20,\n",
      "    'max_seq_length': 20,\n",
      "    'max_train_size': -1,\n",
      "    'model': '',\n",
      "    'n_filters': 128,\n",
      "    'n_layers': 1,\n",
      "    'online_testing': False,\n",
      "    'output': '',\n",
      "    'rho': 1,\n",
      "    'steps_per_checkpoint': 1000,\n",
      "    'test': '',\n",
      "    'train': '',\n",
      "    'vocab': ''}\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "args = load_arguments([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, args, vocab):\n",
    "        dim_y = args.dim_y\n",
    "        dim_z = args.dim_z\n",
    "        dim_h = dim_y + dim_z\n",
    "        dim_emb = args.dim_emb\n",
    "        n_layers = args.n_layers\n",
    "        max_len = args.max_seq_length\n",
    "        filter_sizes = [int(x) for x in args.filter_sizes.split(',')]\n",
    "        n_filters = args.n_filters\n",
    "        beta1, beta2 = 0.5, 0.999\n",
    "        grad_clip = 30.0\n",
    "\n",
    "        self.dropout = tf.compat.v1.placeholder(tf.compat.v1.float32,\n",
    "            name='dropout')\n",
    "        self.learning_rate = tf.compat.v1.placeholder(tf.compat.v1.float32,\n",
    "            name='learning_rate')\n",
    "        self.rho = tf.compat.v1.placeholder(tf.compat.v1.float32,\n",
    "            name='rho')\n",
    "        self.gamma = tf.compat.v1.placeholder(tf.compat.v1.float32,\n",
    "            name='gamma')\n",
    "\n",
    "        self.batch_len = tf.compat.v1.placeholder(tf.compat.v1.int32,\n",
    "            name='batch_len')\n",
    "        self.batch_size = tf.compat.v1.placeholder(tf.compat.v1.int32,\n",
    "            name='batch_size')\n",
    "        self.enc_inputs = tf.compat.v1.placeholder(tf.compat.v1.int32, [None, None],    #size * len\n",
    "            name='enc_inputs')\n",
    "        self.dec_inputs = tf.compat.v1.placeholder(tf.compat.v1.int32, [None, None],\n",
    "            name='dec_inputs')\n",
    "        self.targets = tf.compat.v1.placeholder(tf.compat.v1.int32, [None, None],\n",
    "            name='targets')\n",
    "        self.weights = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None],\n",
    "            name='weights')\n",
    "        self.labels = tf.compat.v1.placeholder(tf.compat.v1.float32, [None],\n",
    "            name='labels')\n",
    "\n",
    "        labels = tf.compat.v1.reshape(self.labels, [-1, 1])\n",
    "\n",
    "        embedding = tf.compat.v1.get_variable('embedding',\n",
    "            initializer=vocab.embedding.astype(np.float32))\n",
    "        with tf.compat.v1.variable_scope('projection'):\n",
    "            proj_W = tf.compat.v1.get_variable('W', [dim_h, vocab.size])\n",
    "            proj_b = tf.compat.v1.get_variable('b', [vocab.size])\n",
    "\n",
    "        enc_inputs = tf.compat.v1.nn.embedding_lookup(embedding, self.enc_inputs)\n",
    "        dec_inputs = tf.compat.v1.nn.embedding_lookup(embedding, self.dec_inputs)\n",
    "\n",
    "        #####   auto-encoder   #####\n",
    "        init_state = tf.compat.v1.concat([linear(labels, dim_y, scope='encoder'),\n",
    "            tf.compat.v1.zeros([self.batch_size, dim_z])], 1)\n",
    "        cell_e = create_cell(dim_h, n_layers, self.dropout)\n",
    "        _, z = tf.compat.v1.nn.dynamic_rnn(cell_e, enc_inputs,\n",
    "            initial_state=init_state, scope='encoder')\n",
    "        z = z[:, dim_y:]\n",
    "\n",
    "        #cell_e = create_cell(dim_z, n_layers, self.dropout)\n",
    "        #_, z = tf.compat.v1.nn.dynamic_rnn(cell_e, enc_inputs,\n",
    "        #    dtype=tf.compat.v1.float32, scope='encoder')\n",
    "\n",
    "        self.h_ori = tf.compat.v1.concat([linear(labels, dim_y,\n",
    "            scope='generator'), z], 1)\n",
    "        self.h_tsf = tf.compat.v1.concat([linear(1-labels, dim_y,\n",
    "            scope='generator', reuse=True), z], 1)\n",
    "\n",
    "        cell_g = create_cell(dim_h, n_layers, self.dropout)\n",
    "        g_outputs, _ = tf.compat.v1.nn.dynamic_rnn(cell_g, dec_inputs,\n",
    "            initial_state=self.h_ori, scope='generator')\n",
    "\n",
    "        # attach h0 in the front\n",
    "        teach_h = tf.compat.v1.concat([tf.compat.v1.expand_dims(self.h_ori, 1), g_outputs], 1)\n",
    "\n",
    "        g_outputs = tf.compat.v1.nn.dropout(g_outputs, self.dropout)\n",
    "        g_outputs = tf.compat.v1.reshape(g_outputs, [-1, dim_h])\n",
    "        g_logits = tf.compat.v1.matmul(g_outputs, proj_W) + proj_b\n",
    "\n",
    "        loss_rec = tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.compat.v1.reshape(self.targets, [-1]), logits=g_logits)\n",
    "        loss_rec *= tf.compat.v1.reshape(self.weights, [-1])\n",
    "        self.loss_rec = tf.compat.v1.reduce_sum(loss_rec) / tf.compat.v1.to_float(self.batch_size)\n",
    "\n",
    "        #####   feed-previous decoding   #####\n",
    "        go = dec_inputs[:,0,:]\n",
    "        soft_func = softsample_word(self.dropout, proj_W, proj_b, embedding,\n",
    "            self.gamma)\n",
    "        hard_func = argmax_word(self.dropout, proj_W, proj_b, embedding)\n",
    "\n",
    "        soft_h_ori, soft_logits_ori = rnn_decode(self.h_ori, go, max_len,\n",
    "            cell_g, soft_func, scope='generator')\n",
    "        soft_h_tsf, soft_logits_tsf = rnn_decode(self.h_tsf, go, max_len,\n",
    "            cell_g, soft_func, scope='generator')\n",
    "\n",
    "        hard_h_ori, self.hard_logits_ori = rnn_decode(self.h_ori, go, max_len,\n",
    "            cell_g, hard_func, scope='generator')\n",
    "        hard_h_tsf, self.hard_logits_tsf = rnn_decode(self.h_tsf, go, max_len,\n",
    "            cell_g, hard_func, scope='generator')\n",
    "\n",
    "        #####   discriminator   #####\n",
    "        # a batch's first half consists of sentences of one style,\n",
    "        # and second half of the other\n",
    "        half = self.batch_size / 2\n",
    "        zeros, ones = self.labels[:half], self.labels[half:]\n",
    "        soft_h_tsf = soft_h_tsf[:, :1+self.batch_len, :]\n",
    "\n",
    "        self.loss_d0, loss_g0 = discriminator(teach_h[:half], soft_h_tsf[half:],\n",
    "            ones, zeros, filter_sizes, n_filters, self.dropout,\n",
    "            scope='discriminator0')\n",
    "        self.loss_d1, loss_g1 = discriminator(teach_h[half:], soft_h_tsf[:half],\n",
    "            ones, zeros, filter_sizes, n_filters, self.dropout,\n",
    "            scope='discriminator1')\n",
    "\n",
    "        #####   optimizer   #####\n",
    "        self.loss_adv = loss_g0 + loss_g1\n",
    "        self.loss = self.loss_rec + self.rho * self.loss_adv\n",
    "\n",
    "        theta_eg = retrive_var(['encoder', 'generator',\n",
    "            'embedding', 'projection'])\n",
    "        theta_d0 = retrive_var(['discriminator0'])\n",
    "        theta_d1 = retrive_var(['discriminator1'])\n",
    "\n",
    "        opt = tf.compat.v1.train.AdamOptimizer(self.learning_rate, beta1, beta2)\n",
    "\n",
    "        grad_rec, _ = zip(*opt.compute_gradients(self.loss_rec, theta_eg))\n",
    "        grad_adv, _ = zip(*opt.compute_gradients(self.loss_adv, theta_eg))\n",
    "        grad, _ = zip(*opt.compute_gradients(self.loss, theta_eg))\n",
    "        grad, _ = tf.compat.v1.clip_by_global_norm(grad, grad_clip)\n",
    "\n",
    "        self.grad_rec_norm = tf.compat.v1.global_norm(grad_rec)\n",
    "        self.grad_adv_norm = tf.compat.v1.global_norm(grad_adv)\n",
    "        self.grad_norm = tf.compat.v1.global_norm(grad)\n",
    "\n",
    "        self.optimize_tot = opt.apply_gradients(zip(grad, theta_eg))\n",
    "        self.optimize_rec = opt.minimize(self.loss_rec, var_list=theta_eg)\n",
    "        self.optimize_d0 = opt.minimize(self.loss_d0, var_list=theta_d0)\n",
    "        self.optimize_d1 = opt.minimize(self.loss_d1, var_list=theta_d1)\n",
    "\n",
    "        self.saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transfer(model, decoder, sess, args, vocab, data0, data1, out_path):\n",
    "    batches, order0, order1 = get_batches(data0, data1,\n",
    "        vocab.word2id, args.batch_size)\n",
    "\n",
    "    #data0_rec, data1_rec = [], []\n",
    "    data0_tsf, data1_tsf = [], []\n",
    "    losses = Accumulator(len(batches), ['loss', 'rec', 'adv', 'd0', 'd1'])\n",
    "    for batch in batches:\n",
    "        rec, tsf = decoder.rewrite(batch)\n",
    "        half = batch['size'] / 2\n",
    "        #data0_rec += rec[:half]\n",
    "        #data1_rec += rec[half:]\n",
    "        data0_tsf += tsf[:half]\n",
    "        data1_tsf += tsf[half:]\n",
    "\n",
    "        loss, loss_rec, loss_adv, loss_d0, loss_d1 = sess.run([model.loss,\n",
    "            model.loss_rec, model.loss_adv, model.loss_d0, model.loss_d1],\n",
    "            feed_dict=feed_dictionary(model, batch, args.rho, args.gamma_min))\n",
    "        losses.add([loss, loss_rec, loss_adv, loss_d0, loss_d1])\n",
    "\n",
    "    n0, n1 = len(data0), len(data1)\n",
    "    #data0_rec = reorder(order0, data0_rec)[:n0]\n",
    "    #data1_rec = reorder(order1, data1_rec)[:n1]\n",
    "    data0_tsf = reorder(order0, data0_tsf)[:n0]\n",
    "    data1_tsf = reorder(order1, data1_tsf)[:n1]\n",
    "\n",
    "    if out_path:\n",
    "        #write_sent(data0_rec, out_path+'.0'+'.rec')\n",
    "        #write_sent(data1_rec, out_path+'.1'+'.rec')\n",
    "        write_sent(data0_tsf, out_path+'.0'+'.tsf')\n",
    "        write_sent(data1_tsf, out_path+'.1'+'.tsf')\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(sess, args, vocab):\n",
    "    model = Model(args, vocab)\n",
    "    if args.load_model:\n",
    "        print( 'Loading model from', args.model)\n",
    "        model.saver.restore(sess, args.model)\n",
    "    else:\n",
    "        print( 'Creating model with fresh parameters.')\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(data, path, min_occur=5):\n",
    "    \"\"\"\n",
    "    Function reads in all data, creates a number id for each word,\n",
    "    a list of all words in order of id, filters out words with low\n",
    "    occurrances. Writes it to a pickle dump.\n",
    "    \"\"\"\n",
    "    word2id = {'<pad>':0, '<go>':1, '<eos>':2, '<unk>':3}\n",
    "    id2word = ['<pad>', '<go>', '<eos>', '<unk>']\n",
    "\n",
    "    words = [word for sent in data for word in sent]\n",
    "    cnt = Counter(words)\n",
    "    for word in cnt:\n",
    "        if cnt[word] >= min_occur:\n",
    "            word2id[word] = len(word2id)\n",
    "            id2word.append(word)\n",
    "    vocab_size = len(word2id)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump((vocab_size, word2id, id2word), f, -1)\n",
    "    \n",
    "    #NOTE: remove later\n",
    "    return (vocab_size, word2id, id2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sents of training file 0: 1000\n",
      "#sents of training file 1: 1000\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/chen_crossaligned/data/yelp/\"\n",
    "train0 = load_sent(path+\"sentiment.train.0\", 1000)\n",
    "train1 = load_sent(path+\"sentiment.train.1\", 1000)\n",
    "print( '#sents of training file 0:', len(train0))\n",
    "print( '#sents of training file 1:', len(train1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# running build_vocab from above\n",
    "(vocab_size, word2id, id2word) = build_vocab(train0 + train1, \"./vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vocab class - reads in build_vocab pickle dump, creates embeddings\n",
    "\n",
    "vocab = Vocabulary(\"./vocab\", args.embedding, args.dim_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#dev set\n",
    "dev0 = load_sent(path+\"sentiment.dev.0\")\n",
    "dev1 = load_sent(path+\"sentiment.dev.1\")\n",
    "\n",
    "#test set\n",
    "test0 = load_sent(path+\"sentiment.test.0\")\n",
    "test1 = load_sent(path+\"sentiment.test.1\")\n",
    "\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "#config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args.beam = 8\n",
    "args.train = \"...\"\n",
    "args.output = \"./output.train\"\n",
    "args.model = \"./model\"\n",
    "args.dev = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/spencerbraun/dev/virtualenvs/cs230/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From ./chen_crossaligned/nn.py:8: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-0e08a6848e98>:55: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/spencerbraun/dev/virtualenvs/cs230/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/spencerbraun/dev/virtualenvs/cs230/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/spencerbraun/dev/virtualenvs/cs230/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-4-0e08a6848e98>:74: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-4-0e08a6848e98>:81: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer gru_cell_2 is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-53729d054c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# create model instance, passing in args and vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-36e14dfb9eab>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(sess, args, vocab)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Loading model from'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0e08a6848e98>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, vocab)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         soft_h_ori, soft_logits_ori = rnn_decode(self.h_ori, go, max_len,\n\u001b[0;32m---> 90\u001b[0;31m             cell_g, soft_func, scope='generator')\n\u001b[0m\u001b[1;32m     91\u001b[0m         soft_h_tsf, soft_logits_tsf = rnn_decode(self.h_tsf, go, max_len,\n\u001b[1;32m     92\u001b[0m             cell_g, soft_func, scope='generator')\n",
      "\u001b[0;32m~/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/chen_crossaligned/nn.py\u001b[0m in \u001b[0;36mrnn_decode\u001b[0;34m(h, inp, length, cell, loop_func, scope)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mh_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mlogits_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/virtualenvs/cs230/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \"\"\"\n\u001b[1;32m   1139\u001b[0m     return self._call_wrapped_cell(\n\u001b[0;32m-> 1140\u001b[0;31m         inputs, state, cell_call_fn=self.cell.__call__, scope=scope)\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/virtualenvs/cs230/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_wrapper_impl.py\u001b[0m in \u001b[0;36m_call_wrapped_cell\u001b[0;34m(self, inputs, state, cell_call_fn, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m       inputs = self._dropout(inputs, \"input\", self._recurrent_input_noise,\n\u001b[1;32m    276\u001b[0m                              self._input_keep_prob)\n\u001b[0;32m--> 277\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell_call_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_should_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Identify which subsets of the state to perform dropout on and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/virtualenvs/cs230/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# method.  See the class docstring for more details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     return base_layer.Layer.__call__(\n\u001b[0;32m--> 386\u001b[0;31m         self, inputs, state, scope=scope, *args, **kwargs)\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/virtualenvs/cs230/lib/python3.7/site-packages/tensorflow_core/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/virtualenvs/cs230/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;31m# are casted, not before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 834\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/virtualenvs/cs230/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0;32m--> 165\u001b[0;31m                          \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                          \u001b[0;34m'its rank is undefined, but the layer requires a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                          'defined rank.')\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer gru_cell_2 is incompatible with the layer: its rank is undefined, but the layer requires a defined rank."
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    \n",
    "    # create model instance, passing in args and vocab\n",
    "    model = create_model(sess, args, vocab)\n",
    "    \n",
    "    \n",
    "    # beam search is something we will learn about \n",
    "    # This creates Decoder instance\n",
    "#     if args.beam > 1:\n",
    "    decoder = beam_search.Decoder(sess, args, vocab, model)\n",
    "#     else:\n",
    "#         decoder = greedy_decoding.Decoder(sess, args, vocab, model)\n",
    "\n",
    "    if args.train:\n",
    "        batches, _, _ = get_batches(train0, train1, vocab.word2id,\n",
    "            args.batch_size, noisy=True)\n",
    "        random.shuffle(batches)\n",
    "\n",
    "        start_time = time.time()\n",
    "        step = 0\n",
    "        \n",
    "        losses = Accumulator(args.steps_per_checkpoint,\n",
    "            ['loss', 'rec', 'adv', 'd0', 'd1'])\n",
    "        \n",
    "        best_dev = float('inf')\n",
    "        learning_rate = args.learning_rate\n",
    "        rho = args.rho\n",
    "        gamma = args.gamma_init\n",
    "        dropout = args.dropout_keep_prob\n",
    "\n",
    "        for epoch in range(1, 1+args.max_epochs):\n",
    "            print( '--------------------epoch %d--------------------' % epoch)\n",
    "            print( 'learning_rate:', learning_rate, '  gamma:', gamma)\n",
    "\n",
    "            for batch in batches:\n",
    "                feed_dict = feed_dictionary(model, batch, rho, gamma,\n",
    "                    dropout, learning_rate)\n",
    "\n",
    "                loss_d0, _ = sess.run([model.loss_d0, model.optimize_d0],\n",
    "                    feed_dict=feed_dict)\n",
    "                loss_d1, _ = sess.run([model.loss_d1, model.optimize_d1],\n",
    "                    feed_dict=feed_dict)\n",
    "\n",
    "                # do not back-propagate from the discriminator\n",
    "                # when it is too poor\n",
    "                if loss_d0 < 1.2 and loss_d1 < 1.2:\n",
    "                    optimize = model.optimize_tot\n",
    "                else:\n",
    "                    optimize = model.optimize_rec\n",
    "\n",
    "                loss, loss_rec, loss_adv, _ = sess.run([model.loss,\n",
    "                    model.loss_rec, model.loss_adv, optimize],\n",
    "                    feed_dict=feed_dict)\n",
    "                losses.add([loss, loss_rec, loss_adv, loss_d0, loss_d1])\n",
    "\n",
    "                #grad_rec, grad_adv, grad = sess.run([model.grad_rec_norm,\n",
    "                #    model.grad_adv_norm, model.grad_norm],\n",
    "                #    feed_dict=feed_dict)\n",
    "                #gradients.add([grad_rec, grad_adv, grad])\n",
    "\n",
    "                step += 1\n",
    "                if step % args.steps_per_checkpoint == 0:\n",
    "                    losses.output('step %d, time %.0fs,'\n",
    "                        % (step, time.time() - start_time))\n",
    "                    losses.clear()\n",
    "\n",
    "                    #gradients.output()\n",
    "                    #gradients.clear()\n",
    "\n",
    "            if args.dev:\n",
    "                dev_losses = transfer(model, decoder, sess, args, vocab,\n",
    "                    dev0, dev1, args.output + '.epoch%d' % epoch)\n",
    "                dev_losses.output('dev')\n",
    "                if dev_losses.values[0] < best_dev:\n",
    "                    best_dev = dev_losses.values[0]\n",
    "                    print( 'saving model...')\n",
    "                    model.saver.save(sess, args.model)\n",
    "\n",
    "            gamma = max(args.gamma_min, gamma * args.gamma_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
