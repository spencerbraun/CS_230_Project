{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import fasttext\n",
    "import pkg_resources\n",
    "import kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gloveFile = \"data\\\\glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "yelp_acc_path = 'acc_yelp.bin'\n",
    "yelp_ppl_path = 'ppl_yelp.binary'\n",
    "yelp_ref0_path = 'yelp.refs.0'\n",
    "yelp_ref1_path = 'yelp.refs.1'\n",
    "\n",
    "\n",
    "yelp_acc_file = pkg_resources.resource_stream(resource_package, yelp_acc_path)\n",
    "yelp_ppl_file = pkg_resources.resource_stream(resource_package, yelp_ppl_path)\n",
    "yelp_ref0_file = pkg_resources.resource_stream(resource_package, yelp_ref0_path)\n",
    "yelp_ref1_file = pkg_resources.resource_stream(resource_package, yelp_ref1_path)\n",
    "\n",
    "\n",
    "self.yelp_ref = []\n",
    "with open(yelp_ref0_file.name, 'r') as fin:\n",
    "    self.yelp_ref.append(fin.readlines())\n",
    "with open(yelp_ref1_file.name, 'r') as fin:\n",
    "    self.yelp_ref.append(fin.readlines())\n",
    "self.classifier_yelp = fasttext.load_model(yelp_acc_file.name)\n",
    "self.yelp_ppl_model = kenlm.Model(yelp_ppl_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Style check from fastText model\n",
    "\n",
    "def yelp_style_check(self, text_transfered, style_origin):\n",
    "        text_transfered = ' '.join(word_tokenize(text_transfered.lower().strip()))\n",
    "        if text_transfered == '':\n",
    "            return False\n",
    "        label = self.classifier_yelp.predict([text_transfered])\n",
    "        style_transfered = label[0][0] == '__label__positive'\n",
    "        return (style_transfered != style_origin)\n",
    "    \n",
    "# Checking the accuracy for different styles\n",
    "    \n",
    "def yelp_acc_b(self, texts, styles_origin):\n",
    "        assert len(texts) == len(styles_origin), 'Size of inputs does not match!'\n",
    "        count = 0\n",
    "        for text, style in zip(texts, styles_origin):\n",
    "            if self.yelp_style_check(text, style):\n",
    "                count += 1\n",
    "        return count / len(texts)\n",
    "\n",
    "def yelp_acc_0(self, texts):\n",
    "        styles_origin = [0] * len(texts)\n",
    "        return self.yelp_acc_b(texts, styles_origin)\n",
    "\n",
    "def yelp_acc_1(self, texts):\n",
    "        styles_origin = [1] * len(texts)\n",
    "        return self.yelp_acc_b(texts, styles_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the NLTK model\n",
    "def nltk_bleu(texts_origin, text_transfered):\n",
    "        texts_origin = [word_tokenize(text_origin.lower().strip()) for text_origin in texts_origin]\n",
    "        text_transfered = word_tokenize(text_transfered.lower().strip())\n",
    "        return sentence_bleu(texts_origin, text_transfered) * 100\n",
    "\n",
    "# Check the BLEU diff between original & transferred text\n",
    "def self_bleu_b(self, texts_origin, texts_transfered):\n",
    "        assert len(texts_origin) == len(texts_transfered), 'Size of inputs does not match!'\n",
    "        sum = 0\n",
    "        n = len(texts_origin)\n",
    "        for x, y in zip(texts_origin, texts_transfered):\n",
    "            sum += self.nltk_bleu([x], y)\n",
    "        return sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measures perplexity of language model\n",
    "\n",
    "def yelp_ppl(self, texts_transfered):\n",
    "        texts_transfered = [' '.join(word_tokenize(itm.lower().strip())) for itm in texts_transfered]\n",
    "        sum = 0\n",
    "        words = []\n",
    "        length = 0\n",
    "        for i, line in enumerate(texts_transfered):\n",
    "            words += [word for word in line.split()]\n",
    "            length += len(line.split())\n",
    "            score = self.yelp_ppl_model.score(line)\n",
    "            sum += score\n",
    "        return math.pow(10, -sum / length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_ngram_lm(kenlm_path, data_path, output_path, N):\n",
    "    \"\"\"\n",
    "    Trains a modified Kneser-Ney n-gram KenLM from a text file.\n",
    "    Creates a .arpa file to store n-grams.\n",
    "    \"\"\"\n",
    "    # create .arpa file of n-grams\n",
    "    curdir = os.path.abspath(os.path.curdir)\n",
    "    #\n",
    "    command = \"bin/lmplz -o \"+str(N)+\" <\"+os.path.join(curdir, data_path) + \\\n",
    "              \" >\"+os.path.join(curdir, output_path)\n",
    "    os.system(\"cd \"+os.path.join(kenlm_path, 'build')+\" && \"+command)\n",
    "\n",
    "    load_kenlm()\n",
    "    # create language model\n",
    "    assert(output_path)  # captured by try..except block outside\n",
    "    model = kenlm.Model(output_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ppl(lm, sentences):\n",
    "    \"\"\"\n",
    "    Assume sentences is a list of strings (space delimited sentences)\n",
    "    \"\"\"\n",
    "    total_nll = 0\n",
    "    total_wc = 0\n",
    "    for sent in sentences:\n",
    "        words = sent.strip().split()\n",
    "        nll = np.sum([- math.log(math.pow(10.0, score)) for score, _, _ in lm.full_scores(sent, bos=True, eos=False)])\n",
    "        word_count = len(words)\n",
    "        total_wc += word_count\n",
    "        total_nll += nll\n",
    "    ppl = np.exp(total_nll / total_wc)\n",
    "    return ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_distance_countvectorizer_method(s1, s2):\n",
    "    \n",
    "    # sentences to list\n",
    "    allsentences = [s1 , s2]\n",
    "    \n",
    "    # packages\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from scipy.spatial import distance\n",
    "    \n",
    "    # text to vector\n",
    "    vectorizer = CountVectorizer()\n",
    "    all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n",
    "    text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n",
    "    text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n",
    "    \n",
    "    # distance of similarity\n",
    "    cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n",
    "    print('Similarity of two sentences are equal to ',round((1-cosine)*100,2),'%')\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    with open(gloveFile, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "    model = {}\n",
    "    for line in content:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess(raw_text):\n",
    "\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    cleaned_words = list(set([w for w in words if w not in stopword_set]))\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "def cosine_distance_between_two_words(word1, word2):\n",
    "    import scipy\n",
    "    return (1- scipy.spatial.distance.cosine(model[word1], model[word2]))\n",
    "\n",
    "def calculate_heat_matrix_for_two_sentences(s1,s2):\n",
    "    s1 = preprocess(s1)\n",
    "    s2 = preprocess(s2)\n",
    "    result_list = [[cosine_distance_between_two_words(word1, word2) for word2 in s2] for word1 in s1]\n",
    "    result_df = pd.DataFrame(result_list)\n",
    "    result_df.columns = s2\n",
    "    result_df.index = s1\n",
    "    return result_df\n",
    "\n",
    "def cosine_distance_wordembedding_method(s1, s2):\n",
    "    import scipy\n",
    "    vector_1 = np.mean([model[word] for word in preprocess(s1)],axis=0)\n",
    "    vector_2 = np.mean([model[word] for word in preprocess(s2)],axis=0)\n",
    "    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
    "    print('Word Embedding method with a cosine distance asses that our two sentences are similar to',round((1-cosine)*100,2),'%')\n",
    "\n",
    "def heat_map_matrix_between_two_sentences(s1,s2):\n",
    "    df = calculate_heat_matrix_for_two_sentences(s1,s2)\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(5,5)) \n",
    "    ax_blue = sns.heatmap(df, cmap=\"YlGnBu\")\n",
    "    # ax_red = sns.heatmap(df)\n",
    "    print(cosine_distance_wordembedding_method(s1, s2))\n",
    "    return ax_blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/style-transformer/outputs/soph/model_iteration_lr_0.0001/\"\n",
    "with open(path + \"gold_text.txt\") as f:\n",
    "    gold = f.readlines()\n",
    "    \n",
    "with open(path + \"rev_output_0.txt\") as f:\n",
    "    rev0 = f.readlines()\n",
    "    \n",
    "with open(path + \"raw_output_0.txt\") as f:\n",
    "    raw0 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(sent):\n",
    "    sent = sent.strip().replace('<pad>', '').strip()\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gold = list(map(process, gold))\n",
    "rev0 = list(map(process, rev0))\n",
    "raw0 = list(map(process, raw0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_sent(originText, transferredText):\n",
    "        texts_origin = [\n",
    "            word_tokenize(text.lower().strip()) \n",
    "            for text in originText\n",
    "        ]\n",
    "        text_transfered = word_tokenize(transferredText.lower().strip())\n",
    "        return sentence_bleu(texts_origin, text_transfered) * 100\n",
    "    \n",
    "\n",
    "\n",
    "def bleu_avg(originText, transferredText):\n",
    "        assert len(originText) == len(transferredText), 'Size of inputs does not match!'\n",
    "        sum = 0\n",
    "        n = len(originText)\n",
    "        for x, y in zip(originText, transferredText):\n",
    "            sum += bleu_sent([x], y)\n",
    "        return sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.02805322367601"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_bleu_b(gold, raw0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_kenlm():\n",
    "    global kenlm\n",
    "    import kenlm\n",
    "\n",
    "def train_ngram_lm(kenlm_path, data_path, output_path, N):\n",
    "    \"\"\"\n",
    "    FROM ARAE\n",
    "    Trains a modified Kneser-Ney n-gram KenLM from a text file.\n",
    "    Creates a .arpa file to store n-grams.\n",
    "    \"\"\"\n",
    "    # create .arpa file of n-grams\n",
    "    curdir = os.path.abspath(os.path.curdir)\n",
    "    #\n",
    "    command = \"bin/lmplz -o \"+str(N)+\" <\"+os.path.join(curdir, data_path) + \\\n",
    "              \" >\"+os.path.join(curdir, output_path)\n",
    "    os.system(\"cd \"+os.path.join(kenlm_path, 'build')+\" && \"+command)\n",
    "\n",
    "    load_kenlm()\n",
    "    # create language model\n",
    "    assert(output_path)  # captured by try..except block outside\n",
    "    model = kenlm.Model(output_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot read model 'kenlm_output' (util/file.cc:183 in std::size_t util::PartialRead(int, void *, std::size_t) threw FDException because `ret < 0'. Is a directory in fd 97 while reading 6 bytes in file /Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/kenlm_output)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32mkenlm.pyx\u001b[0m in \u001b[0;36mkenlm.Model.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: util/file.cc:183 in std::size_t util::PartialRead(int, void *, std::size_t) threw FDException because `ret < 0'.\nIs a directory in fd 97 while reading 6 bytes in file /Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/kenlm_output",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-727d417a7617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ngram_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kenlm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/processed/soph_train_small.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kenlm_output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-6287edf0ae71>\u001b[0m in \u001b[0;36mtrain_ngram_lm\u001b[0;34m(kenlm_path, data_path, output_path, N)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# create language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# captured by try..except block outside\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkenlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mkenlm.pyx\u001b[0m in \u001b[0;36mkenlm.Model.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot read model 'kenlm_output' (util/file.cc:183 in std::size_t util::PartialRead(int, void *, std::size_t) threw FDException because `ret < 0'. Is a directory in fd 97 while reading 6 bytes in file /Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/kenlm_output)"
     ]
    }
   ],
   "source": [
    "train_ngram_lm('kenlm', 'data/processed/soph_train_small.txt', 'kenlm_output', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ppl(lm, sentences):\n",
    "    \"\"\"\n",
    "    Assume sentences is a list of strings (space delimited sentences)\n",
    "    \"\"\"\n",
    "    total_nll = 0\n",
    "    total_wc = 0\n",
    "    for sent in sentences:\n",
    "        words = sent.strip().split()\n",
    "        nll = np.sum([- math.log(math.pow(10.0, score)) for score, _, _ in lm.full_scores(sent, bos=True, eos=False)])\n",
    "        word_count = len(words)\n",
    "        total_wc += word_count\n",
    "        total_nll += nll\n",
    "    ppl = np.exp(total_nll / total_wc)\n",
    "    return ppl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
