{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import spacy\n",
    "import textstat\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloveFile = \"glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalPath = \"/home/ubuntu/style-transformer/outputs/soph_1/model_iteration_lr_0.0001/\"\n",
    "higherLR = \"/home/ubuntu/style-transformer/outputs/soph_1/model_iteration_lr_0.001/\"\n",
    "soph2 = \"/home/ubuntu/style-transformer/outputs/soph_2/\"\n",
    "soph3 = \"/home/ubuntu/style-transformer/outputs/soph_3/\"\n",
    "sophTagged = \"/home/ubuntu/style-transformer/outputs/soph_tagged/\"\n",
    "sophTaggedNp = \"/home/ubuntu/style-transformer/outputs/soph_tagged_np/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sent):\n",
    "    sent = sent.strip().replace('<pad>', '').strip()\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNaiveTest(runNum):\n",
    "    path = f\"/home/ubuntu/style-transformer/data/soph_{runNum}/test.neg\"\n",
    "    with open(path) as f:\n",
    "        naive = f.readlines()\n",
    "    return list(map(process, naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformer(path):\n",
    "    with open(path + \"gold_text.txt\") as f:\n",
    "        gold = f.readlines()\n",
    "    with open(path + \"rev_output_0.txt\") as f:\n",
    "        rev0 = f.readlines()\n",
    "    with open(path + \"raw_output_0.txt\") as f:\n",
    "        raw0 = f.readlines()\n",
    "    with open(path + \"rev_output_1.txt\") as f:\n",
    "        rev1 = f.readlines()\n",
    "    with open(path + \"raw_output_1.txt\") as f:\n",
    "        raw1 = f.readlines()\n",
    "        \n",
    "    gold = list(map(process, gold))\n",
    "    rev0 = list(map(process, rev0))\n",
    "    raw0 = list(map(process, raw0))\n",
    "    \n",
    "    return {0: (gold, rev0, raw0), 1:(gold, rev1, raw1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_sent(originText, transferredText):\n",
    "    texts_origin = [\n",
    "        word_tokenize(text.lower().strip()) \n",
    "        for text in originText\n",
    "    ]\n",
    "    text_transfered = word_tokenize(transferredText.lower().strip())\n",
    "    cc = SmoothingFunction()\n",
    "    return sentence_bleu(texts_origin, text_transfered, smoothing_function=cc.method3)\n",
    "\n",
    "\n",
    "\n",
    "def bleu_avg(originText, transferredText):\n",
    "    sum = 0\n",
    "    n = len(originText)\n",
    "    for x, y in zip(originText, transferredText):\n",
    "        sum += bleu_sent([x], y)\n",
    "    return sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### KenLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMs add probability to each token sequence to indicate how likely it is for the sequence to occur in real text. Train LM on the target language, and the model estimates the probability of seeing a given sentence in the target text using Markov chains.\n",
    "\n",
    "\n",
    "In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample. \n",
    "\n",
    "The perplexity(sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the numberof words. https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf\n",
    "\n",
    "PPLxdenotes theperplexity of sentences transferred from positive sentences evaluated by a language model trainedwith negative sentences and vice versa. https://arxiv.org/pdf/1805.11749.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kenlm():\n",
    "    global kenlm\n",
    "    import kenlm\n",
    "\n",
    "def train_ngram_lm(kenlm_path, data_path, output_path, N, load=False):\n",
    "    if not load:\n",
    "        \n",
    "        curdir = os.path.abspath(os.path.curdir)\n",
    "        command = \"bin/lmplz -o \"+str(N)+\" <\"+os.path.join(curdir, data_path) + \\\n",
    "                  \" >\"+os.path.join(curdir, output_path)\n",
    "        print(command)\n",
    "        os.system(\"cd \"+os.path.join(kenlm_path, 'build')+\" && \"+command)\n",
    "\n",
    "    load_kenlm()\n",
    "\n",
    "    assert(output_path) \n",
    "    model = kenlm.Model(output_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentencePplFrame(reference, transferred, klm):\n",
    "    ppl_dict = {}\n",
    "    for i in range(len(reference)):\n",
    "        ppl_dict[i] = {'ppl':(get_ppl(klm, [reference[i]]), get_ppl(klm, [transferred[i]])), \n",
    "                       'sent1': reference[i],\n",
    "                       'sent2': transferred[i]}\n",
    "    test_df = pd.DataFrame(ppl_dict).T\n",
    "    test_df['ppl1'] = test_df.ppl.apply(lambda x: x[0])\n",
    "    test_df['ppl2'] = test_df.ppl.apply(lambda x: x[1])\n",
    "    test_df = test_df.sort_values('ppl2')\n",
    "    cols = ['ppl1', 'ppl2', 'sent1', 'sent2']\n",
    "    \n",
    "    return test_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin/lmplz -o 5 </home/ubuntu/CS_230_Project/data/processed/soph_train_tagged_nopunct.txt >/home/ubuntu/CS_230_Project/klm_soph_tagged_np.arpa\n"
     ]
    }
   ],
   "source": [
    "kenlm_model = train_ngram_lm(\n",
    "    'kenlm', \n",
    "    'data/processed/soph_train_tagged_nopunct.txt', \n",
    "    'klm_soph_tagged_np.arpa', \n",
    "    5, \n",
    "    load=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.716026782989502 2: <s> they\n",
      "-4.684571266174316 1: follow\n",
      "-3.069218635559082 1: what\n",
      "-5.896757125854492 1: technology\n",
      "\t\"technology\" is an OOV\n",
      "-1.8870635032653809 1: to\n",
      "-1.8208752870559692 2: to make\n",
      "-3.4363248348236084 2: make life\n",
      "-4.816084384918213 1: .\n",
      "-0.05439453572034836 2: . </s>\n",
      "\"technology\" is an OOV\n"
     ]
    }
   ],
   "source": [
    "sentence = gold[10]\n",
    "# Show scores and n-gram matches\n",
    "words = ['<s>'] + sentence.split() + ['</s>']\n",
    "for i, (prob, length, oov) in enumerate(kenlm_model.full_scores(sentence)):\n",
    "    print('{0} {1}: {2}'.format(prob, length, ' '.join(words[i+2-length:i+2])))\n",
    "    if oov:\n",
    "        print('\\t\"{0}\" is an OOV'.format(words[i+1]))\n",
    "\n",
    "# Find out-of-vocabulary words\n",
    "for w in words:\n",
    "    if not w in kenlm_model:\n",
    "        print('\"{0}\" is an OOV'.format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppl(lm, sentences):\n",
    "    \"\"\"\n",
    "    Assume sentences is a list of strings (space delimited sentences)\n",
    "    \"\"\"\n",
    "    total_nll = 0\n",
    "    total_wc = 0\n",
    "    for sent in sentences:\n",
    "        words = sent.strip().split()\n",
    "        nll = np.sum([- math.log(math.pow(10.0, score)) for score, _, _ in lm.full_scores(sent, bos=True, eos=False)])\n",
    "        word_count = len(words)\n",
    "        total_wc += word_count\n",
    "        total_nll += nll\n",
    "    ppl = np.exp(total_nll / total_wc)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities - Jaccard,  Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(sent1, sent2): \n",
    "    a = set(sent1.split()) \n",
    "    b = set(sent2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    with open(gloveFile, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "    model = {}\n",
    "    for line in content:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    return model\n",
    "\n",
    "def cosine_format(raw):\n",
    "\n",
    "    processed = re.sub(\"[^a-zA-Z]\", \" \", raw)\n",
    "    words = processed.lower().split()\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    uniq_words = list(set([w for w in words if w not in stopword_set]))\n",
    "    \n",
    "    return uniq_words\n",
    "\n",
    "def cosine_words(word1, word2):\n",
    "\n",
    "    return (1 - scipy.spatial.distance.cosine(model[word1], model[word2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loadGloveModel(gloveFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sent(sent1, sent2):\n",
    "    if not isinstance(sent1, list):\n",
    "        sent1 = cosine_format(sent1)\n",
    "        sent2 = cosine_format(sent2)\n",
    "\n",
    "    embs1 = np.mean([model[word] for word in sent1], axis=0)\n",
    "    embs2 = np.mean([model[word] for word in sent2], axis=0)\n",
    "\n",
    "    return(1 - scipy.spatial.distance.cosine(embs1, embs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def heat_matrix(sent1, sent2):\n",
    "    \n",
    "    s1 = cosine_format(sent1)\n",
    "    s2 = cosine_format(sent2)\n",
    "    \n",
    "    result_list = [[cosine_words(word1, word2) for word2 in s2] for word1 in s1]\n",
    "    result_df = pd.DataFrame(result_list)\n",
    "    result_df.columns = s2\n",
    "    result_df.index = s1\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def heat_map(s1, s2):\n",
    "    df = heat_matrix(s1, s2)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5,5)) \n",
    "    ax_blue = sns.heatmap(df, cmap=\"YlGnBu\")\n",
    "    print(cosine_sent(s1, s2))\n",
    "    return ax_blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PINC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/cocoxu/Shakespeare/blob/master/python/PINC_sentence.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(list1, list2) :\n",
    "    cnt1 = Counter()\n",
    "    cnt2 = Counter()\n",
    "    for tk1 in list1:\n",
    "        cnt1[tk1] += 1\n",
    "    for tk2 in list2:\n",
    "        cnt2[tk2] += 1    \n",
    "    inter = cnt1 & cnt2\n",
    "    return len(list(inter.elements()))\n",
    "\n",
    "\n",
    "def pinc(ssent, csent):\n",
    "    s1grams = ssent.split(\" \")\n",
    "    c1grams = csent.split(\" \")\n",
    "    s2grams = []\n",
    "    c2grams = []\n",
    "    s3grams = []\n",
    "    c3grams = []\n",
    "    s4grams = []\n",
    "    c4grams = []\n",
    "        \n",
    "    for i in range(0, len(s1grams)-1) :\n",
    "        if i < len(s1grams) - 1:\n",
    "            s2gram = s1grams[i] + \" \" + s1grams[i+1]\n",
    "            s2grams.append(s2gram)\n",
    "        if i < len(s1grams)-2:\n",
    "            s3gram = s1grams[i] + \" \" + s1grams[i+1] + \" \" + s1grams[i+2]\n",
    "            s3grams.append(s3gram)\n",
    "        if i < len(s1grams)-3:\n",
    "            s4gram = s1grams[i] + \" \" + s1grams[i+1] + \" \" + s1grams[i+2] + \" \" + s1grams[i+3]\n",
    "            s4grams.append(s4gram)\n",
    "            \n",
    "    for i in range(0, len(c1grams)-1) :\n",
    "        if i < len(c1grams) - 1:\n",
    "            c2gram = c1grams[i] + \" \" + c1grams[i+1]\n",
    "            c2grams.append(c2gram)\n",
    "        if i < len(c1grams)-2:\n",
    "            c3gram = c1grams[i] + \" \" + c1grams[i+1] + \" \" + c1grams[i+2]\n",
    "            c3grams.append(c3gram)\n",
    "        if i < len(c1grams)-3:\n",
    "            c4gram = c1grams[i] + \" \" + c1grams[i+1] + \" \" + c1grams[i+2] + \" \" + c1grams[i+3]\n",
    "            c4grams.append(c4gram)\n",
    "\n",
    "    score = intersect(s1grams, c1grams) / len(c1grams)\n",
    "    if len(c2grams) > 0:\n",
    "        score += intersect(s2grams, c2grams) / len(c2grams)\n",
    "    if len(c3grams) > 0:\n",
    "        score += intersect(s3grams, c3grams) / len(c3grams)\n",
    "    if len(c4grams) > 0:\n",
    "        score += intersect(s4grams, c4grams) / len(c4grams)\n",
    "    return 1 - score/4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinc_corpus(origText, transferText):\n",
    "    sentcount = len(origText)\n",
    "    pincscore = 0.0\n",
    "\n",
    "    for idx in range(len(origText)):\n",
    "\n",
    "        sline = origText[idx].strip()\n",
    "        cline = transferText[idx].strip()\n",
    "        \n",
    "        sentscore = pinc(sline, cline)       \n",
    "        pincscore += sentscore\n",
    "\n",
    "    pincscore = pincscore / sentcount * 100\n",
    "    \n",
    "    return pincscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceMetrics(sent1, sent2, kenlm_model, output=False):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['bleu'] = bleu_sent(sent1, sent2)\n",
    "    metrics['cosine'] = cosine_sent(sent1, sent2)\n",
    "    metrics['jaccard'] = jaccard_sim(sent1, sent2)\n",
    "    metrics['pinc'] = pinc(sent1, sent2)\n",
    "    metrics['ppl'] = (get_ppl(kenlm_model, [sent1]), get_ppl(kenlm_model, [sent2]))\n",
    "    \n",
    "    if output:\n",
    "        print(f\"Orig: {sent1}\")\n",
    "        print(f\"New: {sent2}\")\n",
    "        heat_map(sent1, sent2)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalMetrics(origData, transferData, kenlm_model):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['bleu'] = bleu_avg(origData, transferData)\n",
    "    metrics['ppl'] = (get_ppl(kenlm_model, origData), \n",
    "                      get_ppl(kenlm_model, transferData))\n",
    "    metrics['pinc'] = pinc_corpus(origData, transferData)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = load_transformer(originalPath)\n",
    "gold_orig, rev_orig, raw_orig = loaded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = load_transformer(higherLR)\n",
    "gold_HLR, rev_HLR, raw_HLR = loaded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = load_transformer(soph2)\n",
    "gold_soph2, rev_soph2, raw_soph2 = loaded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = load_transformer(sophTagged)\n",
    "gold_soph_tag, rev_soph_tag, raw_soph_tag = loaded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = load_transformer(sophTaggedNp)\n",
    "gold_soph_tag_np, rev_soph_tag_np, raw_soph_tag_np = loaded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_1 = readNaiveTest(1)\n",
    "naive_2 = readNaiveTest(2)\n",
    "naive_3 = readNaiveTest(3)\n",
    "naive_tag = readNaiveTest('tagged')\n",
    "naive_tag_np = readNaiveTest('tagged_np')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kenlm_1 = train_ngram_lm('kenlm', 'data/processed/soph_train.txt', 'klm_soph_1.arpa', 5, load=True)\n",
    "# kenlm_2 = train_ngram_lm('kenlm', 'data/processed/soph_train_2.txt', 'klm_soph_2.arpa', 5, load=True)\n",
    "# kenlm_3 = train_ngram_lm('kenlm', 'data/processed/soph_train_3.txt', 'klm_soph_3.arpa', 5, load=True)\n",
    "# kenlm_tag = train_ngram_lm('kenlm', 'data/processed/soph_train_tagged.txt', 'klm_soph_tagged.arpa', 5, load=True)\n",
    "kenlm_tag_np = train_ngram_lm('kenlm', 'data/processed/soph_train_tagged_nopunct.txt', 'klm_soph_tagged_np.arpa', 5, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.3613066422495174,\n",
       " 'ppl': (798.1598083846517, 629.9162131290562),\n",
       " 'pinc': 62.63539704549997}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalMetrics(naive_2, rev_soph2, kenlm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.380887868597427,\n",
       " 'ppl': (447.7485862016299, 282.4529932116936),\n",
       " 'pinc': 58.73869506288379}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalMetrics(naive_tag, rev_soph_tag, kenlm_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.09690859889981704,\n",
       " 'ppl': (546.7043494507218, 73.09098323777687),\n",
       " 'pinc': 83.82863839728782}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalMetrics(naive_tag_np, rev_soph_tag_np, kenlm_tag_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftag = SentencePplFrame(naive_tag_np, rev_soph_tag_np, kenlm_tag_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ppl1</th>\n",
       "      <th>ppl2</th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>482.787222</td>\n",
       "      <td>54.211250</td>\n",
       "      <td>so everyday i had to try to keep my mind on something besides the phone</td>\n",
       "      <td>so i had been to my own hand on to the to my own to the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>1257.029526</td>\n",
       "      <td>45.988393</td>\n",
       "      <td>the feature of rolling hills also affects the cyclist</td>\n",
       "      <td>the man who the of the white fang of the of the which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>980.760771</td>\n",
       "      <td>101.954429</td>\n",
       "      <td>the hibiscus does these actions by coming out of its buds and blooming</td>\n",
       "      <td>the and all its and does that its own and out of its and of its</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>423.573330</td>\n",
       "      <td>69.036768</td>\n",
       "      <td>when i got home i had learn two lessons one was not to worry about anything and i learn how to have patients</td>\n",
       "      <td>when he was to the other i had i had been not i knew and i had been not to the other more than and i could and the other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>330.456966</td>\n",
       "      <td>67.816189</td>\n",
       "      <td>i know that after a long time on the computer i have a massive headache because my eyes just hurt so bad</td>\n",
       "      <td>i know that i know i know i know so much that i know a long on my head on the long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>827.701177</td>\n",
       "      <td>62.681532</td>\n",
       "      <td>nature itself also seems to &lt;caps&gt; a &lt;caps&gt; the winds above the building were very violent</td>\n",
       "      <td>thus the and to the which were very well to the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>907.726414</td>\n",
       "      <td>49.519487</td>\n",
       "      <td>in the story do not exceed posted speed limit the cyclist sees many different settings when he left it was early summer and it was pretty hot outside</td>\n",
       "      <td>in when he was not so he was not even he was not many times and the other it was not in the other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>2061.942170</td>\n",
       "      <td>157.430043</td>\n",
       "      <td>in conclusion you live a happier life when explore the</td>\n",
       "      <td>in when you do when you do a life of when the life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1226.238892</td>\n",
       "      <td>74.116477</td>\n",
       "      <td>the human race has survived over hundreds of years without computers and we can survive hundreds more</td>\n",
       "      <td>the men and we can no more than and more than of the years and more than of the years of the united and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>2550.839142</td>\n",
       "      <td>635.986420</td>\n",
       "      <td>libraries are usually pretty good at sorting things</td>\n",
       "      <td>such are good at good are good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>1667.006319</td>\n",
       "      <td>574.505827</td>\n",
       "      <td>the other practical reason why dirigibles could not more at the empire state building was an existing law against airships flying too low over urban areas</td>\n",
       "      <td>the did not was an existing law against an existing law against an existing law against airships would not even more than any one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>360.025208</td>\n",
       "      <td>60.302671</td>\n",
       "      <td>the steel frame would have to be modified and strengthened</td>\n",
       "      <td>the would have been to the and would be paid and to the ground and its</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>542.009250</td>\n",
       "      <td>39.261224</td>\n",
       "      <td>this is where some kids go because they are addicted to the computer</td>\n",
       "      <td>this they are not go to go to the of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>723.978158</td>\n",
       "      <td>72.689014</td>\n",
       "      <td>it is basically any information in a click</td>\n",
       "      <td>it is a relation in any alternate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>499.350265</td>\n",
       "      <td>45.673305</td>\n",
       "      <td>for one the machines use up load of energy in the house making the electric bill rise and &lt;date&gt;</td>\n",
       "      <td>for one of the and the in the city and the of the city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>570.628212</td>\n",
       "      <td>109.817339</td>\n",
       "      <td>at first none of us really skated cause we were looking around</td>\n",
       "      <td>at least we were no one of the of us we were no one of us of us of us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>948.893445</td>\n",
       "      <td>48.551498</td>\n",
       "      <td>we hardly ever see kids with friends and family hanging around</td>\n",
       "      <td>we all with the and with his own and with the and all of his own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>638.058207</td>\n",
       "      <td>30.625649</td>\n",
       "      <td>he called it a substance it had a flavor of battery acid</td>\n",
       "      <td>he had been a little of a man it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>485.969761</td>\n",
       "      <td>139.408003</td>\n",
       "      <td>this affected him a lot because the desert is very hot</td>\n",
       "      <td>this very little but the him is a little water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>98.255765</td>\n",
       "      <td>83.834317</td>\n",
       "      <td>we were going to &lt;location&gt; that &lt;caps&gt; when all of a sudden the &lt;caps&gt; sign came up on the &lt;caps&gt;</td>\n",
       "      <td>we came on when we were to when we were all to the of a of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>1399.538719</td>\n",
       "      <td>163.137864</td>\n",
       "      <td>we were running suicides</td>\n",
       "      <td>we they were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>541.468522</td>\n",
       "      <td>103.616829</td>\n",
       "      <td>and to make things worse there was this annoying little boy talking to me about some food or something</td>\n",
       "      <td>and there was to me or no longer to me or even about this was no longer of some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>299.209189</td>\n",
       "      <td>31.721332</td>\n",
       "      <td>you have to want things bad enough and put the effort into it</td>\n",
       "      <td>you do nt get it and the way to the it and it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1564.997506</td>\n",
       "      <td>82.377836</td>\n",
       "      <td>though that most libraries split a lot of there sections up by ages</td>\n",
       "      <td>but that there is a few of which there of and up of a few of which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>116.446454</td>\n",
       "      <td>50.639326</td>\n",
       "      <td>i got out a piece of play doe and i made it into the &lt;caps&gt; i put it on the top of the tower</td>\n",
       "      <td>i got and i got a on the of the and i had made it of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>49.131490</td>\n",
       "      <td>24.010395</td>\n",
       "      <td>it was a very good statement</td>\n",
       "      <td>it was a good a good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>1313.372109</td>\n",
       "      <td>118.414855</td>\n",
       "      <td>a lot of people are religious and their particular religion &lt;month&gt; be against looking at suggestive and unholy material</td>\n",
       "      <td>a and at each other people are not are not be no any other kinds of their own knowledge and at their own knowledge and at and at their own knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>257.144343</td>\n",
       "      <td>87.898236</td>\n",
       "      <td>a library should have a variety of books</td>\n",
       "      <td>a of such a should have may should be such a certain of such a certain of a certain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>1325.727417</td>\n",
       "      <td>58.239437</td>\n",
       "      <td>we left my mom and headed to the first store</td>\n",
       "      <td>we came to my and the and the other the night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>341.425682</td>\n",
       "      <td>80.445854</td>\n",
       "      <td>some movies are really offensive and they should be removed from the shelf</td>\n",
       "      <td>some and they should be no longer from the and should be removed and the which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>383.719745</td>\n",
       "      <td>63.652561</td>\n",
       "      <td>only to quickly realize that it was only her fist</td>\n",
       "      <td>only was to her hand that it was no one to her it was that it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>305.784700</td>\n",
       "      <td>54.745153</td>\n",
       "      <td>i heard a couple nights ago that &lt;organization&gt; had an earthquake</td>\n",
       "      <td>i had been an that he had been an old an hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>148.163990</td>\n",
       "      <td>108.185307</td>\n",
       "      <td>a computer is a great thing to have for all these things</td>\n",
       "      <td>a for all of the and all a great thing to all things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>1737.729518</td>\n",
       "      <td>55.314201</td>\n",
       "      <td>the first problem they faced was redesigning the buildings original skeleton to fit the needs of the mooring mast</td>\n",
       "      <td>the last was they had to the of the of the of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>637.240881</td>\n",
       "      <td>60.786237</td>\n",
       "      <td>all loyalties aside in my honest opinion i believe censorship in the &lt;caps&gt; is a bad idea and will never work to solve actual problems with the world</td>\n",
       "      <td>all the and i may i think to my own opinion is not with the world in a great world and with the world to the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>625.256345</td>\n",
       "      <td>42.246590</td>\n",
       "      <td>i think they should just make sure the kids are checking out what their allowed to read</td>\n",
       "      <td>i think that they should make to make the and the of their</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>130.568066</td>\n",
       "      <td>78.129123</td>\n",
       "      <td>they want to sell and advertise their product and they have a right to do so</td>\n",
       "      <td>they should so much to their and so and to do so and to their own and so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>211.132560</td>\n",
       "      <td>50.629581</td>\n",
       "      <td>this showed how it affected him because he thought he is going to drop from the heat</td>\n",
       "      <td>this was he seemed to him and he found he found from the to the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>293.973794</td>\n",
       "      <td>16.317459</td>\n",
       "      <td>he was a guy of color and he was complaining because he read a book about slaves</td>\n",
       "      <td>he was a man and he was he was a man of the and he was a man of a man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>182.816458</td>\n",
       "      <td>67.317436</td>\n",
       "      <td>this form of bulling has happened to many and we have lost many to it</td>\n",
       "      <td>this has we have been found and we have been to many and we have been found it of it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>539.108789</td>\n",
       "      <td>138.852144</td>\n",
       "      <td>they could us that money and buy better books and at a lower price</td>\n",
       "      <td>they could at a and at and that and at a good and at a and at us and that and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>4044.322503</td>\n",
       "      <td>185.894541</td>\n",
       "      <td>censoring these books can give an ease to kids and parents</td>\n",
       "      <td>&lt; p &gt; p &gt; to make an such an and an and an other an other an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>1633.287639</td>\n",
       "      <td>86.445822</td>\n",
       "      <td>you also read about new subjects and learn new things on computers possibly helping you in your classes or</td>\n",
       "      <td>you may you or any other of the new and on or in your own and in on and with and in and the other of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>181.789263</td>\n",
       "      <td>64.810319</td>\n",
       "      <td>they can help us in school and give information we need to do things in life</td>\n",
       "      <td>they can do to make us and we can make the work in life and our life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>105.479998</td>\n",
       "      <td>74.750426</td>\n",
       "      <td>but then i do see it the other way to</td>\n",
       "      <td>but then he do to see it i do it the way of it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>288.736308</td>\n",
       "      <td>138.395285</td>\n",
       "      <td>were part of a movement that will last forever</td>\n",
       "      <td>they will a of that will the last day of a week of which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>190.535639</td>\n",
       "      <td>50.716945</td>\n",
       "      <td>&lt;caps&gt; was last year when we went looking for a dog to adopt</td>\n",
       "      <td>&lt;num&gt; was when we went to a man for when we came to a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>1061.461716</td>\n",
       "      <td>68.017508</td>\n",
       "      <td>many features of the setting affected the cyclist</td>\n",
       "      <td>other the of the of the dogs of the dogs of the of the other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>326.781090</td>\n",
       "      <td>47.604413</td>\n",
       "      <td>they are having a positive effect on people</td>\n",
       "      <td>they are no longer on a certain which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>227.011267</td>\n",
       "      <td>68.025145</td>\n",
       "      <td>one of the teachers tried to help me by telling me that they were laughing at my joke</td>\n",
       "      <td>one of the of the to me they were at him to me that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>2054.252542</td>\n",
       "      <td>131.725294</td>\n",
       "      <td>the architects of the empire state building ran into many problems attempting to allow dirigibles to dock their</td>\n",
       "      <td>the men of the men to their men and their to their feet of their other their other their feet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>337.288633</td>\n",
       "      <td>152.305297</td>\n",
       "      <td>you also can see it from the way she acts</td>\n",
       "      <td>you can she will she will she will she will she will she will she said that it and the way from its way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1111.514086</td>\n",
       "      <td>47.518364</td>\n",
       "      <td>these obstacles were overlooked in the plans to create the empire state building</td>\n",
       "      <td>these of the in to the city to the city and the other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>1732.553963</td>\n",
       "      <td>88.069953</td>\n",
       "      <td>imagine you working on a project in school</td>\n",
       "      <td>&lt; p &gt; p &gt; p &gt; in a on a week on the in a new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>1592.399370</td>\n",
       "      <td>176.473670</td>\n",
       "      <td>&lt;caps&gt; people use computers for work</td>\n",
       "      <td>&lt;num&gt; for each and for work for work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>517.747634</td>\n",
       "      <td>188.400909</td>\n",
       "      <td>like the time where i met this guy named &lt;person&gt;</td>\n",
       "      <td>like i said i met this time i met this i met the of this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>584.096375</td>\n",
       "      <td>75.798493</td>\n",
       "      <td>i had missed about an inning of the game already</td>\n",
       "      <td>i had been an about an of the of the of an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>279.189413</td>\n",
       "      <td>78.757911</td>\n",
       "      <td>you can also use the computer to look up different kinds of people</td>\n",
       "      <td>you will look to the other of which and of the other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>503.292554</td>\n",
       "      <td>150.884318</td>\n",
       "      <td>i will never forget how my parents turned this simple house into a home</td>\n",
       "      <td>i will my house into a house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1076.980960</td>\n",
       "      <td>146.520470</td>\n",
       "      <td>i once spent &lt;num&gt; hours on my computer on &lt;caps&gt;</td>\n",
       "      <td>i once on my on my</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ppl1        ppl2  \\\n",
       "995   482.787222   54.211250    \n",
       "318   1257.029526  45.988393    \n",
       "443   980.760771   101.954429   \n",
       "803   423.573330   69.036768    \n",
       "525   330.456966   67.816189    \n",
       "1412  827.701177   62.681532    \n",
       "1575  907.726414   49.519487    \n",
       "435   2061.942170  157.430043   \n",
       "572   1226.238892  74.116477    \n",
       "1581  2550.839142  635.986420   \n",
       "642   1667.006319  574.505827   \n",
       "1673  360.025208   60.302671    \n",
       "1150  542.009250   39.261224    \n",
       "1256  723.978158   72.689014    \n",
       "640   499.350265   45.673305    \n",
       "399   570.628212   109.817339   \n",
       "1113  948.893445   48.551498    \n",
       "115   638.058207   30.625649    \n",
       "1174  485.969761   139.408003   \n",
       "1483  98.255765    83.834317    \n",
       "1148  1399.538719  163.137864   \n",
       "820   541.468522   103.616829   \n",
       "1356  299.209189   31.721332    \n",
       "207   1564.997506  82.377836    \n",
       "861   116.446454   50.639326    \n",
       "250   49.131490    24.010395    \n",
       "446   1313.372109  118.414855   \n",
       "1540  257.144343   87.898236    \n",
       "1027  1325.727417  58.239437    \n",
       "865   341.425682   80.445854    \n",
       "...          ...         ...    \n",
       "32    383.719745   63.652561    \n",
       "1182  305.784700   54.745153    \n",
       "958   148.163990   108.185307   \n",
       "600   1737.729518  55.314201    \n",
       "1052  637.240881   60.786237    \n",
       "234   625.256345   42.246590    \n",
       "757   130.568066   78.129123    \n",
       "188   211.132560   50.629581    \n",
       "1635  293.973794   16.317459    \n",
       "526   182.816458   67.317436    \n",
       "1033  539.108789   138.852144   \n",
       "1582  4044.322503  185.894541   \n",
       "1612  1633.287639  86.445822    \n",
       "257   181.789263   64.810319    \n",
       "1072  105.479998   74.750426    \n",
       "312   288.736308   138.395285   \n",
       "1473  190.535639   50.716945    \n",
       "936   1061.461716  68.017508    \n",
       "1566  326.781090   47.604413    \n",
       "1527  227.011267   68.025145    \n",
       "681   2054.252542  131.725294   \n",
       "1074  337.288633   152.305297   \n",
       "986   1111.514086  47.518364    \n",
       "953   1732.553963  88.069953    \n",
       "776   1592.399370  176.473670   \n",
       "1345  517.747634   188.400909   \n",
       "1318  584.096375   75.798493    \n",
       "1440  279.189413   78.757911    \n",
       "951   503.292554   150.884318   \n",
       "758   1076.980960  146.520470   \n",
       "\n",
       "                                                                                                                                                           sent1  \\\n",
       "995   so everyday i had to try to keep my mind on something besides the phone                                                                                      \n",
       "318   the feature of rolling hills also affects the cyclist                                                                                                        \n",
       "443   the hibiscus does these actions by coming out of its buds and blooming                                                                                       \n",
       "803   when i got home i had learn two lessons one was not to worry about anything and i learn how to have patients                                                 \n",
       "525   i know that after a long time on the computer i have a massive headache because my eyes just hurt so bad                                                     \n",
       "1412  nature itself also seems to <caps> a <caps> the winds above the building were very violent                                                                   \n",
       "1575  in the story do not exceed posted speed limit the cyclist sees many different settings when he left it was early summer and it was pretty hot outside        \n",
       "435   in conclusion you live a happier life when explore the                                                                                                       \n",
       "572   the human race has survived over hundreds of years without computers and we can survive hundreds more                                                        \n",
       "1581  libraries are usually pretty good at sorting things                                                                                                          \n",
       "642   the other practical reason why dirigibles could not more at the empire state building was an existing law against airships flying too low over urban areas   \n",
       "1673  the steel frame would have to be modified and strengthened                                                                                                   \n",
       "1150  this is where some kids go because they are addicted to the computer                                                                                         \n",
       "1256  it is basically any information in a click                                                                                                                   \n",
       "640   for one the machines use up load of energy in the house making the electric bill rise and <date>                                                             \n",
       "399   at first none of us really skated cause we were looking around                                                                                               \n",
       "1113  we hardly ever see kids with friends and family hanging around                                                                                               \n",
       "115   he called it a substance it had a flavor of battery acid                                                                                                     \n",
       "1174  this affected him a lot because the desert is very hot                                                                                                       \n",
       "1483  we were going to <location> that <caps> when all of a sudden the <caps> sign came up on the <caps>                                                           \n",
       "1148  we were running suicides                                                                                                                                     \n",
       "820   and to make things worse there was this annoying little boy talking to me about some food or something                                                       \n",
       "1356  you have to want things bad enough and put the effort into it                                                                                                \n",
       "207   though that most libraries split a lot of there sections up by ages                                                                                          \n",
       "861   i got out a piece of play doe and i made it into the <caps> i put it on the top of the tower                                                                 \n",
       "250   it was a very good statement                                                                                                                                 \n",
       "446   a lot of people are religious and their particular religion <month> be against looking at suggestive and unholy material                                     \n",
       "1540  a library should have a variety of books                                                                                                                     \n",
       "1027  we left my mom and headed to the first store                                                                                                                 \n",
       "865   some movies are really offensive and they should be removed from the shelf                                                                                   \n",
       "...                                                                          ...                                                                                   \n",
       "32    only to quickly realize that it was only her fist                                                                                                            \n",
       "1182  i heard a couple nights ago that <organization> had an earthquake                                                                                            \n",
       "958   a computer is a great thing to have for all these things                                                                                                     \n",
       "600   the first problem they faced was redesigning the buildings original skeleton to fit the needs of the mooring mast                                            \n",
       "1052  all loyalties aside in my honest opinion i believe censorship in the <caps> is a bad idea and will never work to solve actual problems with the world        \n",
       "234   i think they should just make sure the kids are checking out what their allowed to read                                                                      \n",
       "757   they want to sell and advertise their product and they have a right to do so                                                                                 \n",
       "188   this showed how it affected him because he thought he is going to drop from the heat                                                                         \n",
       "1635  he was a guy of color and he was complaining because he read a book about slaves                                                                             \n",
       "526   this form of bulling has happened to many and we have lost many to it                                                                                        \n",
       "1033  they could us that money and buy better books and at a lower price                                                                                           \n",
       "1582  censoring these books can give an ease to kids and parents                                                                                                   \n",
       "1612  you also read about new subjects and learn new things on computers possibly helping you in your classes or                                                   \n",
       "257   they can help us in school and give information we need to do things in life                                                                                 \n",
       "1072  but then i do see it the other way to                                                                                                                        \n",
       "312   were part of a movement that will last forever                                                                                                               \n",
       "1473  <caps> was last year when we went looking for a dog to adopt                                                                                                 \n",
       "936   many features of the setting affected the cyclist                                                                                                            \n",
       "1566  they are having a positive effect on people                                                                                                                  \n",
       "1527  one of the teachers tried to help me by telling me that they were laughing at my joke                                                                        \n",
       "681   the architects of the empire state building ran into many problems attempting to allow dirigibles to dock their                                              \n",
       "1074  you also can see it from the way she acts                                                                                                                    \n",
       "986   these obstacles were overlooked in the plans to create the empire state building                                                                             \n",
       "953   imagine you working on a project in school                                                                                                                   \n",
       "776   <caps> people use computers for work                                                                                                                         \n",
       "1345  like the time where i met this guy named <person>                                                                                                            \n",
       "1318  i had missed about an inning of the game already                                                                                                             \n",
       "1440  you can also use the computer to look up different kinds of people                                                                                           \n",
       "951   i will never forget how my parents turned this simple house into a home                                                                                      \n",
       "758   i once spent <num> hours on my computer on <caps>                                                                                                            \n",
       "\n",
       "                                                                                                                                                     sent2  \n",
       "995   so i had been to my own hand on to the to my own to the                                                                                               \n",
       "318   the man who the of the white fang of the of the which                                                                                                 \n",
       "443   the and all its and does that its own and out of its and of its                                                                                       \n",
       "803   when he was to the other i had i had been not i knew and i had been not to the other more than and i could and the other                              \n",
       "525   i know that i know i know i know so much that i know a long on my head on the long                                                                    \n",
       "1412  thus the and to the which were very well to the                                                                                                       \n",
       "1575  in when he was not so he was not even he was not many times and the other it was not in the other                                                     \n",
       "435   in when you do when you do a life of when the life                                                                                                    \n",
       "572   the men and we can no more than and more than of the years and more than of the years of the united and                                               \n",
       "1581  such are good at good are good                                                                                                                        \n",
       "642   the did not was an existing law against an existing law against an existing law against airships would not even more than any one                     \n",
       "1673  the would have been to the and would be paid and to the ground and its                                                                                \n",
       "1150  this they are not go to go to the of the                                                                                                              \n",
       "1256  it is a relation in any alternate                                                                                                                     \n",
       "640   for one of the and the in the city and the of the city                                                                                                \n",
       "399   at least we were no one of the of us we were no one of us of us of us                                                                                 \n",
       "1113  we all with the and with his own and with the and all of his own                                                                                      \n",
       "115   he had been a little of a man it                                                                                                                      \n",
       "1174  this very little but the him is a little water                                                                                                        \n",
       "1483  we came on when we were to when we were all to the of a of the                                                                                        \n",
       "1148  we they were                                                                                                                                          \n",
       "820   and there was to me or no longer to me or even about this was no longer of some                                                                       \n",
       "1356  you do nt get it and the way to the it and it                                                                                                         \n",
       "207   but that there is a few of which there of and up of a few of which                                                                                    \n",
       "861   i got and i got a on the of the and i had made it of the                                                                                              \n",
       "250   it was a good a good                                                                                                                                  \n",
       "446   a and at each other people are not are not be no any other kinds of their own knowledge and at their own knowledge and at and at their own knowledge  \n",
       "1540  a of such a should have may should be such a certain of such a certain of a certain                                                                   \n",
       "1027  we came to my and the and the other the night                                                                                                         \n",
       "865   some and they should be no longer from the and should be removed and the which                                                                        \n",
       "...                                                                              ...                                                                        \n",
       "32    only was to her hand that it was no one to her it was that it                                                                                         \n",
       "1182  i had been an that he had been an old an hour                                                                                                         \n",
       "958   a for all of the and all a great thing to all things                                                                                                  \n",
       "600   the last was they had to the of the of the of the                                                                                                     \n",
       "1052  all the and i may i think to my own opinion is not with the world in a great world and with the world to the                                          \n",
       "234   i think that they should make to make the and the of their                                                                                            \n",
       "757   they should so much to their and so and to do so and to their own and so                                                                              \n",
       "188   this was he seemed to him and he found he found from the to the                                                                                       \n",
       "1635  he was a man and he was he was a man of the and he was a man of a man                                                                                 \n",
       "526   this has we have been found and we have been to many and we have been found it of it                                                                  \n",
       "1033  they could at a and at and that and at a good and at a and at us and that and                                                                         \n",
       "1582  < p > p > to make an such an and an and an other an other an                                                                                          \n",
       "1612  you may you or any other of the new and on or in your own and in on and with and in and the other of the                                              \n",
       "257   they can do to make us and we can make the work in life and our life                                                                                  \n",
       "1072  but then he do to see it i do it the way of it                                                                                                        \n",
       "312   they will a of that will the last day of a week of which                                                                                              \n",
       "1473  <num> was when we went to a man for when we came to a                                                                                                 \n",
       "936   other the of the of the dogs of the dogs of the of the other                                                                                          \n",
       "1566  they are no longer on a certain which                                                                                                                 \n",
       "1527  one of the of the to me they were at him to me that                                                                                                   \n",
       "681   the men of the men to their men and their to their feet of their other their other their feet                                                         \n",
       "1074  you can she will she will she will she will she will she will she said that it and the way from its way                                               \n",
       "986   these of the in to the city to the city and the other                                                                                                 \n",
       "953   < p > p > p > in a on a week on the in a new york                                                                                                     \n",
       "776   <num> for each and for work for work                                                                                                                  \n",
       "1345  like i said i met this time i met this i met the of this                                                                                              \n",
       "1318  i had been an about an of the of the of an                                                                                                            \n",
       "1440  you will look to the other of which and of the other                                                                                                  \n",
       "951   i will my house into a house                                                                                                                          \n",
       "758   i once on my on my                                                                                                                                    \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "dftag.sample(frac=1).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.5,\n",
       " 5.2,\n",
       " 2.8,\n",
       " -2.7,\n",
       " 1.3,\n",
       " 7.2,\n",
       " 0.5,\n",
       " 5.6,\n",
       " 2.4,\n",
       " 1.3,\n",
       " 2.1,\n",
       " 4.4,\n",
       " -1.5,\n",
       " 6.0,\n",
       " 4.8,\n",
       " 3.2,\n",
       " 3.6,\n",
       " 4.8,\n",
       " 1.3,\n",
       " 1.3,\n",
       " 3.6,\n",
       " 1.3,\n",
       " 1.3,\n",
       " 0.1,\n",
       " 5.2,\n",
       " -1.2,\n",
       " -0.8,\n",
       " 1.7,\n",
       " 5.6,\n",
       " -0.4,\n",
       " 1.3,\n",
       " 5.2,\n",
       " 3.6,\n",
       " 2.1,\n",
       " 0.9,\n",
       " 2.8,\n",
       " 5.2,\n",
       " 0.9,\n",
       " 4.0,\n",
       " 2.9,\n",
       " 5.6,\n",
       " 7.1,\n",
       " 6.4,\n",
       " 3.3,\n",
       " 4.8,\n",
       " 8.0,\n",
       " 4.8,\n",
       " -0.4,\n",
       " 0.5,\n",
       " -1.5,\n",
       " 1.3,\n",
       " 5.2,\n",
       " -1.5,\n",
       " -0.4,\n",
       " 3.3,\n",
       " 0.5,\n",
       " -0.4,\n",
       " 2.8,\n",
       " 3.2,\n",
       " 10.6,\n",
       " -1.9,\n",
       " 4.8,\n",
       " 4.4,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1.7,\n",
       " 5.6,\n",
       " 0.9,\n",
       " 0.5,\n",
       " 2.1,\n",
       " 4.4,\n",
       " 2.1,\n",
       " 2.5,\n",
       " 7.9,\n",
       " 3.3,\n",
       " -2.7,\n",
       " 6.0,\n",
       " 2.1,\n",
       " 5.6,\n",
       " 0.9,\n",
       " 4.0,\n",
       " 2.1,\n",
       " 4.4,\n",
       " 10.3,\n",
       " 8.3,\n",
       " 3.2,\n",
       " 6.0,\n",
       " 1.7,\n",
       " 8.0,\n",
       " 0.9,\n",
       " 0.5,\n",
       " 4.0,\n",
       " 2.9,\n",
       " 3.2,\n",
       " 2.8,\n",
       " 4.8,\n",
       " 0.1,\n",
       " -0.4,\n",
       " 5.2,\n",
       " 0.9,\n",
       " 3.2,\n",
       " 3.6,\n",
       " 6.0,\n",
       " 2.5,\n",
       " -0.4,\n",
       " 3.6,\n",
       " 4.0,\n",
       " 2.4,\n",
       " -1.2,\n",
       " -1.5,\n",
       " 3.6,\n",
       " 4.0,\n",
       " 9.9,\n",
       " 2.8,\n",
       " 3.6,\n",
       " 0.9,\n",
       " 3.3,\n",
       " 3.2,\n",
       " 2.1,\n",
       " 1.7,\n",
       " 3.2,\n",
       " 4.8,\n",
       " -1.2,\n",
       " 0.5,\n",
       " 6.0,\n",
       " 0.1,\n",
       " 4.8,\n",
       " 3.2,\n",
       " 7.1,\n",
       " -0.4,\n",
       " 2.1,\n",
       " 2.1,\n",
       " 2.4,\n",
       " 6.4,\n",
       " 0.5,\n",
       " -0.4,\n",
       " 4.8,\n",
       " -2.7,\n",
       " 3.6,\n",
       " 2.1,\n",
       " 2.9,\n",
       " 5.2,\n",
       " 4.4,\n",
       " 0.9,\n",
       " -1.9,\n",
       " 0.5,\n",
       " 2.1,\n",
       " -0.4,\n",
       " -3.5,\n",
       " 0.9,\n",
       " 2.1,\n",
       " 5.6,\n",
       " 3.6,\n",
       " 1.7,\n",
       " -1.2,\n",
       " 1.7,\n",
       " 3.2,\n",
       " 3.6,\n",
       " 3.2,\n",
       " 5.6,\n",
       " -2.7,\n",
       " 5.6,\n",
       " -2.3,\n",
       " 4.8,\n",
       " 5.2,\n",
       " 6.4,\n",
       " 10.6,\n",
       " -1.2,\n",
       " 1.3,\n",
       " 5.6,\n",
       " 11.4,\n",
       " 6.8,\n",
       " 2.5,\n",
       " 7.1,\n",
       " 2.9,\n",
       " 3.2,\n",
       " 4.8,\n",
       " 6.0,\n",
       " 0.5,\n",
       " 3.3,\n",
       " -1.5,\n",
       " 7.5,\n",
       " 4.4,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 0.9,\n",
       " 4.0,\n",
       " 5.6,\n",
       " 2.1,\n",
       " -0.4,\n",
       " -2.3,\n",
       " 2.5,\n",
       " 4.4,\n",
       " 3.2,\n",
       " -0.8,\n",
       " 2.8,\n",
       " 3.2,\n",
       " 8.7,\n",
       " 1.7,\n",
       " 3.7,\n",
       " 0.9,\n",
       " 6.0,\n",
       " 1.3,\n",
       " 11.4,\n",
       " 2.4,\n",
       " 5.2,\n",
       " 2.1,\n",
       " 2.8,\n",
       " 3.2,\n",
       " 0.5,\n",
       " -2.7,\n",
       " 5.6,\n",
       " -1.5,\n",
       " 2.8,\n",
       " 6.7,\n",
       " -0.4,\n",
       " 5.6,\n",
       " 2.4,\n",
       " 3.6,\n",
       " -0.4,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 2.1,\n",
       " 0.9,\n",
       " 6.4,\n",
       " 6.7,\n",
       " -1.5,\n",
       " 3.2,\n",
       " 0.1,\n",
       " -0.8,\n",
       " 0.9,\n",
       " 4.8,\n",
       " 1.3,\n",
       " 1.7,\n",
       " 1.3,\n",
       " 0.1,\n",
       " 3.3,\n",
       " -0.8,\n",
       " 0.9,\n",
       " 5.2,\n",
       " -1.5,\n",
       " 1.3,\n",
       " 5.2,\n",
       " 0.9,\n",
       " 0.1,\n",
       " 3.2,\n",
       " 7.2,\n",
       " 4.4,\n",
       " 2.1,\n",
       " 4.8,\n",
       " -1.5,\n",
       " -2.3,\n",
       " 6.8,\n",
       " 3.6,\n",
       " -1.5,\n",
       " 6.4,\n",
       " -2.3,\n",
       " 2.8,\n",
       " 4.8,\n",
       " 15.8,\n",
       " 7.9,\n",
       " 3.6,\n",
       " 6.4,\n",
       " -1.5,\n",
       " 7.2,\n",
       " 9.5,\n",
       " -0.4,\n",
       " 6.0,\n",
       " 3.2,\n",
       " 3.2,\n",
       " 4.4,\n",
       " 4.0,\n",
       " 2.1,\n",
       " 2.1,\n",
       " 1.3,\n",
       " 3.6,\n",
       " 3.6,\n",
       " -1.2,\n",
       " 1.7,\n",
       " 1.7,\n",
       " 5.2,\n",
       " 7.2,\n",
       " 2.1,\n",
       " 0.5,\n",
       " 8.3,\n",
       " 5.2,\n",
       " 3.6,\n",
       " 6.4,\n",
       " 4.4,\n",
       " 11.0,\n",
       " 2.4,\n",
       " 1.7,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 3.6,\n",
       " 3.3,\n",
       " -1.2,\n",
       " -1.2,\n",
       " 5.2,\n",
       " 0.5,\n",
       " 2.5,\n",
       " 0.5,\n",
       " 1.3,\n",
       " 5.2,\n",
       " 0.1,\n",
       " 3.2,\n",
       " 0.9,\n",
       " -0.8,\n",
       " 1.3,\n",
       " 2.1,\n",
       " 1.7,\n",
       " 6.4,\n",
       " 1.7,\n",
       " 3.6,\n",
       " 2.9,\n",
       " -1.2,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 1.3,\n",
       " -0.4,\n",
       " -2.7,\n",
       " 6.4,\n",
       " 5.6,\n",
       " -1.2,\n",
       " -1.9,\n",
       " 0.1,\n",
       " 2.9,\n",
       " 4.4,\n",
       " 3.2,\n",
       " 4.8,\n",
       " 2.4,\n",
       " 4.4,\n",
       " -0.4,\n",
       " -1.9,\n",
       " 0.9,\n",
       " -1.9,\n",
       " 4.0,\n",
       " 9.9,\n",
       " 4.0,\n",
       " 0.1,\n",
       " -1.2,\n",
       " -2.7,\n",
       " 4.4,\n",
       " 0.5,\n",
       " 6.7,\n",
       " 4.0,\n",
       " 0.5,\n",
       " 2.4,\n",
       " 4.8,\n",
       " 0.1,\n",
       " 10.7,\n",
       " 3.6,\n",
       " 4.1,\n",
       " 0.5,\n",
       " 1.7,\n",
       " 2.5,\n",
       " 10.3,\n",
       " 7.6,\n",
       " 1.3,\n",
       " 6.0,\n",
       " 7.5,\n",
       " 9.1,\n",
       " 2.1,\n",
       " 1.7,\n",
       " 2.8,\n",
       " 4.4,\n",
       " 6.4,\n",
       " 2.1,\n",
       " 1.3,\n",
       " 0.5,\n",
       " -1.2,\n",
       " 8.3,\n",
       " 2.1,\n",
       " 1.3,\n",
       " 8.0,\n",
       " 1.3,\n",
       " -1.5,\n",
       " 1.7,\n",
       " -0.8,\n",
       " 3.6,\n",
       " -0.4,\n",
       " 4.8,\n",
       " 3.3,\n",
       " 1.7,\n",
       " 3.2,\n",
       " 8.7,\n",
       " 2.1,\n",
       " 7.9,\n",
       " -1.5,\n",
       " 2.1,\n",
       " 4.8,\n",
       " 2.8,\n",
       " 2.1,\n",
       " 3.6,\n",
       " 2.1,\n",
       " 0.5,\n",
       " 2.9,\n",
       " 2.8,\n",
       " -1.5,\n",
       " 4.0,\n",
       " -0.8,\n",
       " 0.1,\n",
       " 2.1,\n",
       " 4.8,\n",
       " 2.8,\n",
       " 0.9,\n",
       " 0.1,\n",
       " 5.2,\n",
       " 4.0,\n",
       " 3.6,\n",
       " 2.8,\n",
       " 1.7,\n",
       " 4.0,\n",
       " 0.5,\n",
       " -2.3,\n",
       " 10.6,\n",
       " 1.7,\n",
       " -1.2,\n",
       " 3.6,\n",
       " -1.9,\n",
       " 0.1,\n",
       " 10.3,\n",
       " 0.9,\n",
       " 0.9,\n",
       " -0.4,\n",
       " 10.3,\n",
       " 2.8,\n",
       " 1.3,\n",
       " 2.9,\n",
       " 2.1,\n",
       " -2.7,\n",
       " 2.5,\n",
       " 1.3,\n",
       " 1.3,\n",
       " 2.8,\n",
       " 1.3,\n",
       " 1.3,\n",
       " 5.2,\n",
       " 2.4,\n",
       " 0.9,\n",
       " 2.4,\n",
       " 2.1,\n",
       " 2.1,\n",
       " 2.4,\n",
       " 3.6,\n",
       " 6.7,\n",
       " 10.7,\n",
       " 3.7,\n",
       " 2.1,\n",
       " 4.4,\n",
       " 11.4,\n",
       " 4.8,\n",
       " 4.8,\n",
       " 2.1,\n",
       " 2.1,\n",
       " 11.5,\n",
       " -3.1,\n",
       " 3.2,\n",
       " -1.2,\n",
       " 4.8,\n",
       " 4.0,\n",
       " 4.8,\n",
       " 1.7,\n",
       " 4.8,\n",
       " 1.7,\n",
       " 0.9,\n",
       " 2.8,\n",
       " 3.2,\n",
       " 1.7,\n",
       " 1.3,\n",
       " -1.9,\n",
       " 6.0,\n",
       " 1.3,\n",
       " -1.2,\n",
       " 0.9,\n",
       " 9.5,\n",
       " -3.1,\n",
       " -0.8,\n",
       " 2.4,\n",
       " -2.3,\n",
       " -0.4,\n",
       " 6.8,\n",
       " 3.6,\n",
       " 2.1,\n",
       " 2.9,\n",
       " 4.0,\n",
       " 4.8,\n",
       " -0.8,\n",
       " 4.4,\n",
       " 0.9,\n",
       " 10.6,\n",
       " 5.2,\n",
       " 1.7,\n",
       " 10.6,\n",
       " 4.4,\n",
       " 2.8,\n",
       " 0.9,\n",
       " 4.0,\n",
       " 0.9,\n",
       " 2.4,\n",
       " 5.6,\n",
       " 1.7,\n",
       " 0.9,\n",
       " 4.4,\n",
       " 3.6,\n",
       " -2.7,\n",
       " 0.5,\n",
       " 11.8,\n",
       " 4.8,\n",
       " 2.1,\n",
       " 5.6,\n",
       " 2.1,\n",
       " 0.5,\n",
       " -0.4,\n",
       " 4.4,\n",
       " 6.7,\n",
       " 8.7,\n",
       " 5.2,\n",
       " -0.8,\n",
       " 3.3,\n",
       " 11.4,\n",
       " 4.8,\n",
       " 1.3,\n",
       " 5.6,\n",
       " 4.8,\n",
       " 4.8,\n",
       " 4.0,\n",
       " -0.8,\n",
       " 2.4,\n",
       " 4.0,\n",
       " 0.1,\n",
       " 4.8,\n",
       " 9.1,\n",
       " 4.4,\n",
       " 2.9,\n",
       " 1.7,\n",
       " 3.2,\n",
       " 4.8,\n",
       " 3.2,\n",
       " -2.7,\n",
       " 0.5,\n",
       " -1.5,\n",
       " 3.2,\n",
       " 11.1,\n",
       " 7.1,\n",
       " 4.4,\n",
       " 6.8,\n",
       " 0.9,\n",
       " 1.3,\n",
       " 1.7,\n",
       " 4.8,\n",
       " -1.2,\n",
       " 4.0,\n",
       " 6.8,\n",
       " 2.1,\n",
       " -2.7,\n",
       " 3.3,\n",
       " 2.1,\n",
       " 3.6,\n",
       " 4.4,\n",
       " 1.7,\n",
       " 4.4,\n",
       " 4.8,\n",
       " 6.8,\n",
       " 6.4,\n",
       " 9.5,\n",
       " 1.3,\n",
       " 4.0,\n",
       " 2.4,\n",
       " 4.4,\n",
       " 2.5,\n",
       " 5.2,\n",
       " 5.6,\n",
       " 4.0,\n",
       " 1.7,\n",
       " 2.1,\n",
       " -0.8,\n",
       " 11.4,\n",
       " 5.2,\n",
       " 1.3,\n",
       " 0.9,\n",
       " 4.0,\n",
       " 0.5,\n",
       " 4.4,\n",
       " 0.9,\n",
       " 2.1,\n",
       " 11.8,\n",
       " 2.9,\n",
       " 8.3,\n",
       " 5.6,\n",
       " 5.2,\n",
       " -1.2,\n",
       " 5.2,\n",
       " 4.0,\n",
       " 5.2,\n",
       " -3.5,\n",
       " -2.7,\n",
       " 3.6,\n",
       " 4.8,\n",
       " 2.9,\n",
       " 1.3,\n",
       " 11.0,\n",
       " 2.5,\n",
       " 2.4,\n",
       " 4.4,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 0.1,\n",
       " 2.9,\n",
       " 1.7,\n",
       " 6.4,\n",
       " 4.0,\n",
       " 1.7,\n",
       " -1.2,\n",
       " 2.4,\n",
       " 0.5,\n",
       " 4.0,\n",
       " 3.2,\n",
       " 6.4,\n",
       " 0.1,\n",
       " 2.4,\n",
       " -0.4,\n",
       " 3.6,\n",
       " 3.6,\n",
       " 3.2,\n",
       " 2.5,\n",
       " 7.5,\n",
       " -0.8,\n",
       " -0.8,\n",
       " 6.4,\n",
       " 5.2,\n",
       " 2.5,\n",
       " -1.5,\n",
       " 3.6,\n",
       " 20.2,\n",
       " 6.4,\n",
       " -1.5,\n",
       " 2.5,\n",
       " 0.1,\n",
       " -2.7,\n",
       " 1.7,\n",
       " 5.2,\n",
       " 9.1,\n",
       " 5.2,\n",
       " 5.6,\n",
       " 4.0,\n",
       " 9.9,\n",
       " 6.0,\n",
       " 0.9,\n",
       " 4.4,\n",
       " 3.2,\n",
       " -0.8,\n",
       " 4.0,\n",
       " 2.9,\n",
       " 5.2,\n",
       " 7.5,\n",
       " 4.8,\n",
       " -2.7,\n",
       " 1.7,\n",
       " 4.4,\n",
       " 0.5,\n",
       " 1.3,\n",
       " 2.4,\n",
       " 1.7,\n",
       " 3.2,\n",
       " 1.7,\n",
       " 9.5,\n",
       " 1.3,\n",
       " 11.4,\n",
       " 1.3,\n",
       " 1.3,\n",
       " -0.4,\n",
       " 9.9,\n",
       " 2.1,\n",
       " 8.0,\n",
       " 5.6,\n",
       " 4.8,\n",
       " 1.7,\n",
       " 5.2,\n",
       " 1.7,\n",
       " 4.8,\n",
       " 5.2,\n",
       " 3.7,\n",
       " 1.3,\n",
       " 5.6,\n",
       " 4.8,\n",
       " -1.2,\n",
       " 3.2,\n",
       " 9.1,\n",
       " 2.9,\n",
       " 0.5,\n",
       " 0.9,\n",
       " 1.7,\n",
       " 1.3,\n",
       " 2.5,\n",
       " 2.8,\n",
       " 0.9,\n",
       " 1.7,\n",
       " 4.0,\n",
       " 2.5,\n",
       " 6.7,\n",
       " 0.9,\n",
       " 1.7,\n",
       " 0.9,\n",
       " -0.4,\n",
       " 1.3,\n",
       " 6.0,\n",
       " 0.9,\n",
       " 4.0,\n",
       " 4.4,\n",
       " 2.1,\n",
       " 3.2,\n",
       " 7.9,\n",
       " 6.8,\n",
       " 2.9,\n",
       " 2.1,\n",
       " 4.8,\n",
       " 3.6,\n",
       " -1.2,\n",
       " 5.6,\n",
       " 2.1,\n",
       " 2.8,\n",
       " -0.8,\n",
       " -1.2,\n",
       " 2.1,\n",
       " -1.9,\n",
       " 0.1,\n",
       " 4.0,\n",
       " 10.6,\n",
       " 3.6,\n",
       " -0.4,\n",
       " 6.0,\n",
       " 9.1,\n",
       " 2.9,\n",
       " -0.4,\n",
       " 5.2,\n",
       " 1.3,\n",
       " 0.1,\n",
       " -0.4,\n",
       " 3.6,\n",
       " 3.2,\n",
       " 5.6,\n",
       " 7.1,\n",
       " 0.9,\n",
       " -2.7,\n",
       " 3.2,\n",
       " 4.0,\n",
       " 0.1,\n",
       " 3.3,\n",
       " 1.3,\n",
       " 2.8,\n",
       " 3.2,\n",
       " 2.1,\n",
       " -0.4,\n",
       " 2.1,\n",
       " 4.4,\n",
       " 2.1,\n",
       " 3.2,\n",
       " -1.5,\n",
       " 3.6,\n",
       " 6.8,\n",
       " 3.3,\n",
       " 2.9,\n",
       " 0.5,\n",
       " 3.2,\n",
       " 4.4,\n",
       " 4.8,\n",
       " 5.6,\n",
       " 2.1,\n",
       " 4.4,\n",
       " 6.8,\n",
       " 2.9,\n",
       " 2.9,\n",
       " 3.6,\n",
       " 6.7,\n",
       " 3.2,\n",
       " -0.8,\n",
       " 1.7,\n",
       " 1.7,\n",
       " -1.9,\n",
       " 3.2,\n",
       " 3.2,\n",
       " 5.6,\n",
       " 0.5,\n",
       " 3.6,\n",
       " -0.4,\n",
       " -0.4,\n",
       " 2.1,\n",
       " 4.0,\n",
       " 2.8,\n",
       " 6.8,\n",
       " -0.4,\n",
       " 6.4,\n",
       " 3.7,\n",
       " 5.2,\n",
       " 0.9,\n",
       " 2.1,\n",
       " 1.7,\n",
       " -2.3,\n",
       " 4.4,\n",
       " 4.8,\n",
       " 1.3,\n",
       " 3.6,\n",
       " 9.1,\n",
       " 5.2,\n",
       " 3.2,\n",
       " -1.2,\n",
       " 2.5,\n",
       " 4.8,\n",
       " -0.4,\n",
       " 2.1,\n",
       " -3.1,\n",
       " 5.2,\n",
       " 9.9,\n",
       " 9.5,\n",
       " 1.3,\n",
       " 0.1,\n",
       " -1.2,\n",
       " 4.0,\n",
       " -2.7,\n",
       " 3.6,\n",
       " 2.4,\n",
       " 3.6,\n",
       " 0.1,\n",
       " 1.7,\n",
       " 5.2,\n",
       " 1.3,\n",
       " 4.4,\n",
       " 7.1,\n",
       " 2.1,\n",
       " 8.3,\n",
       " 2.9,\n",
       " 6.4,\n",
       " 0.9,\n",
       " 0.5,\n",
       " 0.1,\n",
       " 3.2,\n",
       " 0.5,\n",
       " 6.4,\n",
       " 5.6,\n",
       " 4.8,\n",
       " 1.3,\n",
       " 1.3,\n",
       " 3.6,\n",
       " 4.4,\n",
       " 3.6,\n",
       " 4.8,\n",
       " 1.7,\n",
       " 3.6,\n",
       " 5.6,\n",
       " 4.8,\n",
       " 2.8,\n",
       " 2.1,\n",
       " 2.1,\n",
       " 2.5,\n",
       " 5.2,\n",
       " 1.3,\n",
       " 7.5,\n",
       " 5.6,\n",
       " -1.5,\n",
       " 5.6,\n",
       " 2.8,\n",
       " 4.0,\n",
       " -0.4,\n",
       " 0.9,\n",
       " 3.6,\n",
       " 5.2,\n",
       " 2.4,\n",
       " 3.2,\n",
       " 0.1,\n",
       " 5.2,\n",
       " 2.8,\n",
       " -0.8,\n",
       " 3.2,\n",
       " 0.5,\n",
       " 0.5,\n",
       " -0.4,\n",
       " 6.0,\n",
       " 2.1,\n",
       " 6.7,\n",
       " 2.4,\n",
       " 2.1,\n",
       " 2.8,\n",
       " 11.0,\n",
       " 8.3,\n",
       " -1.2,\n",
       " 5.2,\n",
       " 7.1,\n",
       " 4.8,\n",
       " 3.6,\n",
       " 4.0,\n",
       " 2.8,\n",
       " -2.3,\n",
       " 2.4,\n",
       " 4.4,\n",
       " 4.4,\n",
       " 0.5,\n",
       " 5.2,\n",
       " 3.6,\n",
       " 2.8,\n",
       " -1.9,\n",
       " 3.7,\n",
       " -1.2,\n",
       " 1.7,\n",
       " 3.6,\n",
       " -2.7,\n",
       " 3.2,\n",
       " 2.4,\n",
       " -1.9,\n",
       " -3.1,\n",
       " 4.4,\n",
       " -1.2,\n",
       " 4.4,\n",
       " 1.7,\n",
       " 1.3,\n",
       " 6.0,\n",
       " 3.6,\n",
       " 6.7,\n",
       " 1.3,\n",
       " 6.0,\n",
       " -2.3,\n",
       " 4.8,\n",
       " 3.2,\n",
       " 0.5,\n",
       " 1.3,\n",
       " -2.3,\n",
       " 4.0,\n",
       " 6.4,\n",
       " 11.4,\n",
       " 0.1,\n",
       " 5.6,\n",
       " 2.8,\n",
       " 5.2,\n",
       " 2.1,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 3.2,\n",
       " 3.2,\n",
       " -0.8,\n",
       " 0.5,\n",
       " 5.6,\n",
       " 0.9,\n",
       " 1.7,\n",
       " -0.4,\n",
       " 1.7,\n",
       " 6.4,\n",
       " 4.8,\n",
       " 1.7,\n",
       " 1.3,\n",
       " -1.2,\n",
       " 7.1,\n",
       " 0.1,\n",
       " 2.1,\n",
       " 5.2,\n",
       " 11.8,\n",
       " 1.3,\n",
       " 2.4,\n",
       " 0.1,\n",
       " 1.3,\n",
       " 1.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 3.2,\n",
       " 4.4,\n",
       " 0.9,\n",
       " 5.6,\n",
       " 0.5,\n",
       " 5.6,\n",
       " 0.1,\n",
       " 1.7,\n",
       " 2.9,\n",
       " 0.5,\n",
       " 5.2,\n",
       " 2.1,\n",
       " 3.6,\n",
       " 5.2,\n",
       " 13.4,\n",
       " 4.0,\n",
       " 3.6,\n",
       " 3.2,\n",
       " 5.6,\n",
       " 2.4,\n",
       " 8.3,\n",
       " -0.4,\n",
       " 1.7,\n",
       " -2.7,\n",
       " 2.5,\n",
       " 1.3,\n",
       " 0.9,\n",
       " -1.2,\n",
       " 2.8,\n",
       " 2.4,\n",
       " 0.9,\n",
       " -0.8,\n",
       " 3.3,\n",
       " 2.4,\n",
       " 6.4,\n",
       " 0.5,\n",
       " 1.7,\n",
       " -1.2,\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\n",
    "\n",
    "list(map(textstat.flesch_kincaid_grade, rev_soph_tag_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[116.15,\n",
       " 98.89,\n",
       " 104.98,\n",
       " 119.19,\n",
       " 109.04,\n",
       " 61.33,\n",
       " 111.07,\n",
       " 87.05,\n",
       " 106.0,\n",
       " 109.04,\n",
       " 107.01,\n",
       " 100.92,\n",
       " 116.15,\n",
       " 96.86,\n",
       " 99.91,\n",
       " 103.97,\n",
       " 97.54,\n",
       " 99.91,\n",
       " 103.63,\n",
       " 109.04,\n",
       " 97.54,\n",
       " 109.04,\n",
       " 109.04,\n",
       " 112.09,\n",
       " 98.89,\n",
       " 115.13,\n",
       " 114.12,\n",
       " 108.03,\n",
       " 97.88,\n",
       " 113.1,\n",
       " 109.04,\n",
       " 88.06,\n",
       " 97.54,\n",
       " 107.01,\n",
       " 110.06,\n",
       " 104.98,\n",
       " 93.48,\n",
       " 110.06,\n",
       " 85.69,\n",
       " 94.15,\n",
       " 87.05,\n",
       " 93.82,\n",
       " 79.6,\n",
       " 93.14,\n",
       " 99.91,\n",
       " 48.47,\n",
       " 99.91,\n",
       " 113.1,\n",
       " 111.07,\n",
       " 116.15,\n",
       " 103.63,\n",
       " 93.48,\n",
       " 116.15,\n",
       " 113.1,\n",
       " 87.72,\n",
       " 111.07,\n",
       " 113.1,\n",
       " 104.98,\n",
       " 98.55,\n",
       " 84.68,\n",
       " 117.16,\n",
       " 89.08,\n",
       " 100.92,\n",
       " 111.07,\n",
       " 111.07,\n",
       " 108.03,\n",
       " 97.88,\n",
       " 104.64,\n",
       " 111.07,\n",
       " 101.6,\n",
       " 100.92,\n",
       " 107.01,\n",
       " 89.75,\n",
       " 80.96,\n",
       " 93.14,\n",
       " 119.19,\n",
       " 86.03,\n",
       " 107.01,\n",
       " 97.88,\n",
       " 110.06,\n",
       " 101.94,\n",
       " 101.6,\n",
       " 95.51,\n",
       " 85.7,\n",
       " 79.94,\n",
       " 98.55,\n",
       " 91.45,\n",
       " 108.03,\n",
       " 64.71,\n",
       " 110.06,\n",
       " 111.07,\n",
       " 101.94,\n",
       " 99.57,\n",
       " 98.55,\n",
       " 104.98,\n",
       " 94.49,\n",
       " 112.09,\n",
       " 113.1,\n",
       " 98.89,\n",
       " 110.06,\n",
       " 98.55,\n",
       " 102.95,\n",
       " 80.62,\n",
       " 100.58,\n",
       " 113.1,\n",
       " 97.54,\n",
       " 101.94,\n",
       " 106.0,\n",
       " 115.13,\n",
       " 116.15,\n",
       " 102.95,\n",
       " 101.94,\n",
       " 81.3,\n",
       " 104.98,\n",
       " 102.95,\n",
       " 104.64,\n",
       " 93.14,\n",
       " 103.97,\n",
       " 107.01,\n",
       " 108.03,\n",
       " 103.97,\n",
       " 99.91,\n",
       " 115.13,\n",
       " 105.66,\n",
       " 96.86,\n",
       " 112.09,\n",
       " 99.91,\n",
       " 103.97,\n",
       " 93.82,\n",
       " 113.1,\n",
       " 107.01,\n",
       " 107.01,\n",
       " 106.0,\n",
       " 90.43,\n",
       " 111.07,\n",
       " 113.1,\n",
       " 99.91,\n",
       " 119.19,\n",
       " 102.95,\n",
       " 107.01,\n",
       " 94.15,\n",
       " 71.82,\n",
       " 100.92,\n",
       " 104.64,\n",
       " 117.16,\n",
       " 111.07,\n",
       " 107.01,\n",
       " 113.1,\n",
       " 121.22,\n",
       " 93.81,\n",
       " 96.18,\n",
       " 92.46,\n",
       " 102.95,\n",
       " 108.03,\n",
       " 115.13,\n",
       " 108.03,\n",
       " 98.55,\n",
       " 102.95,\n",
       " 103.97,\n",
       " 97.88,\n",
       " 119.19,\n",
       " 87.05,\n",
       " 118.18,\n",
       " 94.49,\n",
       " 98.89,\n",
       " 79.6,\n",
       " 84.68,\n",
       " 115.13,\n",
       " 109.04,\n",
       " 92.46,\n",
       " 82.65,\n",
       " 56.93,\n",
       " 89.75,\n",
       " 88.4,\n",
       " 99.57,\n",
       " 103.97,\n",
       " 99.91,\n",
       " 96.86,\n",
       " 111.07,\n",
       " 93.14,\n",
       " 116.15,\n",
       " 92.8,\n",
       " 100.92,\n",
       " 96.52,\n",
       " 101.94,\n",
       " 110.06,\n",
       " 91.11,\n",
       " 92.46,\n",
       " 107.01,\n",
       " 113.1,\n",
       " 118.18,\n",
       " 89.75,\n",
       " 95.51,\n",
       " 98.55,\n",
       " 114.12,\n",
       " 104.98,\n",
       " 103.97,\n",
       " 78.93,\n",
       " 102.61,\n",
       " 86.71,\n",
       " 110.06,\n",
       " 96.86,\n",
       " 109.04,\n",
       " 82.65,\n",
       " 106.0,\n",
       " 98.89,\n",
       " 101.6,\n",
       " 104.98,\n",
       " 103.97,\n",
       " 111.07,\n",
       " 119.19,\n",
       " 97.88,\n",
       " 116.15,\n",
       " 104.98,\n",
       " 94.83,\n",
       " 113.1,\n",
       " 92.46,\n",
       " 106.0,\n",
       " 92.12,\n",
       " 113.1,\n",
       " 110.06,\n",
       " 110.06,\n",
       " 107.01,\n",
       " 104.64,\n",
       " 95.85,\n",
       " 94.83,\n",
       " 116.15,\n",
       " 103.97,\n",
       " 112.09,\n",
       " 114.12,\n",
       " 110.06,\n",
       " 94.49,\n",
       " 109.04,\n",
       " 102.61,\n",
       " 109.04,\n",
       " 112.09,\n",
       " 93.14,\n",
       " 114.12,\n",
       " 110.06,\n",
       " 82.65,\n",
       " 116.15,\n",
       " 109.04,\n",
       " 77.23,\n",
       " 110.06,\n",
       " 112.09,\n",
       " 103.97,\n",
       " 77.57,\n",
       " 100.92,\n",
       " 107.01,\n",
       " 99.91,\n",
       " 116.15,\n",
       " 118.18,\n",
       " 78.59,\n",
       " 102.95,\n",
       " 116.15,\n",
       " 95.85,\n",
       " 118.18,\n",
       " 104.98,\n",
       " 83.66,\n",
       " 17.34,\n",
       " 75.54,\n",
       " 97.54,\n",
       " 95.85,\n",
       " 116.15,\n",
       " 82.99,\n",
       " 66.07,\n",
       " 113.1,\n",
       " 69.79,\n",
       " 103.97,\n",
       " 103.97,\n",
       " 84.68,\n",
       " 85.69,\n",
       " 107.01,\n",
       " 101.6,\n",
       " 109.04,\n",
       " 102.95,\n",
       " 97.54,\n",
       " 115.13,\n",
       " 108.03,\n",
       " 102.61,\n",
       " 98.89,\n",
       " 82.99,\n",
       " 107.01,\n",
       " 105.66,\n",
       " 69.11,\n",
       " 98.89,\n",
       " 102.95,\n",
       " 90.43,\n",
       " 95.51,\n",
       " 78.25,\n",
       " 106.0,\n",
       " 108.03,\n",
       " 89.75,\n",
       " 100.58,\n",
       " 102.95,\n",
       " 87.72,\n",
       " 115.13,\n",
       " 115.13,\n",
       " 98.89,\n",
       " 111.07,\n",
       " 100.58,\n",
       " 105.66,\n",
       " 103.63,\n",
       " 77.23,\n",
       " 112.09,\n",
       " 103.97,\n",
       " 104.64,\n",
       " 114.12,\n",
       " 103.63,\n",
       " 101.6,\n",
       " 108.03,\n",
       " 95.85,\n",
       " 108.03,\n",
       " 97.54,\n",
       " 77.91,\n",
       " 115.13,\n",
       " 112.09,\n",
       " 112.09,\n",
       " 109.04,\n",
       " 113.1,\n",
       " 119.19,\n",
       " 74.19,\n",
       " 97.88,\n",
       " 115.13,\n",
       " 117.16,\n",
       " 112.09,\n",
       " 99.57,\n",
       " 90.09,\n",
       " 98.55,\n",
       " 89.08,\n",
       " 106.0,\n",
       " 100.92,\n",
       " 113.1,\n",
       " 117.16,\n",
       " 104.64,\n",
       " 117.16,\n",
       " 101.94,\n",
       " 81.3,\n",
       " 96.52,\n",
       " 112.09,\n",
       " 115.13,\n",
       " 119.19,\n",
       " 100.92,\n",
       " 111.07,\n",
       " 94.83,\n",
       " 101.94,\n",
       " 111.07,\n",
       " 106.0,\n",
       " 94.49,\n",
       " 112.09,\n",
       " 63.02,\n",
       " 92.12,\n",
       " 80.28,\n",
       " 111.07,\n",
       " 102.61,\n",
       " 100.58,\n",
       " 85.7,\n",
       " 76.56,\n",
       " 103.63,\n",
       " 96.86,\n",
       " 92.8,\n",
       " 88.74,\n",
       " 107.01,\n",
       " 108.03,\n",
       " 104.98,\n",
       " 100.92,\n",
       " 90.43,\n",
       " 107.01,\n",
       " 109.04,\n",
       " 111.07,\n",
       " 115.13,\n",
       " 85.36,\n",
       " 107.01,\n",
       " 109.04,\n",
       " 70.13,\n",
       " 92.8,\n",
       " 116.15,\n",
       " 108.03,\n",
       " 114.12,\n",
       " 92.12,\n",
       " 113.1,\n",
       " 94.49,\n",
       " 93.14,\n",
       " 108.03,\n",
       " 103.97,\n",
       " 89.76,\n",
       " 107.01,\n",
       " 75.54,\n",
       " 116.15,\n",
       " 107.01,\n",
       " 99.91,\n",
       " 104.98,\n",
       " 107.01,\n",
       " 97.54,\n",
       " 107.01,\n",
       " 105.66,\n",
       " 94.15,\n",
       " 104.98,\n",
       " 116.15,\n",
       " 101.94,\n",
       " 114.12,\n",
       " 112.09,\n",
       " 107.01,\n",
       " 99.91,\n",
       " 104.98,\n",
       " 110.06,\n",
       " 112.09,\n",
       " 93.48,\n",
       " 101.94,\n",
       " 102.95,\n",
       " 104.98,\n",
       " 102.61,\n",
       " 101.94,\n",
       " 111.07,\n",
       " 118.18,\n",
       " 84.68,\n",
       " 108.03,\n",
       " 115.13,\n",
       " 102.95,\n",
       " 117.16,\n",
       " 106.67,\n",
       " 85.7,\n",
       " 110.06,\n",
       " 110.06,\n",
       " 113.1,\n",
       " 85.7,\n",
       " 104.98,\n",
       " 103.63,\n",
       " 99.57,\n",
       " 107.01,\n",
       " 119.19,\n",
       " 95.17,\n",
       " 109.04,\n",
       " 92.8,\n",
       " 104.98,\n",
       " 109.04,\n",
       " 109.04,\n",
       " 98.89,\n",
       " 106.0,\n",
       " 110.06,\n",
       " 106.0,\n",
       " 101.6,\n",
       " 107.01,\n",
       " 106.0,\n",
       " 97.54,\n",
       " 94.83,\n",
       " 73.85,\n",
       " 86.71,\n",
       " 101.6,\n",
       " 95.51,\n",
       " 82.65,\n",
       " 99.91,\n",
       " 99.91,\n",
       " 96.18,\n",
       " 107.01,\n",
       " 50.16,\n",
       " 120.21,\n",
       " 103.97,\n",
       " 115.13,\n",
       " 94.49,\n",
       " 96.52,\n",
       " 78.25,\n",
       " 108.03,\n",
       " 89.08,\n",
       " 108.03,\n",
       " 104.64,\n",
       " 104.98,\n",
       " 98.55,\n",
       " 108.03,\n",
       " 109.04,\n",
       " 117.16,\n",
       " 86.03,\n",
       " 109.04,\n",
       " 115.13,\n",
       " 110.06,\n",
       " 55.24,\n",
       " 120.21,\n",
       " 114.12,\n",
       " 106.0,\n",
       " 118.18,\n",
       " 113.1,\n",
       " 89.42,\n",
       " 92.12,\n",
       " 107.01,\n",
       " 99.57,\n",
       " 96.52,\n",
       " 94.49,\n",
       " 114.12,\n",
       " 95.51,\n",
       " 104.64,\n",
       " 84.68,\n",
       " 98.89,\n",
       " 108.03,\n",
       " 84.68,\n",
       " 100.92,\n",
       " 104.98,\n",
       " 110.06,\n",
       " 101.94,\n",
       " 99.23,\n",
       " 106.0,\n",
       " 97.88,\n",
       " 108.03,\n",
       " 110.06,\n",
       " 100.92,\n",
       " 102.95,\n",
       " 119.19,\n",
       " 111.07,\n",
       " 70.81,\n",
       " 94.49,\n",
       " 107.01,\n",
       " 59.97,\n",
       " 101.6,\n",
       " 105.66,\n",
       " 113.1,\n",
       " 84.68,\n",
       " 94.83,\n",
       " 89.76,\n",
       " 98.89,\n",
       " 114.12,\n",
       " 93.14,\n",
       " 82.65,\n",
       " 99.91,\n",
       " 109.04,\n",
       " 87.05,\n",
       " 99.91,\n",
       " 99.91,\n",
       " 101.94,\n",
       " 114.12,\n",
       " 106.0,\n",
       " 85.69,\n",
       " 112.09,\n",
       " 94.49,\n",
       " 83.33,\n",
       " 95.51,\n",
       " 94.15,\n",
       " 102.61,\n",
       " 98.55,\n",
       " 94.49,\n",
       " 98.55,\n",
       " 119.19,\n",
       " 111.07,\n",
       " 116.15,\n",
       " 98.55,\n",
       " 34.93,\n",
       " 93.82,\n",
       " 95.51,\n",
       " 84.0,\n",
       " 110.06,\n",
       " 109.04,\n",
       " 108.03,\n",
       " 83.66,\n",
       " 115.13,\n",
       " 85.69,\n",
       " 84.0,\n",
       " 96.18,\n",
       " 119.19,\n",
       " 93.14,\n",
       " 96.18,\n",
       " 97.54,\n",
       " 84.68,\n",
       " 108.03,\n",
       " 100.92,\n",
       " 99.91,\n",
       " 62.34,\n",
       " 95.85,\n",
       " 87.73,\n",
       " 109.04,\n",
       " 96.52,\n",
       " 106.0,\n",
       " 100.92,\n",
       " 95.17,\n",
       " 88.06,\n",
       " 97.88,\n",
       " 96.52,\n",
       " 108.03,\n",
       " 90.77,\n",
       " 114.12,\n",
       " 82.65,\n",
       " 98.89,\n",
       " 109.04,\n",
       " 110.06,\n",
       " 85.69,\n",
       " 111.07,\n",
       " 100.92,\n",
       " 110.06,\n",
       " 107.01,\n",
       " 65.39,\n",
       " 88.74,\n",
       " 85.36,\n",
       " 92.46,\n",
       " 88.06,\n",
       " 115.13,\n",
       " 93.48,\n",
       " 91.11,\n",
       " 93.48,\n",
       " 121.22,\n",
       " 119.19,\n",
       " 102.95,\n",
       " 99.91,\n",
       " 99.57,\n",
       " 109.04,\n",
       " 78.25,\n",
       " 89.75,\n",
       " 106.0,\n",
       " 100.92,\n",
       " 96.86,\n",
       " 96.52,\n",
       " 112.09,\n",
       " 99.57,\n",
       " 108.03,\n",
       " 90.43,\n",
       " 101.94,\n",
       " 102.61,\n",
       " 115.13,\n",
       " 106.0,\n",
       " 100.24,\n",
       " 96.52,\n",
       " 103.97,\n",
       " 95.85,\n",
       " 112.09,\n",
       " 106.0,\n",
       " 113.1,\n",
       " 92.12,\n",
       " 97.54,\n",
       " 103.97,\n",
       " 95.17,\n",
       " 81.97,\n",
       " 114.12,\n",
       " 114.12,\n",
       " 95.85,\n",
       " 71.82,\n",
       " 100.58,\n",
       " 116.15,\n",
       " 102.95,\n",
       " -47.99,\n",
       " 95.85,\n",
       " 116.15,\n",
       " 100.58,\n",
       " 112.09,\n",
       " 119.19,\n",
       " 108.03,\n",
       " 88.06,\n",
       " 72.5,\n",
       " 98.89,\n",
       " 92.46,\n",
       " 101.94,\n",
       " 81.3,\n",
       " 96.86,\n",
       " 110.06,\n",
       " 95.51,\n",
       " 103.97,\n",
       " 114.12,\n",
       " 91.11,\n",
       " 94.15,\n",
       " 93.48,\n",
       " 92.8,\n",
       " 94.49,\n",
       " 119.19,\n",
       " 102.61,\n",
       " 95.51,\n",
       " 111.07,\n",
       " 109.04,\n",
       " 106.0,\n",
       " 108.03,\n",
       " 103.97,\n",
       " 108.03,\n",
       " 82.31,\n",
       " 109.04,\n",
       " 82.65,\n",
       " 109.04,\n",
       " 103.63,\n",
       " 113.1,\n",
       " 86.71,\n",
       " 107.01,\n",
       " 70.13,\n",
       " 97.88,\n",
       " 99.91,\n",
       " 108.03,\n",
       " 98.89,\n",
       " 108.03,\n",
       " 78.25,\n",
       " 93.48,\n",
       " 75.88,\n",
       " 109.04,\n",
       " 81.63,\n",
       " 94.49,\n",
       " 115.13,\n",
       " 98.55,\n",
       " 72.5,\n",
       " 99.57,\n",
       " 111.07,\n",
       " 110.06,\n",
       " 102.61,\n",
       " 103.63,\n",
       " 100.58,\n",
       " 104.98,\n",
       " 110.06,\n",
       " 108.03,\n",
       " 101.94,\n",
       " 100.58,\n",
       " 94.83,\n",
       " 110.06,\n",
       " 102.61,\n",
       " 110.06,\n",
       " 113.1,\n",
       " 109.04,\n",
       " 86.03,\n",
       " 110.06,\n",
       " 101.94,\n",
       " 100.92,\n",
       " 90.77,\n",
       " 98.55,\n",
       " 91.79,\n",
       " 89.42,\n",
       " 88.74,\n",
       " 107.01,\n",
       " 99.91,\n",
       " 102.95,\n",
       " 115.13,\n",
       " 92.46,\n",
       " 101.6,\n",
       " 104.98,\n",
       " 114.12,\n",
       " 115.13,\n",
       " 107.01,\n",
       " 117.16,\n",
       " 112.09,\n",
       " 101.94,\n",
       " 84.68,\n",
       " 92.12,\n",
       " 113.1,\n",
       " 86.03,\n",
       " 88.74,\n",
       " 99.57,\n",
       " 113.1,\n",
       " 77.23,\n",
       " 109.04,\n",
       " 112.09,\n",
       " 113.1,\n",
       " 92.12,\n",
       " 103.97,\n",
       " 87.05,\n",
       " 93.82,\n",
       " 104.64,\n",
       " 119.19,\n",
       " 98.55,\n",
       " 101.94,\n",
       " 112.09,\n",
       " 93.14,\n",
       " 109.04,\n",
       " 104.98,\n",
       " 98.55,\n",
       " 107.01,\n",
       " 113.1,\n",
       " 107.01,\n",
       " 100.92,\n",
       " 107.01,\n",
       " 103.97,\n",
       " 116.15,\n",
       " 92.12,\n",
       " 89.42,\n",
       " 93.14,\n",
       " 99.57,\n",
       " 111.07,\n",
       " 103.97,\n",
       " 84.68,\n",
       " 78.25,\n",
       " 97.88,\n",
       " 96.18,\n",
       " 95.51,\n",
       " 84.0,\n",
       " 99.57,\n",
       " 99.57,\n",
       " 102.95,\n",
       " 94.83,\n",
       " 103.97,\n",
       " 114.12,\n",
       " 108.03,\n",
       " 102.61,\n",
       " 117.16,\n",
       " 103.97,\n",
       " 103.97,\n",
       " 97.88,\n",
       " 111.07,\n",
       " 102.95,\n",
       " 113.1,\n",
       " 113.1,\n",
       " 107.01,\n",
       " 91.11,\n",
       " 104.98,\n",
       " 89.42,\n",
       " 113.1,\n",
       " 74.19,\n",
       " 86.71,\n",
       " 93.48,\n",
       " 110.06,\n",
       " 90.77,\n",
       " 108.03,\n",
       " 118.18,\n",
       " 100.92,\n",
       " 94.49,\n",
       " 109.04,\n",
       " 92.12,\n",
       " 83.33,\n",
       " 93.48,\n",
       " 98.55,\n",
       " 115.13,\n",
       " 95.17,\n",
       " 99.91,\n",
       " 113.1,\n",
       " 107.01,\n",
       " 120.21,\n",
       " 93.48,\n",
       " 86.71,\n",
       " 82.31,\n",
       " 109.04,\n",
       " 112.09,\n",
       " 115.13,\n",
       " 91.11,\n",
       " 119.19,\n",
       " 102.95,\n",
       " 106.0,\n",
       " 92.12,\n",
       " 112.09,\n",
       " 102.61,\n",
       " 93.48,\n",
       " 103.63,\n",
       " 100.92,\n",
       " 93.82,\n",
       " 107.01,\n",
       " 79.94,\n",
       " 88.74,\n",
       " 95.85,\n",
       " 110.06,\n",
       " 111.07,\n",
       " 112.09,\n",
       " 103.97,\n",
       " 111.07,\n",
       " 95.85,\n",
       " 87.05,\n",
       " 99.91,\n",
       " 109.04,\n",
       " 109.04,\n",
       " 92.12,\n",
       " 95.51,\n",
       " 102.95,\n",
       " 94.49,\n",
       " 108.03,\n",
       " 102.95,\n",
       " 97.88,\n",
       " 94.49,\n",
       " 104.98,\n",
       " 107.01,\n",
       " 90.77,\n",
       " 100.58,\n",
       " 98.89,\n",
       " 109.04,\n",
       " 81.97,\n",
       " 87.05,\n",
       " 116.15,\n",
       " 59.97,\n",
       " 104.98,\n",
       " 96.52,\n",
       " 113.1,\n",
       " 110.06,\n",
       " 97.54,\n",
       " 98.89,\n",
       " 106.0,\n",
       " 103.97,\n",
       " 112.09,\n",
       " 93.48,\n",
       " 104.98,\n",
       " 114.12,\n",
       " 103.97,\n",
       " 111.07,\n",
       " 111.07,\n",
       " 113.1,\n",
       " 96.86,\n",
       " 96.18,\n",
       " 94.83,\n",
       " 106.0,\n",
       " 107.01,\n",
       " 104.98,\n",
       " 78.25,\n",
       " 90.77,\n",
       " 115.13,\n",
       " 93.48,\n",
       " 93.82,\n",
       " 94.49,\n",
       " 97.54,\n",
       " 96.52,\n",
       " 104.98,\n",
       " 118.18,\n",
       " 106.0,\n",
       " 100.92,\n",
       " 100.92,\n",
       " 111.07,\n",
       " 98.89,\n",
       " 97.54,\n",
       " 104.98,\n",
       " 117.16,\n",
       " 75.88,\n",
       " 115.13,\n",
       " 108.03,\n",
       " 92.12,\n",
       " 119.19,\n",
       " 98.55,\n",
       " 106.0,\n",
       " 117.16,\n",
       " 120.21,\n",
       " 95.51,\n",
       " 115.13,\n",
       " 79.26,\n",
       " 108.03,\n",
       " 103.63,\n",
       " 91.45,\n",
       " 102.95,\n",
       " 94.83,\n",
       " 109.04,\n",
       " 96.86,\n",
       " 118.18,\n",
       " 99.91,\n",
       " 103.97,\n",
       " 111.07,\n",
       " 109.04,\n",
       " 118.18,\n",
       " 101.94,\n",
       " 68.77,\n",
       " 77.24,\n",
       " 112.09,\n",
       " 87.05,\n",
       " 104.98,\n",
       " 98.89,\n",
       " 107.01,\n",
       " 110.06,\n",
       " 104.64,\n",
       " 98.55,\n",
       " 103.97,\n",
       " 114.12,\n",
       " 111.07,\n",
       " 92.46,\n",
       " 110.06,\n",
       " 102.61,\n",
       " 113.1,\n",
       " 108.03,\n",
       " 95.85,\n",
       " 99.91,\n",
       " 108.03,\n",
       " 109.04,\n",
       " 115.13,\n",
       " 88.4,\n",
       " 106.67,\n",
       " 101.6,\n",
       " 82.65,\n",
       " 81.64,\n",
       " 109.04,\n",
       " 106.0,\n",
       " 112.09,\n",
       " 109.04,\n",
       " 102.61,\n",
       " 110.06,\n",
       " 110.06,\n",
       " 98.55,\n",
       " 100.92,\n",
       " 110.06,\n",
       " 97.88,\n",
       " 111.07,\n",
       " 92.46,\n",
       " 112.09,\n",
       " 108.03,\n",
       " 77.91,\n",
       " 100.24,\n",
       " 98.89,\n",
       " 107.01,\n",
       " 92.12,\n",
       " 93.48,\n",
       " 66.75,\n",
       " 96.52,\n",
       " 102.95,\n",
       " 103.97,\n",
       " 92.46,\n",
       " 106.0,\n",
       " 85.36,\n",
       " 113.1,\n",
       " 108.03,\n",
       " 119.19,\n",
       " 100.58,\n",
       " 109.04,\n",
       " 110.06,\n",
       " 115.13,\n",
       " 104.98,\n",
       " 106.0,\n",
       " 110.06,\n",
       " 114.12,\n",
       " 93.14,\n",
       " 106.0,\n",
       " 95.85,\n",
       " 111.07,\n",
       " 108.03,\n",
       " 115.13,\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(textstat.flesch_reading_ease, rev_soph_tag_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Code from Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the NLTK model\n",
    "def nltk_bleu(texts_origin, text_transfered):\n",
    "        texts_origin = [word_tokenize(text_origin.lower().strip()) for text_origin in texts_origin]\n",
    "        text_transfered = word_tokenize(text_transfered.lower().strip())\n",
    "        return sentence_bleu(texts_origin, text_transfered) * 100\n",
    "\n",
    "# Check the BLEU diff between original & transferred text\n",
    "def self_bleu_b(self, texts_origin, texts_transfered):\n",
    "        assert len(texts_origin) == len(texts_transfered), 'Size of inputs does not match!'\n",
    "        sum = 0\n",
    "        n = len(texts_origin)\n",
    "        for x, y in zip(texts_origin, texts_transfered):\n",
    "            sum += self.nltk_bleu([x], y)\n",
    "        return sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_ngram_lm(kenlm_path, data_path, output_path, N):\n",
    "    \"\"\"\n",
    "    Trains a modified Kneser-Ney n-gram KenLM from a text file.\n",
    "    Creates a .arpa file to store n-grams.\n",
    "    \"\"\"\n",
    "    # create .arpa file of n-grams\n",
    "    curdir = os.path.abspath(os.path.curdir)\n",
    "    #\n",
    "    command = \"bin/lmplz -o \"+str(N)+\" <\"+os.path.join(curdir, data_path) + \\\n",
    "              \" >\"+os.path.join(curdir, output_path)\n",
    "    os.system(\"cd \"+os.path.join(kenlm_path, 'build')+\" && \"+command)\n",
    "\n",
    "    load_kenlm()\n",
    "    # create language model\n",
    "    assert(output_path)  # captured by try..except block outside\n",
    "    model = kenlm.Model(output_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ppl(lm, sentences):\n",
    "    \"\"\"\n",
    "    Assume sentences is a list of strings (space delimited sentences)\n",
    "    \"\"\"\n",
    "    total_nll = 0\n",
    "    total_wc = 0\n",
    "    for sent in sentences:\n",
    "        words = sent.strip().split()\n",
    "        nll = np.sum([- math.log(math.pow(10.0, score)) for score, _, _ in lm.full_scores(sent, bos=True, eos=False)])\n",
    "        word_count = len(words)\n",
    "        total_wc += word_count\n",
    "        total_nll += nll\n",
    "    ppl = np.exp(total_nll / total_wc)\n",
    "    return ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other - https://github.com/adsieg/text_similarity/blob/master/Different%20Embeddings%20%2B%20Cosine%20Similarity%20%2B%20HeatMap%20illustration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance_countvectorizer_method(s1, s2):\n",
    "    \n",
    "    # sentences to list\n",
    "    allsentences = [s1 , s2]\n",
    "    \n",
    "    # packages\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from scipy.spatial import distance\n",
    "    \n",
    "    # text to vector\n",
    "    vectorizer = CountVectorizer()\n",
    "    all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n",
    "    text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n",
    "    text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n",
    "    \n",
    "    # distance of similarity\n",
    "    cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n",
    "    print('Similarity of two sentences are equal to ',round((1-cosine)*100,2),'%')\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    with open(gloveFile, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "    model = {}\n",
    "    for line in content:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess(raw_text):\n",
    "\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    cleaned_words = list(set([w for w in words if w not in stopword_set]))\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "def cosine_distance_between_two_words(word1, word2):\n",
    "    import scipy\n",
    "    return (1- scipy.spatial.distance.cosine(model[word1], model[word2]))\n",
    "\n",
    "def calculate_heat_matrix_for_two_sentences(s1,s2):\n",
    "    s1 = preprocess(s1)\n",
    "    s2 = preprocess(s2)\n",
    "    result_list = [[cosine_distance_between_two_words(word1, word2) for word2 in s2] for word1 in s1]\n",
    "    result_df = pd.DataFrame(result_list)\n",
    "    result_df.columns = s2\n",
    "    result_df.index = s1\n",
    "    return result_df\n",
    "\n",
    "def cosine_distance_wordembedding_method(s1, s2):\n",
    "    import scipy\n",
    "    vector_1 = np.mean([model[word] for word in preprocess(s1)],axis=0)\n",
    "    vector_2 = np.mean([model[word] for word in preprocess(s2)],axis=0)\n",
    "    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
    "    print('Word Embedding method with a cosine distance asses that our two sentences are similar to',round((1-cosine)*100,2),'%')\n",
    "\n",
    "def heat_map_matrix_between_two_sentences(s1,s2):\n",
    "    df = calculate_heat_matrix_for_two_sentences(s1,s2)\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(5,5)) \n",
    "    ax_blue = sns.heatmap(df, cmap=\"YlGnBu\")\n",
    "    # ax_red = sns.heatmap(df)\n",
    "    print(cosine_distance_wordembedding_method(s1, s2))\n",
    "    return ax_blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding method with a cosine distance asses that our two sentences are similar to 98.39 %\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc46c56da20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heat_map_matrix_between_two_sentences(gold[100],raw0[100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
