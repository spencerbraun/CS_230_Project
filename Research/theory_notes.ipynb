{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Notes\n",
    "\n",
    "__Contents__ :\n",
    "1. Architecture overview\n",
    "    - 1.1 [RNN](#RNN)\n",
    "    - 1.2 [LSTM](#LSTM)\n",
    "    - 1.3 [Attention](#Attention)\n",
    "2. General models\n",
    "    - 2.1 [VAE](#VAE)\n",
    "    - 2.2 [Seq2Seq](#Seq2Seq)\n",
    "3. Paper implementations (non-parallel text)\n",
    "    - 3.1 [Style Transfer from Non-Parallel Text by Cross-Alignment](#Paper_3.1) - one of the first big papers on text style transfer (TST); by MIT CSAIL group\n",
    "    - 3.2 [Neural Style Transfer for Non-Parallel Text](#Paper_3.2) - summary of CSAIL paper\n",
    "    - 3.3 [Disentangled Representation Learning for Non-Parallel Text Style Transfer](#Paper_3.3) - the most recent/comprehensive source on TST\n",
    "    - 3.4 [Style Transfer in Text: Exploration and Evaluation](#Paper_3.4) - autoencoder & style embedding versions of seq2seq\n",
    "4. Evaluation\n",
    "    - 4.1 [Massive Exploration of Neural Machine Translation Architectures](#Eval_4.1) - methodology and parameter suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture Overview (Coursera & NLP Notes)\n",
    "\n",
    "***\n",
    "\n",
    "<a name=\"RNN\"></a>\n",
    "### 1.1 RNN: Recurrent Neural Network\n",
    "- _Sources_ : \n",
    "    - $[1]$ [RNN Wiki](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "    - $[2]$ [Coursera Slides](https://cs230.stanford.edu/files/C5M1.pdf)\n",
    "    - $[3]$ [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/pdf/1703.03906.pdf) (Google Brain)\n",
    "\n",
    "#### 1.1.1 Overview $[1]$\n",
    "- _Problems with standard NN_ :\n",
    "    - Inputs, outputs can be different lengths\n",
    "    - Doesn't share features learned across different positions of text\n",
    "- Connections between nodes form directed graph along temporal sequence; allows temporal dynamic behavior\n",
    "- Two classes of RNNs:\n",
    "    1. _Finite impulse_ : response to input is of finite duration (settles to zero in finite time)\n",
    "        - Is DAG, can be \"unrolled\" and replaced with strictly feedforward NN\n",
    "    2. _Infinite impulse_ : may have internal feedback, and may continue to respond indefinitely (although usually decay)\n",
    "        - Directed cyclic graph, cannot be unrolled\n",
    "- Can have additional stored states, potentially controlled by NN $\\to$ incorporating time delays/feedback loops leads to LSTM gated memory principles\n",
    "\n",
    "#### 1.1.2 RNN types $[2]$\n",
    "\n",
    "<img src = \"img/coursera_rnn_types.png\" style = \"width: 500px\"/>\n",
    "\n",
    "#### 1.1.3 Issue of vanishing gradients, illustrated $[2]$\n",
    "\n",
    "<img src = \"img/coursera_vanishing_grads.png\" style = \"width: 500px\"/>\n",
    "    \n",
    "\n",
    "#### 1.1.4 GRU: gated recurrent unit\n",
    "- Gating mechanism in RNN: like LSTM with forget gate, but has fewer parameters (lacks output gate)\n",
    "- Also addresses the vanishing gradients problem\n",
    "- Can have better performance than LSTM on small datasets; but LSTM strictly stronger $[3]$ \n",
    "\n",
    "<a name=\"LSTM\"></a>\n",
    "### 1.2 LSTM: Long Short-Term Memory\n",
    "\n",
    "- _Sources_ : \n",
    "    - $[1]$ [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) \n",
    "        - Original paper on LSTM, cited in Coursera\n",
    "    - $[2]$ [LSTM Wiki](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "    - $[3]$ [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "\n",
    "#### 1.2.1 Motivation $[1]$\n",
    "- DNNs can only be applied to poblems where inputs, targets encoded with vectors of fixed dimensionality; not good for speech\n",
    "- Conventional DNNs have issue of compounding error in backprop\n",
    "- _LSTM soln_ : when error values back-propagated, error remains in LSTM cell $\\implies$ \"error carousel\" feeds error back into each of LSTM gates, until they learn to cut off value\n",
    "\n",
    "\n",
    "#### 1.2.2 What is an LSTM? $[1, \\, 2]$\n",
    "- Has feedback connections, unlike standard _feedforward_ NNs; in feedforward, node connections don't form a cycle\n",
    "- _Idea_ :\n",
    "    - Cell remembers values, and three gates regulate flow of into into/out of cell\n",
    "    - Useful for data with lags\n",
    "    - Each gate has activation: marked increase in number of parameters\n",
    "    - Robustness to range of parameters due to error control\n",
    "    \n",
    "#### 1.2.3 LSTM units $[1]$\n",
    "##### 1.2.3 (a) -  LSTM unit\n",
    "- __Cell__ : keeps track of dependencies between elements in input sequence\n",
    "- __Input gate__ : controls net flow into cell\n",
    "- __Forget gate__ : controls how much value remains in cell\n",
    "- __Output gate__ : controls extent to which cell value used to compute activation\n",
    "    \n",
    "##### 1.2.3 (b) - Why gate units?\n",
    "- Avoids weight conflicts\n",
    "- Input gate learns when to release errors by appropriate scaling\n",
    "    \n",
    "<img src = \"img/coursera_lstm.png\" style = \"width: 500px\"/>    \n",
    "    \n",
    "#### 1.2.4 Application to NLP models $[3]$\n",
    "- _Property of LSTM_ : learns to map input sentence of variable length onto fixed-dimensional vector representation\n",
    "    - Translation objective encourages LSTM to find sentence representations that capture meaning\n",
    "    - Paper claim: model aware of word order and fairly invariant to active/passive voice\n",
    "    \n",
    "<a name=\"Attention\"></a>\n",
    "### 1.3 Model with Attention\n",
    "- _Sources_ :\n",
    "    - $[1]$ [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generalized Models\n",
    "\n",
    "***\n",
    "\n",
    "<a name=\"VAE\"></a>\n",
    "### 2.1 Variational Autoencoder (VAE)\n",
    "\n",
    "- _Sources_ : \n",
    "    - $[1]$ [Google's Neural Machine Translation System](https://arxiv.org/pdf/1609.08144.pdf)\n",
    "    - $[2]$ [VAE tutorial](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\n",
    "    - $[3]$ [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "    - $[4]$ [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) - one of the original papers\n",
    "    - $[5]$ [Stochastic Backprop in DGMs](https://arxiv.org/pdf/1401.4082.pdf) - the other original paper\n",
    "    - $[6]$ [Kullback-Leibler divergence Wiki](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "    - $[7]$ [Intuitively understanding VAEs](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)\n",
    "\n",
    "#### 2.1.1 Encoder-decoder model $[3]$\n",
    "##### 2.1.1 (a) - Encoder\n",
    "- Reads in source data $x$, produces feature representation in latent space $z$\n",
    "    * RNN; input sequence of words $\\to$ fixed vector that is $\\approx$ meaning\n",
    "    * CNN; input img $\\to$ volume that contains higher-level features of img\n",
    "- Aka the __inference network__ : parameterizes appx posterior of latent variables $z$ & outputs parameters to dist $q(z \\mid x)$\n",
    "- Bottleneck: encoder must learn efficient compression of data into hidden representation, $z$, for input $x$\n",
    "- _Note_ : encoder $q_{\\theta} (z \\mid x)$ for parameters $\\theta$ is stochastic; usually $\\sim$ Gaussian\n",
    "\n",
    "<img src = \"img/vae_encoding_visual.png\" style = \"width: 500px\"/>\n",
    "    \n",
    "##### 2.1.1 (b) -  Decoder\n",
    "- Input: representation $z$, output: reconstruction of $x$; aka __generative network__\n",
    "- Notation: $p_{\\phi}(x \\mid z)$ for parameters $\\phi$ (aka __likelihood distribution__)\n",
    "- Decoder gets latent representation of input as $z$, so necessarily loses some information\n",
    "    - $\\implies$ lost information represented by reconstruction log-likelihood $\\log p_{\\phi}(x \\mid z)$\n",
    "    - Measures how effectively decoder can reconstruct input $x$ given latent $z$\n",
    "    \n",
    "##### 2.1.1 (c) Loss function\n",
    "- Log-likelihood loss w/regularization; since each $z_i$ corresponds to an $x_i$, no global latent vars $\\implies$ can decompose loss fn into only terms that depend on single datapt $\\ell_i$:\n",
    "$$\\ell_i(\\theta, \\phi) = -\\mathbb{E}_{z \\sim q_{\\theta}(z \\mid x_i)} \\log p_{\\phi}(x_i \\mid z) + R(x_i)$$\n",
    "- _Notes on loss_ :\n",
    "    - Expectation taken wrt encoder distribution over representations $\\implies$ poor reconstruction yields large loss\n",
    "    - Regularizer $R(x)$: use __Kullback-Leibler divergence__ $[6]$ between distributions\n",
    "        - KL definition for $p, q$ on same probability space $\\mathcal{X}$:\n",
    "        $$ \\mathbb{KL}(q \\mid\\mid p) = \\sum_{x \\in \\mathcal{X}} q(x) \\log \\Big(\\frac{q}{p} \\Big)$$\n",
    "        - _Idea_ : expectation of log-$\\Delta$ between $p, q$\n",
    "        - _Interpretation_ : how much information lost when using $q$ instead of $p$\n",
    "        - Defined iff $\\forall \\, x$, $p = 0 \\implies q = 0$ (absolute continuity)\n",
    "    \n",
    "##### 2.1.1 (d) Example encoder-decoder model (with attention) $[2]$\n",
    "    \n",
    "<img src = \"img/google_brain_encoder_decoder.png\" style = \"width: 500px\"/>\n",
    "\n",
    "#### 2.1.2 Variational autoencoder $[3]$\n",
    "##### 2.1.2 (a) - VAE vs. traditional autoencoder $[7]$\n",
    "\n",
    "<img src = \"img/vae_vs_standard.png\" style = \"width: 500px\"/>\n",
    "\n",
    "##### 2.1.2 (b) - Definition\n",
    "- _Formal def_ : approximate inference in latent Gaussian model where approximate posterior and model likelihood parameterized by NNs (inference, and generative networks)\n",
    "- Assumes distribution on $p$: generally $p \\sim$ unit normal; allows direct sampling\n",
    "    - $\\implies$ if encoder outputs representations $z$ different from those normally distributed, then will receive penalty in loss\n",
    "    - _Note_ : could model word distribution for better encoding as in 161/200?\n",
    "- Train VAE using gradient descent, or other optimizer\n",
    "\n",
    "##### 2.1.2 (c) - VAE backprop\n",
    "- Want small $\\sigma$ in $p$ to ensure that model generates something like $x$ input\n",
    "\n",
    "##### 2.1.2 (d) - KL divergence for two gaussians $[7]$\n",
    "$$ \\mathbb{KL} = \\sum_i \\sigma_i^2 + \\mu_i^2 - \\log(\\sigma_i) - 1 $$\n",
    "\n",
    "***\n",
    "\n",
    "<a name=\"Seq2Seq\"></a>\n",
    "### 2.2 Seq2Seq\n",
    "\n",
    "- _Sources_ : \n",
    "    - $[1]$ [Seq2Seq documentation](https://google.github.io/seq2seq/)\n",
    "    - $[2]$ [Seq2Seq Wiki](https://en.wikipedia.org/wiki/Seq2seq)\n",
    "    - $[3]$ [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf); _authors_ : Sutskever et al., 2014\n",
    "    - $[4]$ [Style Transfer in Text: Exploration and Evaluation](https://arxiv.org/pdf/1711.06861.pdf)\n",
    "\n",
    "    \n",
    "- _Code_ :\n",
    "    - _List of encoders_ : [seq2seq encoders](https://google.github.io/seq2seq/encoders/)\n",
    "    - _List of decoders_ : [seq2seq decoders](https://google.github.io/seq2seq/decoders/)\n",
    "    - _List of models_ : [seq2seq models](https://google.github.io/seq2seq/models/)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Paper Implementations (Non-Parallel Text)\n",
    "\n",
    "***\n",
    "\n",
    "<a name=\"Paper_3.1\"></a>\n",
    "### 3.1 [Style Transfer from Non-Parallel Text by Cross-Alignment](https://arxiv.org/pdf/1705.09655.pdf)\n",
    "- _Authors_ : Shen et al., 2017\n",
    "\n",
    "- _Relevant outside sources_ :\n",
    "    - $[1]$ [Toward Controlled Generation of Text](https://arxiv.org/pdf/1703.00955.pdf) - latent spaces with generated text; _authors_ : Hu et. al, 2017\n",
    "    - $[2]$ [InfoGAN: Representation Learning](https://arxiv.org/pdf/1606.03657.pdf) - more on latent spaces\n",
    "    - $[3]$ [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) - original VAE paper\n",
    "    - $[4]$ [CNN for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf) - model-based evaluation metric\n",
    "    - $[5]$ [word2vec](https://code.google.com/archive/p/word2vec/) - more on model-based evaluation\n",
    "    - $[6]$ [Professor Forcing](https://arxiv.org/pdf/1610.09038.pdf) - RNN training algorithm\n",
    "- _Code implementation_ : [authors' repo](https://github.com/shentianxiao/language-style-transfer)\n",
    "\n",
    "#### 3.1.1 Introduction\n",
    "- Learn encoder $\\to$ take style as input $\\to$ map to style-independent content\n",
    "- Do not use VAE ( _need to research_ ) - need to preserve latent content\n",
    "\n",
    "<img src = \"img/non_parallel_model.png\" style = \"width: 500px\"/>  \n",
    "\n",
    "- Task evaluation:\n",
    "    1. Sentiment modification\n",
    "    2. Decipherment of word substitution ciphers\n",
    "    3. Recovery of word order\n",
    "    \n",
    "#### 3.1.2 Related work\n",
    "- _Vision_ : cannot employ many similar vision methods due to discreteness of NLP\n",
    "    - Generating sentences with controllable attributes by learning disentangled latent representations $[1]$\n",
    "    - More reserach available on latent spaces $[2]$\n",
    "    \n",
    "#### 3.1.3 Formulation\n",
    "- _Motivation_ : want to recover joint distributions of style in order to facilitate style transfer\n",
    "    - __Claim__ : datasets from different styles need to be distinct enough for this paper's methods to work\n",
    "- _Conclusions_ : latent content should have most complexity from input, and latent style variable should have simple effects\n",
    "\n",
    "##### 3.1.3 (a) - Data Generation\n",
    "1. Latent style variable $y$ generated from $p(y)$\n",
    "2. Latent content variable generated from $p(z)$\n",
    "3. Datapoint $x$ generated from conditional dist $p(x \\mid y, z)$\n",
    "\n",
    "##### 3.1.3 (b) - Setup\n",
    "- Two data sets $X_1$ (drawn from $p(x_1, y_1)$), $X_2$ with same content, but different style (drawn from $p(x_2, y_2)$): $y_1, y_2$, both unknown\n",
    "- Want to estimate style transfer functions: $p(x_1 \\mid x_2 ; y_1, y_2)$ and $p(x_2 \\mid x_1; y_1, y_2)$\n",
    "\n",
    "#### 3.1.4 Model\n",
    "##### 3.1.4 (a) - Overview\n",
    "- Use of auto-encoder model:\n",
    "    1. Encoding step to infer content of target\n",
    "    2. Decoding step to generate transferred counterpart\n",
    "- Need content space of source and target to coincide; could employ VAE $[3]$\n",
    "    \n",
    "##### 3.1.4 (b) - Model specifics and loss\n",
    "- Encoder E: infers content $z$ from sentence $x$ and style $y$\n",
    "- Generator G: generates sentence $x$ from from style $y$ and content $z$\n",
    "- E, G form autoencoder when applied to same style\n",
    "\n",
    "<img src = \"img/csail_autoencoder_loss.png\"/>  \n",
    "\n",
    "- Align posteriors to have same distribution; use adversarial discriminator to distinguish between the dists\\\n",
    "- _Goal_ : minimize encoder, generator loss; maximize discriminator distinction\n",
    "\n",
    "\n",
    "<img src = \"img/csail_loss_function.png\"/>  \n",
    "    \n",
    "##### 3.1.4 (c) -  Proposed styles\n",
    "\n",
    "1. _Aligned auto-encoder_ :\n",
    "\n",
    "    - Implement encoder & decoder using single-layer RNNs with GRU\n",
    "    - Need to align distributions: use discriminator D as feed-forward NN with single hidden layer, sigmoid output\n",
    "    - Complicated loss function (see above)\n",
    "    \n",
    "2. _Cross-aligned auto-encoder_ :\n",
    "\n",
    "    - Use two discriminators: $D_1$, $D_2$\n",
    "        - $D_1$ : distinguish between real $x_1$ and transferred $x_2$\n",
    "        - $D_2$ : distinguish between real $x_2$ and transferred $x_1$\n",
    "        \n",
    "##### 3.1.4 (d) - Implementation\n",
    "\n",
    "1. Use softmax distribution over words\n",
    "    - During generating process of transferred $x_2$ from $G(y_1, z_2)$, feed peaked $\\text{softmax}(a^{[\\ell]} / \\gamma)$ into next input\n",
    "2. Use _Professor-Forcing_ $[6]$ to \"match sequence of hidden states instead of output words\"\n",
    "    - Hidden states: information about outputs, and smoothly distributed\n",
    "    - Input to $D_1$ : sequence of hidden states, either from:\n",
    "        1. Teacher forcing by real example $x_1$; $G(y_1, z_1)$\n",
    "        2. Self-fed by previous softmaxes; $G(y_1, z_2)$\n",
    "    \n",
    "        \n",
    "#### 3.1.5 Evaluation\n",
    "- Used model-based evaluation metric for sentiment modification: measure how often a transferred sentence has correct sentiment according to pre-trained sentiment classifier (see $[4, 5]$)\n",
    "\n",
    "<img src = \"img/csail_professor_forcing.png\"/> \n",
    "\n",
    "#### 3.1.6 Code re-implementations\n",
    "- _Vinitra CSAIL summary paper version_ : [repo](https://github.com/vinitra/neural-text-style-transfer) - (see below, $3.2$)\n",
    "- _J. Park Shakespeare non-parallel transfer_ : [repo](https://github.com/jpark96/language-style-transfer)\n",
    "    \n",
    "***\n",
    "\n",
    "<a name=\"Paper_3.2\"></a>\n",
    "### 3.2 [Neural Style Transfer for Non-Parallel Text](https://github.com/vinitra/neural-text-style-transfer/blob/master/reports/Neural_Style_Transfer_for_Non_Parallel_Text.pdf)\n",
    "\n",
    "- _Note_ : essentially a plain-English summary of the CSAIL paper $(3.1)$\n",
    "- [_Full repo_](https://github.com/vinitra/neural-text-style-transfer)\n",
    "\n",
    "#### 3.2.1  Model\n",
    "1. One-layer RNN GRU to find latent representation of content variables ($z$)\n",
    "2. Simple one-layer feedforward NN as discriminator; implicit distribution on $z$\n",
    "3. Feed latent $z$ into decoder RNN (same architecture as encoder), joint optimization\n",
    "4. Optimize decoder after latent representation learned\n",
    "    - Use latent code $z$ and one-hot-encoded style representation\n",
    "    - Train decoder RNN to reconstruct original sentence\n",
    "    \n",
    "<img src = \"img/csail_summary_paper_model.png\" style = \"width: 500px\"/> \n",
    "\n",
    "#### 3.2.2 Evaluation\n",
    "    \n",
    "##### 3.2.2 (a) - Content preservation\n",
    "- Use model-based evaluation like in zebra/horse paper from class; transfer output back to original, then compare via BLEU score\n",
    "\n",
    "<img src = \"img/csail_summary_paper_evaluation.png\"/> \n",
    "\n",
    "##### 3.2.2 (b) - Style evaluation\n",
    "- CSAIL paper equates _style_ and _sentiment_ : problem this paper talks about that we also need to consider\n",
    "- Used authorship attribution as proxy for style: author classification as loss metric (could be adapted to our work)\n",
    "- Could employ technique from $3.2.2 (a)$, again citing the horse paper; could try style transfer in reverse, then evaluate how well the styles match\n",
    "\n",
    "***\n",
    "\n",
    "<a name=\"Paper_3.3\"></a>\n",
    "### 3.3 [Disentangled Representation Learning for Non-Parallel Text Style Transfer](https://arxiv.org/pdf/1808.04339.pdf)\n",
    "\n",
    "- _Relevant outside sources_ :\n",
    "    - $[1]$ [Toward Controlled Generation of Text](https://arxiv.org/pdf/1703.00955.pdf) - latent spaces with generated text; _authors_ : Hu et al., 2017\n",
    "    - $[2]$ [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf); _authors_ : Sutskever et al., 2014\n",
    "    - $[3.1]$ [Style Transfer from Non-Parallel Text by Cross-Alignment](https://arxiv.org/pdf/1705.09655.pdf); _authors_ : Shen et al., 2017\n",
    "- _Code_ :\n",
    "    - _Tensorflow_ : [authors' repo](https://github.com/vineetjohn/linguistic-style-transfer) - updated as of 2019\n",
    "    - _Pytorch_ : [repo](https://github.com/h3lio5/linguistic-style-transfer-pytorch), also has [good paper & model summary](https://github.com/h3lio5/linguistic-style-transfer-pytorch/blob/master/summary.md)\n",
    "    \n",
    "#### 3.3.1 Introduction\n",
    "- _Latent feature_ : no explicit meaning/interpretation to intermediate non-linear transformations (\"black box\" criticism of NNs)\n",
    "- Autoencoder paradigm: sentence latent space as vector representation $\\to$ disentangle to _style_ , _content_\n",
    "\n",
    "##### 3.3.1 (a) - General approach\n",
    "- Conflating sentiment with style -- need to address in our implementation\n",
    "- Multi-task loss operates on latent space\n",
    "- Adversarial loss \"minimizes predictability of information that shouldn't be contained in that space\"\n",
    "\n",
    "##### 3.3.1 (b) - Content approximation\n",
    "- Use __bag of words__ (BoW) features: style-neutral, non-stopwords $\\implies$ disentangling the spaces\n",
    "- Then, employ the models from the CSAIL paper $[3.1]$ and Hu et. al, 2017 $[1]$\n",
    "\n",
    "##### 3.3.1 (c) - Non-parallel text style transfer\n",
    "- Train model on non-parallel but _style-labeled_ corpora\n",
    "    1. Train autoencoder with disentangled latent spaces\n",
    "    2. For inference: autoencoder encodes content, but ignores encoded style\n",
    "    3. Empirically infer style from training data\n",
    "    4. Concatenate content from (2) and style from (3), feed into decoder\n",
    "    \n",
    "#### 3.3.2 Importance of related works\n",
    "- $[1]$ _Hu et al., 2017_ :\n",
    "    - Control sentiment (style proxy) by \"using discriminators to reconstruct sentiment, content from generated sentences\"\n",
    "    - Criticism: no evidence that this disentangles the latent space\n",
    "- $[3.1]$ _Shen et al., 2017 (CSAIL)_ :\n",
    "    - Pair of adversarial discriminators to align recurrent hidden decoder states of original & style-transferred sentences\n",
    "- _Summary_ : most authors up to 2019 basically ignored the content space\n",
    "    - This paper encoes input from the start, which allows for easier analysis\n",
    "    \n",
    "#### 3.3.3 Model overview\n",
    "- Builds heavily on Sutskever et al., 2014, Seq2Seq model $[2]$\n",
    "\n",
    "***\n",
    "\n",
    "<a name=\"Paper_3.4\"></a>\n",
    "### 3.4 [Style Transfer in Text: Exploration and Evaluation](https://arxiv.org/pdf/1711.06861.pdf)\n",
    "\n",
    "#### 3.4.1 Models\n",
    "##### 3.4.1 (a) - Summary\n",
    "<img src = \"img/style_transfer_in_text_models.png\" style = \"width: 500px\"/> \n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "<a name=\"Eval_4.1\"></a>\n",
    "### 4.1 [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/pdf/1703.03906.pdf)\n",
    "\n",
    "- _Source_ : results from Google Brain experiments in 2017 to compare NMT models\n",
    "\n",
    "#### 4.1.1 Results\n",
    "- Large embeddings (they tried $2048$ dimensions) got best results, but only by small margin\n",
    "    - Small embeddings can generally get most semantic information ($\\geq 128$)\n",
    "- LSTMs consistently outperformed GRU in BLEU scores\n",
    "- Need residual connections for deep-layered decoders\n",
    "- Overall best results when using parameterized additive attention\n",
    "- Importance of well-tuned beam search: recommend beam width $\\in [5, 10]$ and length penalty of $\\approx 1.0$\n",
    "\n",
    "***\n",
    "\n",
    "### 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
