{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Theory Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architectures\n",
    "\n",
    "***\n",
    "\n",
    "### 1.1 Encoder-Decoder Model\n",
    "\n",
    "- _Source_ : [Seq2Seq documentation](https://google.github.io/seq2seq/)\n",
    "\n",
    "#### 1.1.1 Encoder-decoder overview\n",
    "\n",
    "- __Encoder__ :\n",
    "    * Reads in source data, produces feature representation in cts space\n",
    "        * RNN; input sequence of words $\\to$ fixed vector that is $\\approx$ meaning\n",
    "        * CNN; input img $\\to$ volume that contains higher-level features of img\n",
    "    * _Idea_ : representation from encoder used by decoder to generate new data\n",
    "    * [_List of encoders_](https://google.github.io/seq2seq/encoders/)\n",
    "    \n",
    "- __Decoder__ :\n",
    "    * Generative model, conditioned on representation from encoder\n",
    "        * RNN decoder: learns to generate translation for encoded sequence in another language\n",
    "    * [_List of decoders_](https://google.github.io/seq2seq/decoders/)\n",
    "    \n",
    "#### 1.1.2 Encoder-decoder model\n",
    "    \n",
    "- __The model__ :\n",
    "    * Defines how encoder/decoder put together & how to calculate/minimize loss fn\n",
    "    * [_List of models_](https://google.github.io/seq2seq/models/)\n",
    "    \n",
    "\n",
    "### 1.2 RNN: Recurrent Neural Network\n",
    "- _Sources_ : \n",
    "    - $[1]$ [RNN Wiki](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "    - $[2]$ [Coursera Slides](https://cs230.stanford.edu/files/C5M1.pdf)\n",
    "    - $[3]$ [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/pdf/1703.03906.pdf) (Google Brain)\n",
    "\n",
    "#### 1.2.1 Overview $[1]$\n",
    "- _Problems with standard NN_ :\n",
    "    - Inputs, outputs can be different lengths\n",
    "    - Doesn't share features learned across different positions of text\n",
    "- Connections between nodes form directed graph along temporal sequence; allows temporal dynamic behavior\n",
    "- Two classes of RNNs:\n",
    "    1. _Finite impulse_ : response to input is of finite duration (settles to zero in finite time)\n",
    "        - Is DAG, can be \"unrolled\" and replaced with strictly feedforward NN\n",
    "    2. _Infinite impulse_ : may have internal feedback, and may continue to respond indefinitely (although usually decay)\n",
    "        - Directed cyclic graph, cannot be unrolled\n",
    "- Can have additional stored states, potentially controlled by NN $\\to$ incorporating time delays/feedback loops leads to LSTM gated memory principles\n",
    "\n",
    "#### 1.2.2 RNN types $[2]$\n",
    "\n",
    "<img src = \"img/coursera_rnn_types.png\" alt = \"drawing\" width = \"400\"/>\n",
    "\n",
    "#### 1.2.3 Issue of vanishing gradients, illustrated $[2]$\n",
    "\n",
    "<img src = \"img/coursera_vanishing_grads.png\" alt = \"drawing\" width = \"600\"/>\n",
    "    \n",
    "\n",
    "#### 1.2.4 GRU: gated recurrent unit\n",
    "- Gating mechanism in RNN: like LSTM with forget gate, but has fewer parameters (lacks output gate)\n",
    "- Also addresses the vanishing gradients problem\n",
    "- Can have better performance than LSTM on small datasets; but LSTM strictly stronger $[3]$ \n",
    "\n",
    "### 1.3 LSTM: Long Short-Term Memory\n",
    "\n",
    "- _Sources_ : \n",
    "    - $[1]$ [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) \n",
    "        - Original paper on LSTM, cited in Coursera\n",
    "    - $[2]$ [LSTM Wiki](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "    - $[3]$ [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "\n",
    "#### 1.3.1 Motivation $[1]$\n",
    "- DNNs can only be applied to poblems where inputs, targets encoded with vectors of fixed dimensionality; not good for speech\n",
    "- Conventional DNNs have issue of compounding error in backprop\n",
    "- _LSTM soln_ : when error values back-propagated, error remains in LSTM cell $\\implies$ \"error carousel\" feeds error back into each of LSTM gates, until they learn to cut off value\n",
    "\n",
    "\n",
    "#### 1.3.2 What is an LSTM? $[2]$\n",
    "- Has feedback connections, unlike standard _feedforward_ NNs; in feedforward, node connections don't form a cycle\n",
    "\n",
    "- _LSTM unit_ :  \n",
    "    - __Cell__ : keeps track of dependencies between elements in input sequence\n",
    "    - __Input gate__ : controls net flow into cell\n",
    "    - __Forget gate__ : controls how much value remains in cell\n",
    "    - __Output gate__ : controls extent to which cell value used to compute activation\n",
    "    \n",
    "- _Idea_ :\n",
    "    - Cell remembers values, and three gates regulate flow of into into/out of cell\n",
    "    - Useful for data with lags\n",
    "    - Each gate has activation: more things to train\n",
    "    \n",
    "#### 1.3.3 Application to NLP models $[3]$\n",
    "- _Property of LSTM_ : learns to map input sentence of variable length onto fixed-dimensional vector representation\n",
    "    - Translation objective encourages LSTM to find sentence representations that capture meaning\n",
    "    - Paper claim: model aware of word order and fairly invariant to active/passive voice\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seq2Seq\n",
    "\n",
    "- _Sources_ : \n",
    "    - [Seq2Seq documentation](https://google.github.io/seq2seq/)\n",
    "    - [Wiki](https://en.wikipedia.org/wiki/Seq2seq)\n",
    "\n",
    "### 2.1 Model Overview\n",
    "- Turns one sequence into another; uses RNN, LSTM, or GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
