{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture Overview\n",
    "\n",
    "***\n",
    "\n",
    "### 1.1 Encoder-Decoder Model\n",
    "\n",
    "- _Source_ : [Seq2Seq documentation](https://google.github.io/seq2seq/)\n",
    "\n",
    "#### 1.1.1 Encoder-decoder overview\n",
    "\n",
    "- __Encoder__ :\n",
    "    * Reads in source data, produces feature representation in cts space\n",
    "        * RNN; input sequence of words $\\to$ fixed vector that is $\\approx$ meaning\n",
    "        * CNN; input img $\\to$ volume that contains higher-level features of img\n",
    "    * _Idea_ : representation from encoder used by decoder to generate new data\n",
    "    * [_List of encoders_](https://google.github.io/seq2seq/encoders/)\n",
    "    \n",
    "- __Decoder__ :\n",
    "    * Generative model, conditioned on representation from encoder\n",
    "        * RNN decoder: learns to generate translation for encoded sequence in another language\n",
    "    * [_List of decoders_](https://google.github.io/seq2seq/decoders/)\n",
    "    \n",
    "#### 1.1.2 Encoder-decoder model\n",
    "    \n",
    "- __The model__ :\n",
    "    * Defines how encoder/decoder put together & how to calculate/minimize loss fn\n",
    "    * [_List of models_](https://google.github.io/seq2seq/models/)\n",
    "    \n",
    "    <img src = \"img/google_brain_encoder_decoder.png\" style = \"width: 500px\"/>\n",
    "\n",
    "### 1.2 RNN: Recurrent Neural Network\n",
    "- _Sources_ : \n",
    "    - $[1]$ [RNN Wiki](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "    - $[2]$ [Coursera Slides](https://cs230.stanford.edu/files/C5M1.pdf)\n",
    "    - $[3]$ [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/pdf/1703.03906.pdf) (Google Brain)\n",
    "\n",
    "#### 1.2.1 Overview $[1]$\n",
    "- _Problems with standard NN_ :\n",
    "    - Inputs, outputs can be different lengths\n",
    "    - Doesn't share features learned across different positions of text\n",
    "- Connections between nodes form directed graph along temporal sequence; allows temporal dynamic behavior\n",
    "- Two classes of RNNs:\n",
    "    1. _Finite impulse_ : response to input is of finite duration (settles to zero in finite time)\n",
    "        - Is DAG, can be \"unrolled\" and replaced with strictly feedforward NN\n",
    "    2. _Infinite impulse_ : may have internal feedback, and may continue to respond indefinitely (although usually decay)\n",
    "        - Directed cyclic graph, cannot be unrolled\n",
    "- Can have additional stored states, potentially controlled by NN $\\to$ incorporating time delays/feedback loops leads to LSTM gated memory principles\n",
    "\n",
    "#### 1.2.2 RNN types $[2]$\n",
    "\n",
    "<img src = \"img/coursera_rnn_types.png\" style = \"width: 500px\"/>\n",
    "\n",
    "#### 1.2.3 Issue of vanishing gradients, illustrated $[2]$\n",
    "\n",
    "<img src = \"img/coursera_vanishing_grads.png\" style = \"width: 500px\"/>\n",
    "    \n",
    "\n",
    "#### 1.2.4 GRU: gated recurrent unit\n",
    "- Gating mechanism in RNN: like LSTM with forget gate, but has fewer parameters (lacks output gate)\n",
    "- Also addresses the vanishing gradients problem\n",
    "- Can have better performance than LSTM on small datasets; but LSTM strictly stronger $[3]$ \n",
    "\n",
    "### 1.3 LSTM: Long Short-Term Memory\n",
    "\n",
    "- _Sources_ : \n",
    "    - $[1]$ [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) \n",
    "        - Original paper on LSTM, cited in Coursera\n",
    "    - $[2]$ [LSTM Wiki](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "    - $[3]$ [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "\n",
    "#### 1.3.1 Motivation $[1]$\n",
    "- DNNs can only be applied to poblems where inputs, targets encoded with vectors of fixed dimensionality; not good for speech\n",
    "- Conventional DNNs have issue of compounding error in backprop\n",
    "- _LSTM soln_ : when error values back-propagated, error remains in LSTM cell $\\implies$ \"error carousel\" feeds error back into each of LSTM gates, until they learn to cut off value\n",
    "\n",
    "\n",
    "#### 1.3.2 What is an LSTM? $[1, \\, 2]$\n",
    "- Has feedback connections, unlike standard _feedforward_ NNs; in feedforward, node connections don't form a cycle\n",
    "- _Idea_ :\n",
    "    - Cell remembers values, and three gates regulate flow of into into/out of cell\n",
    "    - Useful for data with lags\n",
    "    - Each gate has activation: marked increase in number of parameters\n",
    "    - Robustness to range of parameters due to error control\n",
    "    \n",
    "#### 1.3.3 LSTM units $[1]$\n",
    "- _LSTM unit_ :  \n",
    "    - __Cell__ : keeps track of dependencies between elements in input sequence\n",
    "    - __Input gate__ : controls net flow into cell\n",
    "    - __Forget gate__ : controls how much value remains in cell\n",
    "    - __Output gate__ : controls extent to which cell value used to compute activation\n",
    "- _Why gate units?_ :\n",
    "    - Avoids weight conflicts\n",
    "    - Input gate learns when to release errors by appropriate scaling\n",
    "    \n",
    "<img src = \"img/coursera_lstm.png\" style = \"width: 500px\"/>    \n",
    "    \n",
    "#### 1.3.4 Application to NLP models $[3]$\n",
    "- _Property of LSTM_ : learns to map input sentence of variable length onto fixed-dimensional vector representation\n",
    "    - Translation objective encourages LSTM to find sentence representations that capture meaning\n",
    "    - Paper claim: model aware of word order and fairly invariant to active/passive voice\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generalized Models\n",
    "\n",
    "***\n",
    "\n",
    "### 2.1 Seq2Seq\n",
    "\n",
    "- _Sources_ : \n",
    "    - [Seq2Seq documentation](https://google.github.io/seq2seq/)\n",
    "    - [Wiki](https://en.wikipedia.org/wiki/Seq2seq)\n",
    "\n",
    "#### 2.1.1 Model Overview\n",
    "- Turns one sequence into another; uses RNN, LSTM, or GRU\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Paper Implementations: Non-Parallel Corpora\n",
    "\n",
    "***\n",
    "\n",
    "### 3.1 [Style Transfer from Non-Parallel Text by Cross-Alignment](https://arxiv.org/pdf/1705.09655.pdf)\n",
    "\n",
    "- _Relevant outside sources_ :\n",
    "    - $[1]$ [Toward Controlled Generation of Text](https://arxiv.org/pdf/1703.00955.pdf) - latent spaces with generated text\n",
    "    - $[2]$ [InfoGAN: Representation Learning](https://arxiv.org/pdf/1606.03657.pdf) - more on latent spaces\n",
    "    - $[3]$ [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) - VAEs\n",
    "    - $[4]$ [CNN for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf) - model-based evaluation metric\n",
    "    - $[5]$ [word2vec](https://code.google.com/archive/p/word2vec/) - more on model-based evaluation\n",
    "    - $[6]$ [Professor Forcing](https://arxiv.org/pdf/1610.09038.pdf) - RNN training algorithm\n",
    "\n",
    "#### 3.1.1 Introduction\n",
    "- Learn encoder $\\to$ take style as input $\\to$ map to style-independent content\n",
    "- Do not use VAE ( _need to research_ ) - need to preserve latent content\n",
    "\n",
    "<img src = \"img/non_parallel_model.png\" style = \"width: 500px\"/>  \n",
    "\n",
    "- Task evaluation:\n",
    "    1. Sentiment modification\n",
    "    2. Decipherment of word substitution ciphers\n",
    "    3. Recovery of word order\n",
    "    \n",
    "#### 3.1.2 Related work\n",
    "- _Vision_ : cannot employ many similar vision methods due to discreteness of NLP\n",
    "    - Generating sentences with controllable attributes by learning disentangled latent representations $[1]$\n",
    "    - More reserach available on latent spaces $[2]$\n",
    "    \n",
    "#### 3.1.3 Formulation\n",
    "- _Motivation_ : want to recover joint distributions of style in order to facilitate style transfer\n",
    "    - __Claim__ : datasets from different styles need to be distinct enough for this paper's methods to work\n",
    "- _Conclusions_ : latent content should have most complexity from input, and latent style variable should have simple effects\n",
    "\n",
    "##### 3.1.3 (a) - Data Generation\n",
    "1. Latent style variable $y$ generated from $p(y)$\n",
    "2. Latent content variable generated from $p(z)$\n",
    "3. Datapoint $x$ generated from conditional dist $p(x \\mid y, z)$\n",
    "\n",
    "##### 3.1.3 (b) - Setup\n",
    "- Two data sets $X_1$ (drawn from $p(x_1, y_1)$), $X_2$ with same content, but different style (drawn from $p(x_2, y_2)$): $y_1, y_2$, both unknown\n",
    "- Want to estimate style transfer functions: $p(x_1 \\mid x_2 ; y_1, y_2)$ and $p(x_2 \\mid x_1; y_1, y_2)$\n",
    "\n",
    "#### 3.1.4 Model\n",
    "##### 3.1.4 (a) - Overview\n",
    "- Use of auto-encoder model:\n",
    "    1. Encoding step to infer content of target\n",
    "    2. Decoding step to generate transferred counterpart\n",
    "- Need content space of source and target to coincide; could employ VAE $[3]$\n",
    "    \n",
    "##### 3.1.4 (b) - Model specifics and loss\n",
    "- Encoder E: infers content $z$ from sentence $x$ and style $y$\n",
    "- Generator G: generates sentence $x$ from from style $y$ and content $z$\n",
    "- E, G form autoencoder when applied to same style\n",
    "\n",
    "<img src = \"img/csail_autoencoder_loss.png\"/>  \n",
    "\n",
    "- Align posteriors to have same distribution; use adversarial discriminator to distinguish between the dists\\\n",
    "- _Goal_ : minimize encoder, generator loss; maximize discriminator distinction\n",
    "\n",
    "\n",
    "<img src = \"img/csail_loss_function.png\"/>  \n",
    "    \n",
    "##### 3.1.4 (c) -  Proposed styles\n",
    "\n",
    "1. _Aligned auto-encoder_ :\n",
    "\n",
    "    - Implement encoder & decoder using single-layer RNNs with GRU\n",
    "    - Need to align distributions: use discriminator D as feed-forward NN with single hidden layer, sigmoid output\n",
    "    - Complicated loss function (see above)\n",
    "    \n",
    "2. _Cross-aligned auto-encoder_ :\n",
    "\n",
    "    - Use two discriminators: $D_1$, $D_2$\n",
    "        - $D_1$ : distinguish between real $x_1$ and transferred $x_2$\n",
    "        - $D_2$ : distinguish between real $x_2$ and transferred $x_1$\n",
    "        \n",
    "##### 3.1.4 (d) - Implementation\n",
    "\n",
    "1. Use softmax distribution over words\n",
    "    - During generating process of transferred $x_2$ from $G(y_1, z_2)$, feed peaked $\\text{softmax}(a^{[\\ell]} / \\gamma)$ into next input\n",
    "2. Use _Professor-Forcing_ $[6]$ to \"match sequence of hidden states instead of output words\"\n",
    "    - Hidden states: information about outputs, and smoothly distributed\n",
    "    - Input to $D_1$ : sequence of hidden states, either from:\n",
    "        1. Teacher forcing by real example $x_1$; $G(y_1, z_1)$\n",
    "        2. Self-fed by previous softmaxes; $G(y_1, z_2)$\n",
    "    \n",
    "        \n",
    "#### 3.1.5 Evaluation\n",
    "- Used model-based evaluation metric for sentiment modification: measure how often a transferred sentence has correct sentiment according to pre-trained sentiment classifier (see $[4, 5]$)\n",
    "\n",
    "<img src = \"img/csail_professor_forcing.png\"/> \n",
    "    \n",
    "***\n",
    "\n",
    "### 3.2 [Neural Style Transfer for Non-Parallel Text](https://github.com/vinitra/neural-text-style-transfer/blob/master/reports/Neural_Style_Transfer_for_Non_Parallel_Text.pdf)\n",
    "\n",
    "- _Note_ : essentially a plain-English follow-up to approach in the CSAIL paper (3.1)\n",
    "\n",
    "***\n",
    "\n",
    "### 3.2 [Disentangled Representation Learning for Non-Parallel Text Style Transfer](https://arxiv.org/pdf/1808.04339.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
