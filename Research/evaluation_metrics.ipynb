{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation & Loss Functions\n",
    "\n",
    "__Contents__ :\n",
    "\n",
    "1. [Disentangled Representation Learning](#paper_1), John et al. 2018\n",
    "    - Loss functions: [multitask](#paper_1_mult), [discriminator](#paper_1_disc), [KL regularization](#paper_1_KL), [reconstruction](#paper_1_recon)\n",
    "    - [Evaluation metrics](#paper_1_eval):\n",
    "        - _Style loss_ : pretrained classifier\n",
    "        - _Content loss_ : cosine similarity for pure content, KN language smoother for fluency\n",
    "    \n",
    "2. [Adversarially Regularized Autoencoders](#paper_2), Zhao et al. 2018\n",
    "\n",
    "3. [Style Transformer](#paper_3), Dai et al. 2019\n",
    "    - _Evaluation_ :\n",
    "        - [Reqiurements](#paper_3_eval)\n",
    "        - Metrics: [style](#paper_3_eval_style) (classification), [content](#paper_3_eval_content) (BLEU), [fluency](#paper_3_eval_fluency) (PPL)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_1\"></a>\n",
    "## 1. [Disentangled Representation Learning for Non-Parallel Text Style Transfer](https://arxiv.org/pdf/1808.04339.pdf)\n",
    "\n",
    "- _Authors_ : John et al. 2018\n",
    "- _Source code_ : [torch reimplementation](https://github.com/h3lio5/linguistic-style-transfer-pytorch)\n",
    "\n",
    "### 1.1 Loss\n",
    "\n",
    "- _Source_ : [model.py](https://github.com/h3lio5/linguistic-style-transfer-pytorch/blob/master/linguistic_style_transfer_pytorch/model.py)\n",
    "\n",
    "- _Math_ : $J_{TOT} = J_{VAE} + \\lambda_{mul(s)} J_{mul(s)} + \\lambda_{adv(s)} J_{adv(s)} + \\lambda_{mul(c)} J_{mul(c)} + \\lambda_{adv(c)} J_{adv(c)}$\n",
    "    \n",
    "    and $J_{VAE} = J_{AE} + KL$ for standard autoencoding expectation-based loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from linguistic_style_transfer_pytorch.config import ModelConfig, GeneralConfig\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import math\n",
    "\n",
    "mconfig = ModelConfig()\n",
    "gconfig = GeneralConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_1_mult\"></a>\n",
    "#### 1.1.1 Multitask loss\n",
    "- _Style classifier_ : trained model that predicts the style label -- $J_{mul(s)}$\n",
    "- _Content classifier_ : predicts bag of words (BoW) representation of sentence -- $J_{mul(c)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content multitask loss\n",
    "def get_content_mul_loss(self, content_emb, content_bow):\n",
    "        \"\"\"\n",
    "        This loss quantifies the amount of content information preserved\n",
    "        in the content space\n",
    "        Returns:\n",
    "        cross entropy loss of the content classifier\n",
    "        \"\"\"\n",
    "        # predictions\n",
    "        preds = nn.Softmax(dim=1)(\n",
    "            self.content_classifier(self.dropout(content_emb)))\n",
    "        # label smoothing\n",
    "        smoothed_content_bow = content_bow * \\\n",
    "            (1-mconfig.label_smoothing) + \\\n",
    "            mconfig.label_smoothing/mconfig.content_bow_dim\n",
    "        # calculate cross entropy loss\n",
    "        content_mul_loss = nn.BCELoss()(preds, smoothed_content_bow)\n",
    "\n",
    "        return content_mul_loss\n",
    "\n",
    "# Style multitask loss\n",
    "def get_style_mul_loss(self, style_emb, style_labels):\n",
    "        \"\"\"\n",
    "        This loss quantifies the amount of style information preserved\n",
    "        in the style space\n",
    "        Returns:\n",
    "        cross entropy loss of the style classifier\n",
    "        \"\"\"\n",
    "        # predictions\n",
    "        preds = nn.Softmax(dim=1)(\n",
    "            self.style_classifier(self.dropout(style_emb)))\n",
    "        # label smoothing\n",
    "        smoothed_style_labels = style_labels * \\\n",
    "            (1-mconfig.label_smoothing) + \\\n",
    "            mconfig.label_smoothing/mconfig.num_style\n",
    "        # calculate cross entropy loss\n",
    "        style_mul_loss = nn.BCELoss()(preds, smoothed_style_labels)\n",
    "        \n",
    "        return style_mul_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_1_disc\"></a>\n",
    "#### 1.1.2 Discriminator Loss\n",
    "\n",
    "- _Style discriminator_ : trained to predict style label -- $J_{adv(s)}$\n",
    "- _Style generator_ : trained to increase entropy/likelihood of predictions\n",
    "- _Content discriminator, generator_ work in the same way -- $J_{adv(c)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial content generator/'predictor'\n",
    "def get_content_disc_preds(self, style_emb):\n",
    "        \"\"\"\n",
    "        Returns predictions about the content using style embedding\n",
    "        as input\n",
    "        output shape : [batch_size,content_bow_dim]\n",
    "        \"\"\"\n",
    "        # predictions\n",
    "        # Note: detach the style embedding since when don't want the gradient to flow\n",
    "        #       all the way to the encoder. content_disc_loss is used only to change the\n",
    "        #       parameters of the discriminator network\n",
    "        preds = nn.Softmax(dim=1)(self.content_disc(\n",
    "            self.dropout(style_emb.detach())))\n",
    "\n",
    "        return preds\n",
    "\n",
    "# Adversarial content discriminator loss\n",
    "def get_content_disc_loss(self, content_disc_preds, content_bow):\n",
    "        \"\"\"\n",
    "        It essentially quantifies the amount of information about content\n",
    "        contained in the style space\n",
    "        Returns:\n",
    "        cross entropy loss of content discriminator\n",
    "        \"\"\"\n",
    "        # label smoothing\n",
    "        smoothed_content_bow = content_bow * \\\n",
    "            (1-mconfig.label_smoothing) + \\\n",
    "            mconfig.label_smoothing/mconfig.content_bow_dim\n",
    "        # calculate cross entropy loss\n",
    "        content_disc_loss = nn.BCELoss()(content_disc_preds, smoothed_content_bow)\n",
    "\n",
    "        return content_disc_loss\n",
    "    \n",
    "# Adversarial style generator/\"predictor\"\n",
    "def get_style_disc_preds(self, content_emb):\n",
    "        \"\"\"\n",
    "        Returns predictions about style using content embeddings\n",
    "        as input\n",
    "        output shape: [batch_size,num_style]\n",
    "        \"\"\"\n",
    "        # predictions\n",
    "        # Note: detach the content embedding since when don't want the gradient to flow\n",
    "        #       all the way to the encoder. style_disc_loss is used only to change the\n",
    "        #       parameters of the discriminator network\n",
    "        preds = nn.Softmax(dim=1)(self.style_disc(\n",
    "            self.dropout(content_emb.detach())))\n",
    "\n",
    "        return preds\n",
    "    \n",
    "# Adversarial style discriminator loss\n",
    "def get_style_disc_loss(self, style_disc_preds, style_labels):\n",
    "        \"\"\"\n",
    "        It essentially quantifies the amount of information about style\n",
    "        contained in the content space\n",
    "        Returns:\n",
    "        cross entropy loss of style discriminator\n",
    "        \"\"\"\n",
    "        # label smoothing\n",
    "        smoothed_style_labels = style_labels * \\\n",
    "            (1-mconfig.label_smoothing) + \\\n",
    "            mconfig.label_smoothing/mconfig.num_style\n",
    "        # calculate cross entropy loss\n",
    "\n",
    "        style_disc_loss = nn.BCELoss()(style_disc_preds, smoothed_style_labels)\n",
    "\n",
    "        return style_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_1_KL\"></a>\n",
    "#### 1.1.3 KL regularization\n",
    "\n",
    "- Regularization to ensure that reconstruction from encoder/decoder is good -- $KL$\n",
    "- Measures how much information is lost during encoding/decoding\n",
    "- This model, like most VAEs, assumes the latent model is Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL divergence loss\n",
    "\n",
    "def get_kl_loss(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mu: batch of means of the gaussian distribution followed by the latent variables\n",
    "            log_var: batch of log variances(log_var) of the gaussian distribution followed by the latent variables\n",
    "        Returns:\n",
    "            total loss(float)\n",
    "        \"\"\"\n",
    "        kl_loss = torch.mean((-0.5*torch.sum(1+log_var -\n",
    "                                             log_var.exp()-mu.pow(2), dim=1)))\n",
    "        return kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_1_recon\"></a>\n",
    "#### 1.1.4 VAE autoencoding loss\n",
    "\n",
    "- Loss of reconstructing the input from VAE model -- $J_{AE}$\n",
    "- Employs KL regularization above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoding/'reconstruction' loss\n",
    "\n",
    "def get_recon_loss(self, output_logits, input_sentences):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output_logits: logits of output sentences at each time step, shape = (max_seq_length,batch_size,vocab_size)\n",
    "            input_sentences: batch of token indices of input sentences, shape = (batch_size,max_seq_length)\n",
    "        Returns:\n",
    "            reconstruction loss calculated using cross entropy loss function\n",
    "        \"\"\"\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        recon_loss = loss(\n",
    "            output_logits.view(-1, mconfig.vocab_size), input_sentences.view(-1))\n",
    "\n",
    "        return recon_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<a name=\"paper_1_eval\"></a>\n",
    "### 1.2 Evaluation\n",
    "\n",
    "#### 1.2.1 Style Transfer\n",
    "- Evaluate based on CNN to predict sentiment, based on implementation of word2vec sentiment classifier [(Kim 2014)](https://arxiv.org/pdf/1408.5882.pdf)\n",
    "\n",
    "#### 1.2.2 Content Preservation\n",
    "- _Cosine similarity_ between source, generated sentence embeddings\n",
    "- _Word overlap_ : unigram word overlap rate of original and style transferred sentence\n",
    "- _Language fluency_ : trigram Kneser-Ney smoothed language model; number $\\approx 0 \\implies$ more fluent\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_2\"></a>\n",
    "## 2. [Adversarially Regularized Autoencoders](https://arxiv.org/pdf/1706.04223.pdf)\n",
    "\n",
    "- _Authors_ : Zhao et al. 2018\n",
    "- _Source code_ : [ARAE](https://github.com/jakezhaojb/ARAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Evaluation\n",
    "\n",
    "- _Source_ : [utils.py](https://github.com/jakezhaojb/ARAE/blob/master/lang/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_3\"></a>\n",
    "## 3. [Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation](https://arxiv.org/pdf/1905.05621.pdf)\n",
    "\n",
    "- _Authors_ : Dai et al. 2019\n",
    "- _Source code_ : [fastnlp](https://github.com/fastnlp/style-transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_3_eval\"></a>\n",
    "\n",
    "### 3.1 Evaluation\n",
    "\n",
    "- _Source_ : [evaluator.py](https://github.com/fastnlp/style-transformer/blob/master/evaluator/evaluator.py) -- called from `Evaluator` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kenlm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-08b321f74272>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkenlm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kenlm'"
     ]
    }
   ],
   "source": [
    "# Requirements\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import fasttext\n",
    "import pkg_resources\n",
    "import kenlm\n",
    "import math\n",
    "\n",
    "def __init__(self):\n",
    "        resource_package = __name__\n",
    "\n",
    "        yelp_acc_path = 'acc_yelp.bin'\n",
    "        yelp_ppl_path = 'ppl_yelp.binary'\n",
    "        yelp_ref0_path = 'yelp.refs.0'\n",
    "        yelp_ref1_path = 'yelp.refs.1'\n",
    "\n",
    "        \n",
    "        yelp_acc_file = pkg_resources.resource_stream(resource_package, yelp_acc_path)\n",
    "        yelp_ppl_file = pkg_resources.resource_stream(resource_package, yelp_ppl_path)\n",
    "        yelp_ref0_file = pkg_resources.resource_stream(resource_package, yelp_ref0_path)\n",
    "        yelp_ref1_file = pkg_resources.resource_stream(resource_package, yelp_ref1_path)\n",
    "\n",
    "        \n",
    "        self.yelp_ref = []\n",
    "        with open(yelp_ref0_file.name, 'r') as fin:\n",
    "            self.yelp_ref.append(fin.readlines())\n",
    "        with open(yelp_ref1_file.name, 'r') as fin:\n",
    "            self.yelp_ref.append(fin.readlines())\n",
    "        self.classifier_yelp = fasttext.load_model(yelp_acc_file.name)\n",
    "        self.yelp_ppl_model = kenlm.Model(yelp_ppl_file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_3_eval_style\"></a>\n",
    "#### 3.1.1 Style Evaluation\n",
    "- Evaluate based on two sentiment classifiers trained from [_fastText_](https://github.com/facebookresearch/fastText) (FaceBook classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style check from fastText model\n",
    "\n",
    "def yelp_style_check(self, text_transfered, style_origin):\n",
    "        text_transfered = ' '.join(word_tokenize(text_transfered.lower().strip()))\n",
    "        if text_transfered == '':\n",
    "            return False\n",
    "        label = self.classifier_yelp.predict([text_transfered])\n",
    "        style_transfered = label[0][0] == '__label__positive'\n",
    "        return (style_transfered != style_origin)\n",
    "    \n",
    "# Checking the accuracy for different styles\n",
    "    \n",
    "def yelp_acc_b(self, texts, styles_origin):\n",
    "        assert len(texts) == len(styles_origin), 'Size of inputs does not match!'\n",
    "        count = 0\n",
    "        for text, style in zip(texts, styles_origin):\n",
    "            if self.yelp_style_check(text, style):\n",
    "                count += 1\n",
    "        return count / len(texts)\n",
    "\n",
    "def yelp_acc_0(self, texts):\n",
    "        styles_origin = [0] * len(texts)\n",
    "        return self.yelp_acc_b(texts, styles_origin)\n",
    "\n",
    "def yelp_acc_1(self, texts):\n",
    "        styles_origin = [1] * len(texts)\n",
    "        return self.yelp_acc_b(texts, styles_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_3_eval_content\"></a>\n",
    "\n",
    "#### 3.1.2 Content Evaluation\n",
    "\n",
    "- Calculate BLEU score between transferred sentence and input using NLTK\n",
    "- Higher BLEU $\\implies$ kept more words from source -- maybe not the best eval for our model since content $\\neq$ literal words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NLTK model\n",
    "def nltk_bleu(self, texts_origin, text_transfered):\n",
    "        texts_origin = [word_tokenize(text_origin.lower().strip()) for text_origin in texts_origin]\n",
    "        text_transfered = word_tokenize(text_transfered.lower().strip())\n",
    "        return sentence_bleu(texts_origin, text_transfered) * 100\n",
    "\n",
    "# Check the BLEU diff between original & transferred text\n",
    "def self_bleu_b(self, texts_origin, texts_transfered):\n",
    "        assert len(texts_origin) == len(texts_transfered), 'Size of inputs does not match!'\n",
    "        sum = 0\n",
    "        n = len(texts_origin)\n",
    "        for x, y in zip(texts_origin, texts_transfered):\n",
    "            sum += self.nltk_bleu([x], y)\n",
    "        return sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"paper_3_eval_fluency\"></a>\n",
    "\n",
    "#### 3.1.3 Fluency Evaluation\n",
    "\n",
    "- Measures perplexity of transferred sentence by training 5-gram language model on two datasets using KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures perplexity of language model\n",
    "\n",
    "def yelp_ppl(self, texts_transfered):\n",
    "        texts_transfered = [' '.join(word_tokenize(itm.lower().strip())) for itm in texts_transfered]\n",
    "        sum = 0\n",
    "        words = []\n",
    "        length = 0\n",
    "        for i, line in enumerate(texts_transfered):\n",
    "            words += [word for word in line.split()]\n",
    "            length += len(line.split())\n",
    "            score = self.yelp_ppl_model.score(line)\n",
    "            sum += score\n",
    "        return math.pow(10, -sum / length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
