{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation & Loss Functions\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [Disentangled Representation Learning for Non-Parallel Text Style Transfer](https://arxiv.org/pdf/1808.04339.pdf)\n",
    "\n",
    "- Code source: [_torch reimplementation_](https://github.com/h3lio5/linguistic-style-transfer-pytorch)\n",
    "\n",
    "### 1.1 Loss Functions\n",
    "\n",
    "- Source: [_model.py_](https://github.com/h3lio5/linguistic-style-transfer-pytorch/blob/master/linguistic_style_transfer_pytorch/model.py)\n",
    "\n",
    "- _Math_ : $J_{TOT} = J_{VAE} + \\lambda_{mul(s)} J_{mul(s)} + \\lambda_{adv(s)} J_{adv(s)} + \\lambda_{mul(c)} J_{mul(c)} + \\lambda_{adv(c)} J_{adv(c)}$\n",
    "    \n",
    "    and $J_{VAE} = J_{AE} + KL$ for standard autoencoding expectation-based loss\n",
    "\n",
    "#### 1.1.1 Multitask loss\n",
    "- _Style classifier_ : trained model that predicts the style label -- $J_{mul(s)}$\n",
    "- _Content classifier_ : predicts bag of words (BoW) representation of sentence -- $J_{mul(c)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content multitask loss\n",
    "def get_content_mul_loss(self, content_emb, content_bow):\n",
    "        \"\"\"\n",
    "        This loss quantifies the amount of content information preserved\n",
    "        in the content space\n",
    "        Returns:\n",
    "        cross entropy loss of the content classifier\n",
    "        \"\"\"\n",
    "        # predictions\n",
    "        preds = nn.Softmax(dim=1)(\n",
    "            self.content_classifier(self.dropout(content_emb)))\n",
    "        # label smoothing\n",
    "        smoothed_content_bow = content_bow * \\\n",
    "            (1-mconfig.label_smoothing) + \\\n",
    "            mconfig.label_smoothing/mconfig.content_bow_dim\n",
    "        # calculate cross entropy loss\n",
    "        content_mul_loss = nn.BCELoss()(preds, smoothed_content_bow)\n",
    "\n",
    "        return content_mul_loss\n",
    "\n",
    "# Style multitask loss\n",
    "def get_style_mul_loss(self, style_emb, style_labels):\n",
    "        \"\"\"\n",
    "        This loss quantifies the amount of style information preserved\n",
    "        in the style space\n",
    "        Returns:\n",
    "        cross entropy loss of the style classifier\n",
    "        \"\"\"\n",
    "        # predictions\n",
    "        preds = nn.Softmax(dim=1)(\n",
    "            self.style_classifier(self.dropout(style_emb)))\n",
    "        # label smoothing\n",
    "        smoothed_style_labels = style_labels * \\\n",
    "            (1-mconfig.label_smoothing) + \\\n",
    "            mconfig.label_smoothing/mconfig.num_style\n",
    "        # calculate cross entropy loss\n",
    "        style_mul_loss = nn.BCELoss()(preds, smoothed_style_labels)\n",
    "        \n",
    "        return style_mul_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Discriminator Loss\n",
    "\n",
    "- _Style discriminator_ : trained to predict style label -- $J_{adv(s)}$\n",
    "- _Style generator_ : trained to increase entropy/likelihood of predictions\n",
    "- _Content discriminator, generator_ work in the same way -- $J_{adv(c)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial content generator/'predictor'\n",
    "def get_content_disc_preds(self, style_emb):\n",
    "        \"\"\"\n",
    "        Returns predictions about the content using style embedding\n",
    "        as input\n",
    "        output shape : [batch_size,content_bow_dim]\n",
    "        \"\"\"\n",
    "        # predictions\n",
    "        # Note: detach the style embedding since when don't want the gradient to flow\n",
    "        #       all the way to the encoder. content_disc_loss is used only to change the\n",
    "        #       parameters of the discriminator network\n",
    "        preds = nn.Softmax(dim=1)(self.content_disc(\n",
    "            self.dropout(style_emb.detach())))\n",
    "\n",
    "        return preds\n",
    "\n",
    "# Adversarial content discriminator loss\n",
    "def get_content_disc_loss(self, content_disc_preds, content_bow):\n",
    "        \"\"\"\n",
    "        It essentially quantifies the amount of information about content\n",
    "        contained in the style space\n",
    "        Returns:\n",
    "        cross entropy loss of content discriminator\n",
    "        \"\"\"\n",
    "        # label smoothing\n",
    "        smoothed_content_bow = content_bow * \\\n",
    "            (1-mconfig.label_smoothing) + \\\n",
    "            mconfig.label_smoothing/mconfig.content_bow_dim\n",
    "        # calculate cross entropy loss\n",
    "        content_disc_loss = nn.BCELoss()(content_disc_preds, smoothed_content_bow)\n",
    "\n",
    "        return content_disc_loss\n",
    "    \n",
    "# Adversarial style generator/\"predictor\"\n",
    "def get_style_disc_preds(self, content_emb):\n",
    "        \"\"\"\n",
    "        Returns predictions about style using content embeddings\n",
    "        as input\n",
    "        output shape: [batch_size,num_style]\n",
    "        \"\"\"\n",
    "        # predictions\n",
    "        # Note: detach the content embedding since when don't want the gradient to flow\n",
    "        #       all the way to the encoder. style_disc_loss is used only to change the\n",
    "        #       parameters of the discriminator network\n",
    "        preds = nn.Softmax(dim=1)(self.style_disc(\n",
    "            self.dropout(content_emb.detach())))\n",
    "\n",
    "        return preds\n",
    "    \n",
    "# Adversarial style discriminator loss\n",
    "def get_style_disc_loss(self, style_disc_preds, style_labels):\n",
    "        \"\"\"\n",
    "        It essentially quantifies the amount of information about style\n",
    "        contained in the content space\n",
    "        Returns:\n",
    "        cross entropy loss of style discriminator\n",
    "        \"\"\"\n",
    "        # label smoothing\n",
    "        smoothed_style_labels = style_labels * \\\n",
    "            (1-mconfig.label_smoothing) + \\\n",
    "            mconfig.label_smoothing/mconfig.num_style\n",
    "        # calculate cross entropy loss\n",
    "\n",
    "        style_disc_loss = nn.BCELoss()(style_disc_preds, smoothed_style_labels)\n",
    "\n",
    "        return style_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 KL regularization\n",
    "\n",
    "- Regularization to ensure that reconstruction from encoder/decoder is good -- $KL$\n",
    "- Measures how much information is lost during encoding/decoding\n",
    "- This model, like most VAEs, assumes the latent model is Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL divergence loss\n",
    "\n",
    "def get_kl_loss(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mu: batch of means of the gaussian distribution followed by the latent variables\n",
    "            log_var: batch of log variances(log_var) of the gaussian distribution followed by the latent variables\n",
    "        Returns:\n",
    "            total loss(float)\n",
    "        \"\"\"\n",
    "        kl_loss = torch.mean((-0.5*torch.sum(1+log_var -\n",
    "                                             log_var.exp()-mu.pow(2), dim=1)))\n",
    "        return kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 VAE autoencoding loss\n",
    "\n",
    "- Loss of reconstructing the input from VAE model -- $J_{AE}$\n",
    "- Employs KL regularization above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoding/'reconstruction' loss\n",
    "\n",
    "def get_recon_loss(self, output_logits, input_sentences):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output_logits: logits of output sentences at each time step, shape = (max_seq_length,batch_size,vocab_size)\n",
    "            input_sentences: batch of token indices of input sentences, shape = (batch_size,max_seq_length)\n",
    "        Returns:\n",
    "            reconstruction loss calculated using cross entropy loss function\n",
    "        \"\"\"\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        recon_loss = loss(\n",
    "            output_logits.view(-1, mconfig.vocab_size), input_sentences.view(-1))\n",
    "\n",
    "        return recon_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 1.2 Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
