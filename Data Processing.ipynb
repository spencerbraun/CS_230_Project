{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/asap-aes/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-104fb36c8054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0messayList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0messay_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/asap-aes/train.tsv'"
     ]
    }
   ],
   "source": [
    "path = \"data/raw/\"\n",
    "essay_file = path+ \"asap-aes/train.tsv\"\n",
    "\n",
    "i=0\n",
    "\n",
    "essayList = []\n",
    "with open(essay_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"Id\"):\n",
    "            continue\n",
    "        splitLine = line.strip().split(\"\\t\")\n",
    "        scores = map(int, splitLine[2:4])\n",
    "        avgscore = sum(scores)/2\n",
    "        essay = sent_tokenize(splitLine[4])\n",
    "        for sent in essay:\n",
    "            essayList.append([sent, avgscore])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/raw/\"\n",
    "short_answer_file = path+ \"asap-sas/train.tsv\"\n",
    "\n",
    "i=0\n",
    "\n",
    "dataList = []\n",
    "with open(short_answer_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"Id\"):\n",
    "            continue\n",
    "        splitLine = line.strip().split(\"\\t\")\n",
    "        scores = map(int, splitLine[2:4])\n",
    "        avgscore = sum(scores)/2\n",
    "        essay = sent_tokenize(splitLine[4])\n",
    "        for sent in essay:\n",
    "            dataList.append([sent, avgscore])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "highScores = []\n",
    "lowScores = []\n",
    "for data in dataList:\n",
    "    if len(word_tokenize(data[0])) > 20:\n",
    "        continue\n",
    "    if data[1] >= 2:\n",
    "        highScores.append(data[0])\n",
    "    if data[1] ==0:\n",
    "        lowScores.append(data[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('sas_high.csv', 'w', newline='\\n') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(highScores)\n",
    "    \n",
    "\n",
    "with open('sas_low.csv', 'w', newline='\\n') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(lowScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 11279,\n",
       "         0.0: 11879,\n",
       "         2.0: 11669,\n",
       "         0.5: 2255,\n",
       "         3.0: 2247,\n",
       "         2.5: 510,\n",
       "         1.5: 3216})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([x[1] for x in dataList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.shuffle(cleanLow)\n",
    "train_low = cleanLow[:5000]\n",
    "test_low = cleanLow[5500:]\n",
    "dev_low = cleanLow[5000:5500]\n",
    "\n",
    "random.shuffle(cleanHigh)\n",
    "train_high = cleanHigh[:5000]\n",
    "test_high = cleanHigh[5500:]\n",
    "dev_high = cleanHigh[5000:5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean(data_list):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    newList = []\n",
    "    for sent in data_list:\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        #sent = re.sub(r'\\d+', '', sent)\n",
    "        \n",
    "\n",
    "        #sent = re.sub('['+string.punctuation+']', '', sent)\n",
    "        sent = sent.lower()\n",
    "        \n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanLow = clean(lowScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanHigh = clean(highScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6057"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanLow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7221"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanHigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sas_data.1', 'w') as f:\n",
    "    f.writelines(cleanHigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sas_data.0', 'w') as f:\n",
    "    f.writelines(cleanLow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_label = [1 for x in cleanHigh]\n",
    "low_label = [0 for x in cleanLow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = high_label + low_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = cleanHigh + cleanLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(all_data, labels))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "shuff_data, shuff_labels = zip(*c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../text_style_transfer/model/data/sas_data', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', shuff_data)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "# with open('../text_style_transfer/model/data/sas.test.1', 'w') as f:\n",
    "#     retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', test_high)\n",
    "#     f.writelines(retokenized)\n",
    "    \n",
    "# with open('../text_style_transfer/model/data/sas.dev.1', 'w') as f:\n",
    "#     retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', dev_high)\n",
    "#     f.writelines(retokenized)\n",
    "\n",
    "with open('../text_style_transfer/model/data/sas_labels', 'w') as f:\n",
    "    f.writelines(map(lambda x: str(x) + '\\n', shuff_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('../language-style-transfer/data/sas.train.1', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', train_high)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.test.1', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', test_high)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.dev.1', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', dev_high)\n",
    "    f.writelines(retokenized)\n",
    "\n",
    "with open('../language-style-transfer/data/sas.train.0', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', train_low)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.test.0', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', test_low)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.dev.0', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', dev_low)\n",
    "    f.writelines(retokenized)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You would need many more pieces of information to replicate the experiment .'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(word_tokenize(highScores[0][0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some additional information that we would need to replicate the experiment is how much vinegar should be placed in each identical container how or what tool to use to measure the mass of the four different samples and how much distilled water to use to rinse the four samples after taking them out of the vinegar'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(dataList[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, vocab_file, emb_file='', dim_emb=0):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.size, self.word2id, self.id2word = pickle.load(f)\n",
    "        self.dim_emb = dim_emb\n",
    "        self.embedding = np.random.random_sample(\n",
    "            (self.size, self.dim_emb)) - 0.5\n",
    "\n",
    "        if emb_file:\n",
    "            print( 'Loading word vectors from', emb_file)\n",
    "            with open(emb_file) as f:\n",
    "                for line in f:\n",
    "                    parts = line.split()\n",
    "                    word = parts[0]\n",
    "                    vec = np.array([float(x) for x in parts[1:]])\n",
    "                    if word in self.word2id:\n",
    "                        self.embedding[self.word2id[word]] = vec\n",
    "\n",
    "        for i in range(self.size):\n",
    "            self.embedding[i] /= LA.norm(self.embedding[i])\n",
    "\n",
    "def build_vocab(data, path, min_occur=5):\n",
    "    word2id = {'<pad>':0, '<go>':1, '<eos>':2, '<unk>':3}\n",
    "    id2word = ['<pad>', '<go>', '<eos>', '<unk>']\n",
    "\n",
    "    words = [word for sent in data for word in sent]\n",
    "    cnt = Counter(words)\n",
    "    for word in cnt:\n",
    "        if cnt[word] >= min_occur:\n",
    "            word2id[word] = len(word2id)\n",
    "            id2word.append(word)\n",
    "    vocab_size = len(word2id)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump((vocab_size, word2id, id2word), f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/data/sas.test.0') as f:\n",
    "    data_0 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/data/sas.test.1') as f:\n",
    "    data_1 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/tmp/sas.test.0.tsf') as f:\n",
    "    test_0 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/tmp/sas.test.1.tsf') as f:\n",
    "    test_1 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_unk(sent):\n",
    "    sent = re.sub(r'<unk>', '', sent)\n",
    "    \n",
    "    return word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_strip = list(map(strip_unk, test_1))\n",
    "test_0_strip = list(map(strip_unk, test_0))\n",
    "data_1_strip = list(map(strip_unk, data_1))\n",
    "data_0_strip = list(map(strip_unk, data_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu(sent1, sent2):\n",
    "    \n",
    "    score = nltk.translate.bleu_score.sentence_bleu([sent1], sent2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bleus_1 = []\n",
    "for i in range(len(test_1_strip)):\n",
    "    bleus_1.append(bleu(data_1_strip[i],test_1_strip[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'author',\n",
       "  'first',\n",
       "  'starts',\n",
       "  'out',\n",
       "  'with',\n",
       "  'the',\n",
       "  'introduction',\n",
       "  'which',\n",
       "  'explains',\n",
       "  'what',\n",
       "  'it',\n",
       "  'is',\n",
       "  'about'],\n",
       " ['the',\n",
       "  'author',\n",
       "  'organizes',\n",
       "  'the',\n",
       "  'article',\n",
       "  'in',\n",
       "  'what',\n",
       "  'it',\n",
       "  'is',\n",
       "  'going',\n",
       "  'about',\n",
       "  'space',\n",
       "  'junk'])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_strip[0],test_1_strip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['as',\n",
       "  'the',\n",
       "  'article',\n",
       "  'states',\n",
       "  'in',\n",
       "  'paragraph',\n",
       "  'the',\n",
       "  'snakes',\n",
       "  'have',\n",
       "  'imperiled',\n",
       "  'endangered',\n",
       "  'species',\n",
       "  'in',\n",
       "  'the',\n",
       "  'florida',\n",
       "  'keys'],\n",
       " ['in', 'the', 'and', 'the', 'koala', 'are', 'different', 'of', 'the'])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_strip[1],test_1_strip[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
