{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/asap-aes/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-104fb36c8054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0messayList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0messay_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/asap-aes/train.tsv'"
     ]
    }
   ],
   "source": [
    "path = \"data/raw/\"\n",
    "essay_file = path+ \"asap-aes/train.tsv\"\n",
    "\n",
    "i=0\n",
    "\n",
    "essayList = []\n",
    "with open(essay_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"Id\"):\n",
    "            continue\n",
    "        splitLine = line.strip().split(\"\\t\")\n",
    "        scores = map(int, splitLine[2:4])\n",
    "        avgscore = sum(scores)/2\n",
    "        essay = sent_tokenize(splitLine[4])\n",
    "        for sent in essay:\n",
    "            essayList.append([sent, avgscore])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/raw/\"\n",
    "short_answer_file = path+ \"asap-sas/train.tsv\"\n",
    "\n",
    "i=0\n",
    "\n",
    "dataList = []\n",
    "with open(short_answer_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"Id\"):\n",
    "            continue\n",
    "        splitLine = line.strip().split(\"\\t\")\n",
    "        scores = map(int, splitLine[2:4])\n",
    "        avgscore = sum(scores)/2\n",
    "        essay = sent_tokenize(splitLine[4])\n",
    "        for sent in essay:\n",
    "            dataList.append([sent, avgscore])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highScores = []\n",
    "lowScores = []\n",
    "for data in dataList:\n",
    "    if len(word_tokenize(data[0])) > 20:\n",
    "        continue\n",
    "    if data[1] >= 2:\n",
    "        highScores.append(data[0])\n",
    "    if data[1] ==0:\n",
    "        lowScores.append(data[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('sas_high.csv', 'w', newline='\\n') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(highScores)\n",
    "    \n",
    "\n",
    "with open('sas_low.csv', 'w', newline='\\n') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(lowScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 11279,\n",
       "         0.0: 11879,\n",
       "         2.0: 11669,\n",
       "         0.5: 2255,\n",
       "         3.0: 2247,\n",
       "         2.5: 510,\n",
       "         1.5: 3216})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([x[1] for x in dataList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "random.shuffle(cleanLow)\n",
    "train_low = cleanLow[:5000]\n",
    "test_low = cleanLow[5500:]\n",
    "dev_low = cleanLow[5000:5500]\n",
    "\n",
    "random.shuffle(cleanHigh)\n",
    "train_high = cleanHigh[:5000]\n",
    "test_high = cleanHigh[5500:]\n",
    "dev_high = cleanHigh[5000:5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean(data_list):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    newList = []\n",
    "    for sent in data_list:\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        #sent = re.sub(r'\\d+', '', sent)\n",
    "        \n",
    "\n",
    "        #sent = re.sub('['+string.punctuation+']', '', sent)\n",
    "        sent = sent.lower()\n",
    "        \n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanLow = clean(lowScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanHigh = clean(highScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6057"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanLow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7221"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanHigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sas_data.1', 'w') as f:\n",
    "    f.writelines(cleanHigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sas_data.0', 'w') as f:\n",
    "    f.writelines(cleanLow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_label = [1 for x in cleanHigh]\n",
    "low_label = [0 for x in cleanLow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = high_label + low_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = cleanHigh + cleanLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(all_data, labels))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "shuff_data, shuff_labels = zip(*c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../text_style_transfer/model/data/sas_data', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', shuff_data)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "# with open('../text_style_transfer/model/data/sas.test.1', 'w') as f:\n",
    "#     retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', test_high)\n",
    "#     f.writelines(retokenized)\n",
    "    \n",
    "# with open('../text_style_transfer/model/data/sas.dev.1', 'w') as f:\n",
    "#     retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', dev_high)\n",
    "#     f.writelines(retokenized)\n",
    "\n",
    "with open('../text_style_transfer/model/data/sas_labels', 'w') as f:\n",
    "    f.writelines(map(lambda x: str(x) + '\\n', shuff_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('../language-style-transfer/data/sas.train.1', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', train_high)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.test.1', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', test_high)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.dev.1', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', dev_high)\n",
    "    f.writelines(retokenized)\n",
    "\n",
    "with open('../language-style-transfer/data/sas.train.0', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', train_low)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.test.0', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', test_low)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.dev.0', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', dev_low)\n",
    "    f.writelines(retokenized)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You would need many more pieces of information to replicate the experiment .'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(word_tokenize(highScores[0][0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some additional information that we would need to replicate the experiment is how much vinegar should be placed in each identical container how or what tool to use to measure the mass of the four different samples and how much distilled water to use to rinse the four samples after taking them out of the vinegar'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(dataList[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, vocab_file, emb_file='', dim_emb=0):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.size, self.word2id, self.id2word = pickle.load(f)\n",
    "        self.dim_emb = dim_emb\n",
    "        self.embedding = np.random.random_sample(\n",
    "            (self.size, self.dim_emb)) - 0.5\n",
    "\n",
    "        if emb_file:\n",
    "            print( 'Loading word vectors from', emb_file)\n",
    "            with open(emb_file) as f:\n",
    "                for line in f:\n",
    "                    parts = line.split()\n",
    "                    word = parts[0]\n",
    "                    vec = np.array([float(x) for x in parts[1:]])\n",
    "                    if word in self.word2id:\n",
    "                        self.embedding[self.word2id[word]] = vec\n",
    "\n",
    "        for i in range(self.size):\n",
    "            self.embedding[i] /= LA.norm(self.embedding[i])\n",
    "\n",
    "def build_vocab(data, path, min_occur=5):\n",
    "    word2id = {'<pad>':0, '<go>':1, '<eos>':2, '<unk>':3}\n",
    "    id2word = ['<pad>', '<go>', '<eos>', '<unk>']\n",
    "\n",
    "    words = [word for sent in data for word in sent]\n",
    "    cnt = Counter(words)\n",
    "    for word in cnt:\n",
    "        if cnt[word] >= min_occur:\n",
    "            word2id[word] = len(word2id)\n",
    "            id2word.append(word)\n",
    "    vocab_size = len(word2id)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump((vocab_size, word2id, id2word), f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/data/sas.test.0') as f:\n",
    "    data_0 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/data/sas.test.1') as f:\n",
    "    data_1 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/tmp/sas.test.0.tsf') as f:\n",
    "    test_0 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/tmp/sas.test.1.tsf') as f:\n",
    "    test_1 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_unk(sent):\n",
    "    sent = re.sub(r'<unk>', '', sent)\n",
    "    \n",
    "    return word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_1_strip = list(map(strip_unk, test_1))\n",
    "test_0_strip = list(map(strip_unk, test_0))\n",
    "data_1_strip = list(map(strip_unk, data_1))\n",
    "data_0_strip = list(map(strip_unk, data_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu(sent1, sent2):\n",
    "    \n",
    "    score = nltk.translate.bleu_score.sentence_bleu([sent1], sent2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bleus_1 = []\n",
    "for i in range(len(test_1_strip)):\n",
    "    bleus_1.append(bleu(data_1_strip[i],test_1_strip[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'author',\n",
       "  'first',\n",
       "  'starts',\n",
       "  'out',\n",
       "  'with',\n",
       "  'the',\n",
       "  'introduction',\n",
       "  'which',\n",
       "  'explains',\n",
       "  'what',\n",
       "  'it',\n",
       "  'is',\n",
       "  'about'],\n",
       " ['the',\n",
       "  'author',\n",
       "  'organizes',\n",
       "  'the',\n",
       "  'article',\n",
       "  'in',\n",
       "  'what',\n",
       "  'it',\n",
       "  'is',\n",
       "  'going',\n",
       "  'about',\n",
       "  'space',\n",
       "  'junk'])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_strip[0],test_1_strip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['as',\n",
       "  'the',\n",
       "  'article',\n",
       "  'states',\n",
       "  'in',\n",
       "  'paragraph',\n",
       "  'the',\n",
       "  'snakes',\n",
       "  'have',\n",
       "  'imperiled',\n",
       "  'endangered',\n",
       "  'species',\n",
       "  'in',\n",
       "  'the',\n",
       "  'florida',\n",
       "  'keys'],\n",
       " ['in', 'the', 'and', 'the', 'koala', 'are', 'different', 'of', 'the'])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_strip[1],test_1_strip[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Kids Way Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'get'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d7e200ae5451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.mykidsway.com/essays/a-house-on-fire-2/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"span\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/virtualenvs/cs230/lib/python3.7/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m         raise AttributeError(\n\u001b[0;32m-> 2081\u001b[0;31m             \u001b[0;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'get'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "req = requests.get(\"https://www.mykidsway.com/essays/a-house-on-fire-2/\")\n",
    "soup = BeautifulSoup(req.content, 'html.parser')\n",
    "text = soup.find_all(\"span\", itemprop=\"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for item in text:\n",
    "    data.append(item.get_text().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paginated_links = \"https://www.mykidsway.com/essays/page/{}/\"\n",
    "\n",
    "all_essays = req = requests.get(\"https://www.mykidsway.com/essays/\")\n",
    "essay_html = BeautifulSoup(all_essays.content, 'html.parser')\n",
    "divs = essay_html.find_all(\"div\", class_=\"hovereffect\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_links = []\n",
    "for content in divs:\n",
    "    all_links.append(content.find(\"a\").get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    new_page = paginated_links.format(str(i))\n",
    "    \n",
    "    all_essays = req = requests.get(new_page)\n",
    "    essay_html = BeautifulSoup(all_essays.content, 'html.parser')\n",
    "    divs = essay_html.find_all(\"div\", class_=\"hovereffect\")\n",
    "    \n",
    "\n",
    "    for content in divs:\n",
    "        all_links.append(content.find(\"a\").get(\"href\"))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getText(link):\n",
    "    \n",
    "    req = requests.get(link)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    text = soup.find_all(\"span\", itemprop=\"description\")\n",
    "    \n",
    "    data = []\n",
    "    for item in text:\n",
    "        split_text = item.get_text().split('\\n')\n",
    "        total_len = sum([len(x) for x in split_text])\n",
    "        if total_len > 2000:\n",
    "            print(\"skipping \", link)\n",
    "            continue\n",
    "            \n",
    "        for sentence in split_text:\n",
    "            data.append(sentence)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# all_sentences = []\n",
    "# for link in set(all_links):\n",
    "#     print(link)\n",
    "#     data_list = getText(link)\n",
    "#     for sentence in data_list:\n",
    "#         all_sentences.append(sentence)\n",
    "        \n",
    "#     sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean(data_list):\n",
    "#     regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    newList = []\n",
    "    for sent in data_list:\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        if bool(re.search(r'\\d', sent)):\n",
    "            continue\n",
    "            \n",
    "        sent = sent.lower()\n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanedEssays = clean(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reordered = []\n",
    "for sent in cleanedEssays:\n",
    "    split_sent = sent_tokenize(sent.strip())\n",
    "    for sentence in split_sent:\n",
    "        if (len(word_tokenize(sentence)) > 20) or (len(word_tokenize(sentence)) < 4):\n",
    "            continue\n",
    "        reordered.append(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/KMW_essays.txt\", 'w') as f:\n",
    "    f.writelines(reordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hewlett AES processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "aes_file = \"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/raw/asap-aes/training_set_rel3.tsv\"\n",
    "aes_list = [] \n",
    "with open(aes_file, encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        aes_list.append(line.strip().split('\\t'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aes_df = pd.DataFrame(aes_list[1:], columns=aes_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'applymap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-afa6c78a0c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maes_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rater1_domain1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maes_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rater1_domain1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'applymap'"
     ]
    }
   ],
   "source": [
    "aes_df[[\"rater1_domain1\"]] = aes_df[\"rater1_domain1\"].applymap(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cols = ['rater1_domain1', 'rater2_domain1',\n",
    "       'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2',\n",
    "       'domain2_score']\n",
    "\n",
    "aes_df[num_cols] = aes_df[num_cols].applymap(lambda x: np.nan if (x == \"\") or (x is None) else int(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "12973    0.0\n",
       "12974    0.0\n",
       "12975    0.0\n",
       "12976    0.0\n",
       "12977    0.0\n",
       "Name: domain2_score, Length: 12978, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aes_df[\"domain2_score\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aes_df[\"total_score\"] = (aes_df[\"domain1_score\"] + aes_df[\"domain2_score\"].fillna(aes_df[\"domain1_score\"]))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear local newspaper, I think effects compute...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear @CAPS1 @CAPS2, I believe that using comp...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear, @CAPS1 @CAPS2 @CAPS3 More and more peop...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear Local Newspaper, @CAPS1 I have found tha...</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear @LOCATION1, I know having computers has ...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>\" In most stories mothers and daughters are ei...</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>\" I never understood the meaning laughter is t...</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>\"When you laugh, is @CAPS5 out of habit, or is...</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12976</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>\"                               Trippin' on fe...</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12977</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>\" Many people believe that laughter can improv...</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12978 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id essay_set                                              essay  \\\n",
       "0            1         1  \"Dear local newspaper, I think effects compute...   \n",
       "1            2         1  \"Dear @CAPS1 @CAPS2, I believe that using comp...   \n",
       "2            3         1  \"Dear, @CAPS1 @CAPS2 @CAPS3 More and more peop...   \n",
       "3            4         1  \"Dear Local Newspaper, @CAPS1 I have found tha...   \n",
       "4            5         1  \"Dear @LOCATION1, I know having computers has ...   \n",
       "...        ...       ...                                                ...   \n",
       "12973    21626         8  \" In most stories mothers and daughters are ei...   \n",
       "12974    21628         8  \" I never understood the meaning laughter is t...   \n",
       "12975    21629         8  \"When you laugh, is @CAPS5 out of habit, or is...   \n",
       "12976    21630         8  \"                               Trippin' on fe...   \n",
       "12977    21633         8  \" Many people believe that laughter can improv...   \n",
       "\n",
       "       domain1_score  domain2_score  total_score  \n",
       "0                  8            NaN          8.0  \n",
       "1                  9            NaN          9.0  \n",
       "2                  7            NaN          7.0  \n",
       "3                 10            NaN         10.0  \n",
       "4                  8            NaN          8.0  \n",
       "...              ...            ...          ...  \n",
       "12973             35            NaN         35.0  \n",
       "12974             32            NaN         32.0  \n",
       "12975             40            NaN         40.0  \n",
       "12976             40            NaN         40.0  \n",
       "12977             40            NaN         40.0  \n",
       "\n",
       "[12978 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['essay_id', 'essay_set', 'essay', 'domain1_score', 'domain2_score', 'total_score']\n",
    "aes_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12be78a10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAQyklEQVR4nO3cf4xdZZ3H8fd3OyK/XFrETEjb7HRjo6nbBdkJYDBmoLtQwVj+QMOGaDXd9J/q4qaJwm52yaokmIiIZDVpLLtoGitW3TZoxG7h/uEfFKkgpVSWWajSSaFqS90iug773T/uM+7d7rRzp3Nn7tz7vF/JzZzznOec+3xnbj/n9Lnn3shMJEl1+INuD0CSNHcMfUmqiKEvSRUx9CWpIoa+JFVkoNsDOJULLrggh4aGTnv/V155hXPOOadzA+qSfqkDrGU+6pc6wFom7Nmz5xeZ+abJts3r0B8aGuKxxx477f0bjQYjIyOdG1CX9EsdYC3zUb/UAdYyISJ+erJtTu9IUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JF5vUncmdq79gxPnTLd+b8eQ/ccd2cP6cktcMrfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFWkr9CPibyJiX0Q8FRFfi4gzI2JZROyOiNGI+HpEnFH6vr6sj5btQy3HubW0PxMR18xOSZKkk5ky9CNiMfDXwHBm/gmwALgR+AxwV2a+GTgKrCu7rAOOlva7Sj8iYkXZ723AauCLEbGgs+VIkk6l3emdAeCsiBgAzgYOAVcB28r2+4Dry/Kask7ZvioiorRvzczfZubzwChw6cxLkCS1a2CqDpk5FhGfBX4GvAp8H9gDvJyZ46XbQWBxWV4MvFD2HY+IY8AbS/sjLYdu3ef3ImI9sB5gcHCQRqMx/aqKwbNg48rxqTt22EzGPJnjx493/JjdYi3zT7/UAdbSjilDPyIW0bxKXwa8DHyD5vTMrMjMTcAmgOHh4RwZGTntY92zZTt37p2yxI47cNNIR4/XaDSYye9hPrGW+adf6gBraUc70zt/DjyfmT/PzN8B3wKuABaW6R6AJcBYWR4DlgKU7ecBv2xtn2QfSdIcaCf0fwZcHhFnl7n5VcDTwMPADaXPWmB7Wd5R1inbH8rMLO03lrt7lgHLgUc7U4YkqR3tzOnvjohtwI+AceBxmtMv3wG2RsSnS9vmsstm4KsRMQocoXnHDpm5LyLup3nCGAc2ZOZrHa5HknQKbU14Z+ZtwG0nND/HJHffZOZvgPed5Di3A7dPc4ySpA7xE7mSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVaSv0I2JhRGyLiJ9ExP6IeEdEnB8ROyPi2fJzUekbEfGFiBiNiCcj4pKW46wt/Z+NiLWzVZQkaXLtXunfDXwvM98KXATsB24BdmXmcmBXWQd4N7C8PNYDXwKIiPOB24DLgEuB2yZOFJKkuTFl6EfEecC7gM0AmflfmfkysAa4r3S7D7i+LK8BvpJNjwALI+JC4BpgZ2YeycyjwE5gdUerkSSdUmTmqTtEXAxsAp6meZW/B7gZGMvMhaVPAEczc2FEPADckZk/KNt2AZ8ARoAzM/PTpf3vgVcz87MnPN96mv9DYHBw8M+2bt162sUdPnKMl1497d1P28rF53X0eMePH+fcc8/t6DG7xVrmn36pA6xlwpVXXrknM4cn2zbQxv4DwCXARzNzd0Tczf9O5QCQmRkRpz57tCkzN9E8yTA8PJwjIyOnfax7tmznzr3tlNhZB24a6ejxGo0GM/k9zCfWMv/0Sx1gLe1oZ07/IHAwM3eX9W00TwIvlWkbys/DZfsYsLRl/yWl7WTtkqQ5MmXoZ+aLwAsR8ZbStIrmVM8OYOIOnLXA9rK8A/hguYvncuBYZh4CHgSujohF5Q3cq0ubJGmOtDv38VFgS0ScATwHfJjmCeP+iFgH/BR4f+n7XeBaYBT4delLZh6JiE8BPyz9PpmZRzpShSSpLW2FfmY+AUz2psCqSfomsOEkx7kXuHc6A5QkdY6fyJWkihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SapI26EfEQsi4vGIeKCsL4uI3RExGhFfj4gzSvvry/po2T7UcoxbS/szEXFNp4uRJJ3adK70bwb2t6x/BrgrM98MHAXWlfZ1wNHSflfpR0SsAG4E3gasBr4YEQtmNnxJ0nS0FfoRsQS4DvhyWQ/gKmBb6XIfcH1ZXlPWKdtXlf5rgK2Z+dvMfB4YBS7tRBGSpPYMtNnv88DHgTeU9TcCL2fmeFk/CCwuy4uBFwAyczwijpX+i4FHWo7Zus/vRcR6YD3A4OAgjUaj3Vr+n8GzYOPK8ak7dthMxjyZ48ePd/yY3WIt80+/1AHW0o4pQz8i3gMczsw9ETHS8RGcIDM3AZsAhoeHc2Tk9J/yni3buXNvu+e1zjlw00hHj9doNJjJ72E+sZb5p1/qAGtpRzuJeAXw3oi4FjgT+EPgbmBhRAyUq/0lwFjpPwYsBQ5GxABwHvDLlvYJrftIkubAlHP6mXlrZi7JzCGab8Q+lJk3AQ8DN5Rua4HtZXlHWadsfygzs7TfWO7uWQYsBx7tWCWSpCnNZO7jE8DWiPg08DiwubRvBr4aEaPAEZonCjJzX0TcDzwNjAMbMvO1GTy/JGmaphX6mdkAGmX5OSa5+yYzfwO87yT73w7cPt1BSpI6w0/kSlJFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVWTK0I+IpRHxcEQ8HRH7IuLm0n5+ROyMiGfLz0WlPSLiCxExGhFPRsQlLcdaW/o/GxFrZ68sSdJk2rnSHwc2ZuYK4HJgQ0SsAG4BdmXmcmBXWQd4N7C8PNYDX4LmSQK4DbgMuBS4beJEIUmaG1OGfmYeyswfleX/BPYDi4E1wH2l233A9WV5DfCVbHoEWBgRFwLXADsz80hmHgV2Aqs7Wo0k6ZSmNacfEUPA24HdwGBmHiqbXgQGy/Ji4IWW3Q6WtpO1S5LmyEC7HSPiXOCbwMcy81cR8fttmZkRkZ0YUESspzktxODgII1G47SPNXgWbFw53olhTctMxjyZ48ePd/yY3WIt80+/1AHW0o62Qj8iXkcz8Ldk5rdK80sRcWFmHirTN4dL+xiwtGX3JaVtDBg5ob1x4nNl5iZgE8Dw8HCOjIyc2KVt92zZzp172z6vdc7eVzp6uI0rX+POH0x9zAN3XNfR550NjUaDmfxN55N+qaVf6gBraUc7d+8EsBnYn5mfa9m0A5i4A2ctsL2l/YPlLp7LgWNlGuhB4OqIWFTewL26tEmS5kg7l8FXAB8A9kbEE6Xtb4E7gPsjYh3wU+D9Zdt3gWuBUeDXwIcBMvNIRHwK+GHp98nMPNKRKiRJbZky9DPzB0CcZPOqSfonsOEkx7oXuHc6A5QkdY6fyJWkihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIgPdHoA6Z+iW73TtuQ/ccV3XnltS+7zSl6SKGPqSVBFDX5IqYuhLUkUMfUmqiHfvqCPavXNo48pxPtTBu4y8a0iaHq/0Jakihr4kVcTQl6SKGPqSVBHfyJV6TKe/bmM6b677xnnvM/Sl07R37FhH70SS5oLTO5JUEUNfkioy59M7EbEauBtYAHw5M++Y6zGof3Tz66Q3ruzaU3dNt37fvpfQOXN6pR8RC4B/At4NrAD+MiJWzOUYJKlmc32lfykwmpnPAUTEVmAN8PQcj0NSD/FrPjonMnPuniziBmB1Zv5VWf8AcFlmfqSlz3pgfVl9C/DMDJ7yAuAXM9h/vuiXOsBa5qN+qQOsZcIfZeabJtsw727ZzMxNwKZOHCsiHsvM4U4cq5v6pQ6wlvmoX+oAa2nHXN+9MwYsbVlfUtokSXNgrkP/h8DyiFgWEWcANwI75ngMklStOZ3eyczxiPgI8CDNWzbvzcx9s/iUHZkmmgf6pQ6wlvmoX+oAa5nSnL6RK0nqLj+RK0kVMfQlqSJ9GfoRsToinomI0Yi4pdvjmY6IuDciDkfEUy1t50fEzoh4tvxc1M0xtiMilkbEwxHxdETsi4ibS3sv1nJmRDwaET8utfxjaV8WEbvL6+zr5eaEnhARCyLi8Yh4oKz3ZC0RcSAi9kbEExHxWGnrxdfYwojYFhE/iYj9EfGO2aqj70K/D77q4V+A1Se03QLsyszlwK6yPt+NAxszcwVwObCh/B16sZbfAldl5kXAxcDqiLgc+AxwV2a+GTgKrOviGKfrZmB/y3ov13JlZl7cck97L77G7ga+l5lvBS6i+beZnToys68ewDuAB1vWbwVu7fa4plnDEPBUy/ozwIVl+ULgmW6P8TRq2g78Ra/XApwN/Ai4jOanJQdK+/953c3nB83Px+wCrgIeAKKHazkAXHBCW0+9xoDzgOcpN9bMdh19d6UPLAZeaFk/WNp62WBmHirLLwKD3RzMdEXEEPB2YDc9WkuZDnkCOAzsBP4DeDkzx0uXXnqdfR74OPDfZf2N9G4tCXw/IvaUr3CB3nuNLQN+DvxzmXL7ckScwyzV0Y+h39eyedrvmftsI+Jc4JvAxzLzV63beqmWzHwtMy+meZV8KfDWLg/ptETEe4DDmbmn22PpkHdm5iU0p3M3RMS7Wjf2yGtsALgE+FJmvh14hROmcjpZRz+Gfj9+1cNLEXEhQPl5uMvjaUtEvI5m4G/JzG+V5p6sZUJmvgw8THMKZGFETHzAsVdeZ1cA742IA8BWmlM8d9ObtZCZY+XnYeDbNE/IvfYaOwgczMzdZX0bzZPArNTRj6Hfj1/1sANYW5bX0pwfn9ciIoDNwP7M/FzLpl6s5U0RsbAsn0XzvYn9NMP/htKtJ2rJzFszc0lmDtH8t/FQZt5ED9YSEedExBsmloGrgafosddYZr4IvBARbylNq2h+3fzs1NHtNzFm6Y2Ra4F/pznv+nfdHs80x/414BDwO5pXAOtozrnuAp4F/g04v9vjbKOOd9L87+iTwBPlcW2P1vKnwOOllqeAfyjtfww8CowC3wBe3+2xTrOuEeCBXq2ljPnH5bFv4t96j77GLgYeK6+xfwUWzVYdfg2DJFWkH6d3JEknYehLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0JekivwPft7XnMihsg0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aes_df.total_score.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aes_essays = aes_df.query(\"total_score > 1\")[\"essay\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanAES(dataList):\n",
    "    newList = []\n",
    "    for sent in dataList:\n",
    "        sent = re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)', '', sent)\n",
    "        sent = sent.replace(\"\\\\\",\"\")\n",
    "        sent = sent.replace(\"\\\\'\",\"\")\n",
    "        sent = sent.strip().strip(\"'\").strip('\"')\n",
    "\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        \n",
    "        sent = ' '.join(sent.split())\n",
    "        sent = sent.lower()#.decode('utf8', 'ignore')\n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_aes = cleanAES(aes_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "d.check(\"league\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10826/10826 [01:46<00:00, 101.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# spell = SpellChecker()\n",
    "\n",
    "d_check = lambda sent: map(lambda x: d.check(x), word_tokenize(sent))\n",
    "split_aes = []\n",
    "for essay in tqdm(clean_aes):\n",
    "    split_up = sent_tokenize(essay)\n",
    "    for sent in split_up:\n",
    "        words = word_tokenize(sent)\n",
    "        if not all(list(d_check(sent))):\n",
    "            continue\n",
    "        if len(words) > 30:\n",
    "            continue\n",
    "        if len(words) < 4:\n",
    "            continue\n",
    "        split_aes.append(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# list(map(lambda x: d_check, split_aes[0:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'would',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'if',\n",
       " 'your',\n",
       " 'teenager',\n",
       " 'is',\n",
       " 'always',\n",
       " 'on',\n",
       " 'the',\n",
       " 'phone',\n",
       " 'with',\n",
       " 'friends',\n",
       " '!']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(split_aes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/aes.txt\", 'w') as f:\n",
    "    f.writelines(split_aes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84373"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_aes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hewlett AES  - Keep tokens in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "aes_file = \"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/raw/asap-aes/training_set_rel3.tsv\"\n",
    "aes_list = [] \n",
    "with open(aes_file, encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        aes_list.append(line.strip().split('\\t'))\n",
    "        \n",
    "aes_df = pd.DataFrame(aes_list[1:], columns=aes_list[0])    \n",
    "num_cols = ['rater1_domain1', 'rater2_domain1',\n",
    "       'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2',\n",
    "       'domain2_score']\n",
    "\n",
    "aes_df[num_cols] = aes_df[num_cols].applymap(lambda x: np.nan if (x == \"\") or (x is None) else int(x))\n",
    "aes_df[\"total_score\"] = (aes_df[\"domain1_score\"] + aes_df[\"domain2_score\"].fillna(aes_df[\"domain1_score\"]))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aes_essays = aes_df.query(\"total_score > 1\")[\"essay\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_list = []\n",
    "new_strings = []\n",
    "for string in aes_essays:\n",
    "    matches = re.findall(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[0-9]+)', string)\n",
    "    token_list.extend(list(set(matches)))\n",
    "    \n",
    "    string = string.replace('@' , '')\n",
    "    replacement = {x: \"<\" + re.sub('[0-9]', '', x) + \">\" for x in matches}\n",
    "    for match in matches:\n",
    "        string = string.replace(match, replacement[match])\n",
    "    new_strings.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(set([re.sub('[0-9]', '', x) for x in token_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MONTH',\n",
       " 'NUM',\n",
       " 'DR',\n",
       " 'DATE',\n",
       " 'LOCATION',\n",
       " 'STATE',\n",
       " 'CITY',\n",
       " 'TIME',\n",
       " 'MONEY',\n",
       " 'ORGANIZATION',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'CAPS']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanAES(dataList):\n",
    "    newList = []\n",
    "    for sent in dataList:\n",
    "        sent = re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)', '', sent)\n",
    "        sent = sent.replace(\"\\\\\",\"\")\n",
    "        sent = sent.replace(\"\\\\'\",\"\")\n",
    "        sent = sent.strip().strip(\"'\").strip('\"')\n",
    "\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        \n",
    "        sent = ' '.join(sent.split())\n",
    "        sent = sent.lower()#.decode('utf8', 'ignore')\n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "d_check = lambda sent: map(lambda x: d.check(x), word_tokenize(sent))\n",
    "split_aes = []\n",
    "for essay in tqdm(clean_aes):\n",
    "    split_up = sent_tokenize(essay)\n",
    "    for sent in split_up:\n",
    "        words = word_tokenize(sent)\n",
    "        if not all(list(d_check(sent))):\n",
    "            continue\n",
    "        if len(words) > 30:\n",
    "            continue\n",
    "        if len(words) < 4:\n",
    "            continue\n",
    "        split_aes.append(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sophisticated Writing - Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secondCorpus = [\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/1906/cardinal-1906.txt?sequence=3&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/1658/WoolfWaves-1658.txt?sequence=4&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/0172/moderns-0172.txt?sequence=4&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/3246/3246.txt?sequence=8&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/2042/joywoman-2042.txt?sequence=4&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/3135/3135.txt?sequence=8&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/1711/wiseman-1711.txt?sequence=4&isAllowed=y\",\n",
    "    \"https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/3245/3245.txt?sequence=8&isAllowed=y\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5827/pg5827.txt\", #Russell, The Problems of Philosophy\n",
    "    \"http://www.gutenberg.org/cache/epub/15718/pg15718.txt\", #Bleyer, How To Write Special Feature Articles\n",
    "    \"https://www.gutenberg.org/files/492/492-0.txt\", #Essays in the Art of Writing, by Robert Louis\n",
    "    \"https://www.gutenberg.org/files/37090/37090-0.txt\", #Our Knowledge of the External World as a Field for Scientific Method in Philosoph, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/42580/42580-8.txt\", #Expository Writing, by Mervin James Curl\n",
    "    \"http://www.gutenberg.org/cache/epub/2529/pg2529.txt\", #The Analysis of Mind, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/38280/38280-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/215/215-0.txt\", # London, call of the wild.\n",
    "    \"http://www.gutenberg.org/cache/epub/910/pg910.txt\",\n",
    "    \"https://www.gutenberg.org/files/25110/25110-0.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/32168/pg32168.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/16712/pg16712.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/7514/pg7514.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/18477/pg18477.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5669/pg5669.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5123/pg5123.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/10378/pg10378.txt\",\n",
    "    \"https://www.gutenberg.org/files/140/140-0.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/44082/pg44082.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "firstCorpus = [\n",
    "    \"http://www.gutenberg.org/cache/epub/5827/pg5827.txt\", #Russell, The Problems of Philosophy\n",
    "    \"http://www.gutenberg.org/cache/epub/15718/pg15718.txt\", #Bleyer, How To Write Special Feature Articles\n",
    "    \"https://www.gutenberg.org/files/492/492-0.txt\", #Essays in the Art of Writing, by Robert Louis\n",
    "    \"https://www.gutenberg.org/files/37090/37090-0.txt\", #Our Knowledge of the External World as a Field for Scientific Method in Philosoph, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/42580/42580-8.txt\", #Expository Writing, by Mervin James Curl\n",
    "    \"http://www.gutenberg.org/cache/epub/2529/pg2529.txt\", #The Analysis of Mind, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/38280/38280-0.txt\", #Modern Essays, by Various\n",
    "    \"https://www.gutenberg.org/files/205/205-0.txt\", #Walden, and On The Duty Of Civil Disobedience, by Henry David Thoreau\n",
    "    \"https://www.gutenberg.org/files/1022/1022-0.txt\", #Walking, by Henry David Thoreau\n",
    "    \"http://www.gutenberg.org/cache/epub/34901/pg34901.txt\",\n",
    "    \"https://www.gutenberg.org/files/98/98-0.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/32168/pg32168.txt\",\n",
    "    \"https://www.gutenberg.org/files/766/766-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/1250/1250-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/140/140-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/1400/1400-0.txt\",\n",
    "    \"https://www.gutenberg.org/files/215/215-0.txt\", # London, call of the wild.\n",
    "    \"http://www.gutenberg.org/cache/epub/910/pg910.txt\", #London White Fang\n",
    "    \"https://www.gutenberg.org/files/786/786-0.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/815/pg815.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/10378/pg10378.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5123/pg5123.txt\",\n",
    "    \"http://www.gutenberg.org/cache/epub/5669/pg5669.txt\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filterSentences(sentList):\n",
    "    filteredList = []\n",
    "    for sent in sentList:\n",
    "        sent = sent.replace(\"\\\\\",\"\")\n",
    "        sent = sent.replace(\"\\\\'\",\"\")\n",
    "\n",
    "\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        if bool(re.search(r'\\d', sent)):\n",
    "            continue\n",
    "        if bool(re.search(r\"\\b[A-Z][A-Z]+\\b\", sent)):\n",
    "            continue\n",
    "        if bool(re.search(r'\\\"', sent)):\n",
    "            continue\n",
    "        if bool(re.search(r'_', sent)):\n",
    "            continue\n",
    "\n",
    "        sent = sent.strip()\n",
    "        sent = sent.lower()\n",
    "        sent = ' '.join(sent.split())\n",
    "        filteredList.append(sent + '\\n')\n",
    "\n",
    "    return filteredList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readCorpus(url):\n",
    "    content = requests.get(url).content.decode('ascii', 'ignore')\n",
    "    content_list = sent_tokenize(content.replace('\\r\\n', ' '))\n",
    "    \n",
    "    filtered_list = filterSentences(content_list)\n",
    "    \n",
    "    return filtered_list[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allGuten = []\n",
    "for url in firstCorpus:\n",
    "    allGuten.append(readCorpus(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allGuten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69955"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in allGuten])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allSophs = [y for x in allGuten for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/allsophs.txt\", 'w') as f:\n",
    "    f.writelines(allSophs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/allsophs.txt\", 'r') as f:\n",
    "    allSophs = f.read_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Second Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allSecondCorpus = []\n",
    "for url in secondCorpus:\n",
    "    allSecondCorpus.append(readCorpus(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51291"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in allSecondCorpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allSophs = [y for x in allSecondCorpus for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctSoph = [y for x in allSecondCorpus for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removePunc(sent):\n",
    "    sent = re.sub('['+string.punctuation+']', '', sent)\n",
    "    sent = ' '.join(sent.split())\n",
    "    return(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allSophs = list(map(removePunc, punctSoph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/KMW_essays.txt\", 'r') as f:\n",
    "    kmw = f.readlines()\n",
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/aes.txt\", 'r') as f:\n",
    "    split_aes = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allnaive = kmw + split_aes[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allnaive = list(map(removePunc, allnaive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but people can also contribute at individual level by planting trees and preventing denudation of vegetable cover\\n',\n",
       " 'unemployment is a great curse\\n',\n",
       " 'it brings to the surface the evil side of human nature\\n',\n",
       " 'government has to keep an eye on the unemployed and to keep their numbers down\\n',\n",
       " 'it has opened employment exchanges in all cities\\n',\n",
       " 'the unemployed are required to register with these\\n',\n",
       " 'whenever there is a vacancy suitable to their qualifications they are considered for it\\n',\n",
       " 'number of unemployed is rising fast\\n',\n",
       " 'there is always a great rush at the windows\\n',\n",
       " 'the clerks take their own time in verifying and registering\\n',\n",
       " 'restless people can be seen standing helplessly in queues for long hours\\n',\n",
       " 'they have to join the other queue and are again at the fag end\\n',\n",
       " 'quite often by the time their turn come it is time for closing\\n',\n",
       " 'they have to go jump to come another day\\n',\n",
       " 'though there are so many people yet no cheerful voices are heard\\n',\n",
       " 'one could hear people grumbling or cursing their fate\\n',\n",
       " 'despair is writ large on their faces\\n',\n",
       " 'have they not visited the place for months after months or even years\\n',\n",
       " 'the only people who make hay there are the hawkers\\n',\n",
       " 'for them the long queues the waiting people are a source of income\\n',\n",
       " 'they can be seen selling their eatables and tempting the hungry disgruntled people\\n',\n",
       " 'childhood is very impressionable age\\n',\n",
       " 'one incident of my childhood is indelibly printed in my mind\\n',\n",
       " 'i was eleven years old\\n',\n",
       " 'i had just joined middle school\\n',\n",
       " 'they treated me as an outsider\\n',\n",
       " 'to add insult to injury they often befooled me and made me a target of their jokes\\n',\n",
       " 'no wonder i got into the habit of visiting the nearby orchard for solitude and incidentally stealing fruit\\n',\n",
       " 'once i got into the orchard and climbed up a huge mango tree\\n',\n",
       " 'after eating to the fill and filling my pockets with mangoes\\n']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allnaive[100:130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "keepshort = []\n",
    "for sent in allSophs:\n",
    "    words = len(word_tokenize(sent))\n",
    "    if words <= 30:\n",
    "        keepshort.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37211"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keepshort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keepnum = 33408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split1 = int(keepnum * 0.05)\n",
    "remain = split1 % 128\n",
    "split1 += remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1676\n",
      "31732\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "random.shuffle(keepshort)\n",
    "selectedSophs = keepshort[0:keepnum]\n",
    "sophstrain = selectedSophs[split1:]\n",
    "sophtest = selectedSophs[0:split1]\n",
    "print(len(sophtest))\n",
    "print(len(sophstrain))\n",
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/soph_test_3.txt\", 'w') as f:\n",
    "    f.writelines(sophtest)\n",
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/soph_train_3.txt\", 'w') as f:\n",
    "    f.writelines(sophstrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33408"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sophtest) + len(sophstrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_aes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/KMW_essays.txt\", 'r') as f:\n",
    "    kmw = f.readlines()\n",
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/aes.txt\", 'r') as f:\n",
    "    split_aes = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allnaive = kmw + split_aes[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53759"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allnaive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allnaive = [x for x in allnaive if len(x) > 20]\n",
    "naiveshort = []\n",
    "for sent in allnaive:\n",
    "    words = len(word_tokenize(sent))\n",
    "    if words <= 25:\n",
    "        naiveshort.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50895"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(naiveshort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['since computers have become such a popular thing i think that in the future typing or using a computer will be in the\\n',\n",
       " 'do you have picture files laying your house\\n',\n",
       " 'with computers you store all and files neatly\\n',\n",
       " 'how your might you can bark those easy to find everything becomes all that are locate\\n',\n",
       " 'even doctors are computers your medical needs and give time\\n',\n",
       " 'computers can give organize\\n',\n",
       " 'furthermore computers continue to help instead with store say keep friends i can talk with friends or family far away\\n',\n",
       " 'you can learn about new things like people and places far away\\n',\n",
       " 'kids try to learn people talk like websites things that they are not able to and over they can see are on\\n',\n",
       " 'dear a world without computers\\n',\n",
       " 'i picture it as a better place\\n',\n",
       " 'computers be helpful for work and business but young children are losing time to exercise enjoy nature and interact with family and friends\\n',\n",
       " 'what do you prefer having computers or no\\n',\n",
       " 'in my opinion people would be better off without them\\n',\n",
       " 'lack of exercise is brought up countless times in society\\n',\n",
       " 'if the number of computers used in each household increase the amount of obese people in will as well\\n',\n",
       " 'if computers take away exercise than health issues will more often and at a younger age\\n',\n",
       " 'older citizens will be a big target for a heart attack\\n',\n",
       " 'no one should lose exercise over playing virtual games and instant messaging on the internet\\n',\n",
       " 'less people get a chance to enjoy nature when they are on the computer all day\\n']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naiveshort[15500:15520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1676\n",
      "31732\n",
      "33408\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "random.shuffle(allnaive)\n",
    "selectedNaive = allnaive[0:keepnum]\n",
    "naivetrain = selectedNaive[split1:]\n",
    "naivetest = selectedNaive[0:split1]\n",
    "print(len(naivetest))\n",
    "print(len(naivetrain))\n",
    "print(len(naivetrain) + len(naivetest))\n",
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/naive_test_3.txt\", 'w') as f:\n",
    "    f.writelines(naivetest)\n",
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/naive_train_3.txt\", 'w') as f:\n",
    "    f.writelines(naivetrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
