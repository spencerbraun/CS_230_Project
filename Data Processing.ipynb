{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/asap-aes/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-104fb36c8054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0messayList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0messay_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/asap-aes/train.tsv'"
     ]
    }
   ],
   "source": [
    "path = \"data/raw/\"\n",
    "essay_file = path+ \"asap-aes/train.tsv\"\n",
    "\n",
    "i=0\n",
    "\n",
    "essayList = []\n",
    "with open(essay_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"Id\"):\n",
    "            continue\n",
    "        splitLine = line.strip().split(\"\\t\")\n",
    "        scores = map(int, splitLine[2:4])\n",
    "        avgscore = sum(scores)/2\n",
    "        essay = sent_tokenize(splitLine[4])\n",
    "        for sent in essay:\n",
    "            essayList.append([sent, avgscore])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/raw/\"\n",
    "short_answer_file = path+ \"asap-sas/train.tsv\"\n",
    "\n",
    "i=0\n",
    "\n",
    "dataList = []\n",
    "with open(short_answer_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"Id\"):\n",
    "            continue\n",
    "        splitLine = line.strip().split(\"\\t\")\n",
    "        scores = map(int, splitLine[2:4])\n",
    "        avgscore = sum(scores)/2\n",
    "        essay = sent_tokenize(splitLine[4])\n",
    "        for sent in essay:\n",
    "            dataList.append([sent, avgscore])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highScores = []\n",
    "lowScores = []\n",
    "for data in dataList:\n",
    "    if len(word_tokenize(data[0])) > 20:\n",
    "        continue\n",
    "    if data[1] >= 2:\n",
    "        highScores.append(data[0])\n",
    "    if data[1] ==0:\n",
    "        lowScores.append(data[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('sas_high.csv', 'w', newline='\\n') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(highScores)\n",
    "    \n",
    "\n",
    "with open('sas_low.csv', 'w', newline='\\n') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(lowScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 11279,\n",
       "         0.0: 11879,\n",
       "         2.0: 11669,\n",
       "         0.5: 2255,\n",
       "         3.0: 2247,\n",
       "         2.5: 510,\n",
       "         1.5: 3216})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([x[1] for x in dataList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "random.shuffle(cleanLow)\n",
    "train_low = cleanLow[:5000]\n",
    "test_low = cleanLow[5500:]\n",
    "dev_low = cleanLow[5000:5500]\n",
    "\n",
    "random.shuffle(cleanHigh)\n",
    "train_high = cleanHigh[:5000]\n",
    "test_high = cleanHigh[5500:]\n",
    "dev_high = cleanHigh[5000:5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean(data_list):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    newList = []\n",
    "    for sent in data_list:\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        #sent = re.sub(r'\\d+', '', sent)\n",
    "        \n",
    "\n",
    "        #sent = re.sub('['+string.punctuation+']', '', sent)\n",
    "        sent = sent.lower()\n",
    "        \n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanLow = clean(lowScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanHigh = clean(highScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6057"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanLow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7221"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanHigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sas_data.1', 'w') as f:\n",
    "    f.writelines(cleanHigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sas_data.0', 'w') as f:\n",
    "    f.writelines(cleanLow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_label = [1 for x in cleanHigh]\n",
    "low_label = [0 for x in cleanLow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = high_label + low_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = cleanHigh + cleanLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(all_data, labels))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "shuff_data, shuff_labels = zip(*c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../text_style_transfer/model/data/sas_data', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', shuff_data)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "# with open('../text_style_transfer/model/data/sas.test.1', 'w') as f:\n",
    "#     retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', test_high)\n",
    "#     f.writelines(retokenized)\n",
    "    \n",
    "# with open('../text_style_transfer/model/data/sas.dev.1', 'w') as f:\n",
    "#     retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', dev_high)\n",
    "#     f.writelines(retokenized)\n",
    "\n",
    "with open('../text_style_transfer/model/data/sas_labels', 'w') as f:\n",
    "    f.writelines(map(lambda x: str(x) + '\\n', shuff_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('../language-style-transfer/data/sas.train.1', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', train_high)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.test.1', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', test_high)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.dev.1', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', dev_high)\n",
    "    f.writelines(retokenized)\n",
    "\n",
    "with open('../language-style-transfer/data/sas.train.0', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', train_low)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.test.0', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', test_low)\n",
    "    f.writelines(retokenized)\n",
    "    \n",
    "with open('../language-style-transfer/data/sas.dev.0', 'w') as f:\n",
    "    retokenized = map(lambda x: ' '.join(word_tokenize(x)) + '\\n', dev_low)\n",
    "    f.writelines(retokenized)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You would need many more pieces of information to replicate the experiment .'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(word_tokenize(highScores[0][0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some additional information that we would need to replicate the experiment is how much vinegar should be placed in each identical container how or what tool to use to measure the mass of the four different samples and how much distilled water to use to rinse the four samples after taking them out of the vinegar'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(dataList[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, vocab_file, emb_file='', dim_emb=0):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.size, self.word2id, self.id2word = pickle.load(f)\n",
    "        self.dim_emb = dim_emb\n",
    "        self.embedding = np.random.random_sample(\n",
    "            (self.size, self.dim_emb)) - 0.5\n",
    "\n",
    "        if emb_file:\n",
    "            print( 'Loading word vectors from', emb_file)\n",
    "            with open(emb_file) as f:\n",
    "                for line in f:\n",
    "                    parts = line.split()\n",
    "                    word = parts[0]\n",
    "                    vec = np.array([float(x) for x in parts[1:]])\n",
    "                    if word in self.word2id:\n",
    "                        self.embedding[self.word2id[word]] = vec\n",
    "\n",
    "        for i in range(self.size):\n",
    "            self.embedding[i] /= LA.norm(self.embedding[i])\n",
    "\n",
    "def build_vocab(data, path, min_occur=5):\n",
    "    word2id = {'<pad>':0, '<go>':1, '<eos>':2, '<unk>':3}\n",
    "    id2word = ['<pad>', '<go>', '<eos>', '<unk>']\n",
    "\n",
    "    words = [word for sent in data for word in sent]\n",
    "    cnt = Counter(words)\n",
    "    for word in cnt:\n",
    "        if cnt[word] >= min_occur:\n",
    "            word2id[word] = len(word2id)\n",
    "            id2word.append(word)\n",
    "    vocab_size = len(word2id)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump((vocab_size, word2id, id2word), f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/data/sas.test.0') as f:\n",
    "    data_0 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/data/sas.test.1') as f:\n",
    "    data_1 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/tmp/sas.test.0.tsf') as f:\n",
    "    test_0 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../language-style-transfer/tmp/sas.test.1.tsf') as f:\n",
    "    test_1 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_unk(sent):\n",
    "    sent = re.sub(r'<unk>', '', sent)\n",
    "    \n",
    "    return word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_1_strip = list(map(strip_unk, test_1))\n",
    "test_0_strip = list(map(strip_unk, test_0))\n",
    "data_1_strip = list(map(strip_unk, data_1))\n",
    "data_0_strip = list(map(strip_unk, data_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu(sent1, sent2):\n",
    "    \n",
    "    score = nltk.translate.bleu_score.sentence_bleu([sent1], sent2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bleus_1 = []\n",
    "for i in range(len(test_1_strip)):\n",
    "    bleus_1.append(bleu(data_1_strip[i],test_1_strip[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'author',\n",
       "  'first',\n",
       "  'starts',\n",
       "  'out',\n",
       "  'with',\n",
       "  'the',\n",
       "  'introduction',\n",
       "  'which',\n",
       "  'explains',\n",
       "  'what',\n",
       "  'it',\n",
       "  'is',\n",
       "  'about'],\n",
       " ['the',\n",
       "  'author',\n",
       "  'organizes',\n",
       "  'the',\n",
       "  'article',\n",
       "  'in',\n",
       "  'what',\n",
       "  'it',\n",
       "  'is',\n",
       "  'going',\n",
       "  'about',\n",
       "  'space',\n",
       "  'junk'])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_strip[0],test_1_strip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['as',\n",
       "  'the',\n",
       "  'article',\n",
       "  'states',\n",
       "  'in',\n",
       "  'paragraph',\n",
       "  'the',\n",
       "  'snakes',\n",
       "  'have',\n",
       "  'imperiled',\n",
       "  'endangered',\n",
       "  'species',\n",
       "  'in',\n",
       "  'the',\n",
       "  'florida',\n",
       "  'keys'],\n",
       " ['in', 'the', 'and', 'the', 'koala', 'are', 'different', 'of', 'the'])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_strip[1],test_1_strip[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Kids Way Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'get'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d7e200ae5451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.mykidsway.com/essays/a-house-on-fire-2/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"span\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/virtualenvs/cs230/lib/python3.7/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m         raise AttributeError(\n\u001b[0;32m-> 2081\u001b[0;31m             \u001b[0;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'get'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "req = requests.get(\"https://www.mykidsway.com/essays/a-house-on-fire-2/\")\n",
    "soup = BeautifulSoup(req.content, 'html.parser')\n",
    "text = soup.find_all(\"span\", itemprop=\"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for item in text:\n",
    "    data.append(item.get_text().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paginated_links = \"https://www.mykidsway.com/essays/page/{}/\"\n",
    "\n",
    "all_essays = req = requests.get(\"https://www.mykidsway.com/essays/\")\n",
    "essay_html = BeautifulSoup(all_essays.content, 'html.parser')\n",
    "divs = essay_html.find_all(\"div\", class_=\"hovereffect\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_links = []\n",
    "for content in divs:\n",
    "    all_links.append(content.find(\"a\").get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    new_page = paginated_links.format(str(i))\n",
    "    \n",
    "    all_essays = req = requests.get(new_page)\n",
    "    essay_html = BeautifulSoup(all_essays.content, 'html.parser')\n",
    "    divs = essay_html.find_all(\"div\", class_=\"hovereffect\")\n",
    "    \n",
    "\n",
    "    for content in divs:\n",
    "        all_links.append(content.find(\"a\").get(\"href\"))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getText(link):\n",
    "    \n",
    "    req = requests.get(link)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    text = soup.find_all(\"span\", itemprop=\"description\")\n",
    "    \n",
    "    data = []\n",
    "    for item in text:\n",
    "        split_text = item.get_text().split('\\n')\n",
    "        total_len = sum([len(x) for x in split_text])\n",
    "        if total_len > 2000:\n",
    "            print(\"skipping \", link)\n",
    "            continue\n",
    "            \n",
    "        for sentence in split_text:\n",
    "            data.append(sentence)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# all_sentences = []\n",
    "# for link in set(all_links):\n",
    "#     print(link)\n",
    "#     data_list = getText(link)\n",
    "#     for sentence in data_list:\n",
    "#         all_sentences.append(sentence)\n",
    "        \n",
    "#     sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean(data_list):\n",
    "#     regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    newList = []\n",
    "    for sent in data_list:\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        if bool(re.search(r'\\d', sent)):\n",
    "            continue\n",
    "            \n",
    "        sent = sent.lower()\n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanedEssays = clean(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reordered = []\n",
    "for sent in cleanedEssays:\n",
    "    split_sent = sent_tokenize(sent.strip())\n",
    "    for sentence in split_sent:\n",
    "        if (len(word_tokenize(sentence)) > 20) or (len(word_tokenize(sentence)) < 4):\n",
    "            continue\n",
    "        reordered.append(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/KMW_essays.txt\", 'w') as f:\n",
    "    f.writelines(reordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hewlett AES processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "aes_file = \"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/raw/asap-aes/training_set_rel3.tsv\"\n",
    "aes_list = [] \n",
    "with open(aes_file, encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        aes_list.append(line.strip().split('\\t'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aes_df = pd.DataFrame(aes_list[1:], columns=aes_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'applymap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-afa6c78a0c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maes_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rater1_domain1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maes_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rater1_domain1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'applymap'"
     ]
    }
   ],
   "source": [
    "aes_df[[\"rater1_domain1\"]] = aes_df[\"rater1_domain1\"].applymap(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cols = ['rater1_domain1', 'rater2_domain1',\n",
    "       'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2',\n",
    "       'domain2_score']\n",
    "\n",
    "aes_df[num_cols] = aes_df[num_cols].applymap(lambda x: np.nan if (x == \"\") or (x is None) else int(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "12973    0.0\n",
       "12974    0.0\n",
       "12975    0.0\n",
       "12976    0.0\n",
       "12977    0.0\n",
       "Name: domain2_score, Length: 12978, dtype: float64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aes_df[\"domain2_score\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aes_df[\"total_score\"] = (aes_df[\"domain1_score\"] + aes_df[\"domain2_score\"].fillna(aes_df[\"domain1_score\"]))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear local newspaper, I think effects compute...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear @CAPS1 @CAPS2, I believe that using comp...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear, @CAPS1 @CAPS2 @CAPS3 More and more peop...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear Local Newspaper, @CAPS1 I have found tha...</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Dear @LOCATION1, I know having computers has ...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>\" In most stories mothers and daughters are ei...</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>\" I never understood the meaning laughter is t...</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>\"When you laugh, is @CAPS5 out of habit, or is...</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12976</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>\"                               Trippin' on fe...</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12977</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>\" Many people believe that laughter can improv...</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12978 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id essay_set                                              essay  \\\n",
       "0            1         1  \"Dear local newspaper, I think effects compute...   \n",
       "1            2         1  \"Dear @CAPS1 @CAPS2, I believe that using comp...   \n",
       "2            3         1  \"Dear, @CAPS1 @CAPS2 @CAPS3 More and more peop...   \n",
       "3            4         1  \"Dear Local Newspaper, @CAPS1 I have found tha...   \n",
       "4            5         1  \"Dear @LOCATION1, I know having computers has ...   \n",
       "...        ...       ...                                                ...   \n",
       "12973    21626         8  \" In most stories mothers and daughters are ei...   \n",
       "12974    21628         8  \" I never understood the meaning laughter is t...   \n",
       "12975    21629         8  \"When you laugh, is @CAPS5 out of habit, or is...   \n",
       "12976    21630         8  \"                               Trippin' on fe...   \n",
       "12977    21633         8  \" Many people believe that laughter can improv...   \n",
       "\n",
       "       domain1_score  domain2_score  total_score  \n",
       "0                  8            NaN          8.0  \n",
       "1                  9            NaN          9.0  \n",
       "2                  7            NaN          7.0  \n",
       "3                 10            NaN         10.0  \n",
       "4                  8            NaN          8.0  \n",
       "...              ...            ...          ...  \n",
       "12973             35            NaN         35.0  \n",
       "12974             32            NaN         32.0  \n",
       "12975             40            NaN         40.0  \n",
       "12976             40            NaN         40.0  \n",
       "12977             40            NaN         40.0  \n",
       "\n",
       "[12978 rows x 6 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['essay_id', 'essay_set', 'essay', 'domain1_score', 'domain2_score', 'total_score']\n",
    "aes_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x122043210>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAQyklEQVR4nO3cf4xdZZ3H8fd3OyK/XFrETEjb7HRjo6nbBdkJYDBmoLtQwVj+QMOGaDXd9J/q4qaJwm52yaokmIiIZDVpLLtoGitW3TZoxG7h/uEfFKkgpVSWWajSSaFqS90iug773T/uM+7d7rRzp3Nn7tz7vF/JzZzznOec+3xnbj/n9Lnn3shMJEl1+INuD0CSNHcMfUmqiKEvSRUx9CWpIoa+JFVkoNsDOJULLrggh4aGTnv/V155hXPOOadzA+qSfqkDrGU+6pc6wFom7Nmz5xeZ+abJts3r0B8aGuKxxx477f0bjQYjIyOdG1CX9EsdYC3zUb/UAdYyISJ+erJtTu9IUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JF5vUncmdq79gxPnTLd+b8eQ/ccd2cP6cktcMrfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFWkr9CPibyJiX0Q8FRFfi4gzI2JZROyOiNGI+HpEnFH6vr6sj5btQy3HubW0PxMR18xOSZKkk5ky9CNiMfDXwHBm/gmwALgR+AxwV2a+GTgKrCu7rAOOlva7Sj8iYkXZ723AauCLEbGgs+VIkk6l3emdAeCsiBgAzgYOAVcB28r2+4Dry/Kask7ZvioiorRvzczfZubzwChw6cxLkCS1a2CqDpk5FhGfBX4GvAp8H9gDvJyZ46XbQWBxWV4MvFD2HY+IY8AbS/sjLYdu3ef3ImI9sB5gcHCQRqMx/aqKwbNg48rxqTt22EzGPJnjx493/JjdYi3zT7/UAdbSjilDPyIW0bxKXwa8DHyD5vTMrMjMTcAmgOHh4RwZGTntY92zZTt37p2yxI47cNNIR4/XaDSYye9hPrGW+adf6gBraUc70zt/DjyfmT/PzN8B3wKuABaW6R6AJcBYWR4DlgKU7ecBv2xtn2QfSdIcaCf0fwZcHhFnl7n5VcDTwMPADaXPWmB7Wd5R1inbH8rMLO03lrt7lgHLgUc7U4YkqR3tzOnvjohtwI+AceBxmtMv3wG2RsSnS9vmsstm4KsRMQocoXnHDpm5LyLup3nCGAc2ZOZrHa5HknQKbU14Z+ZtwG0nND/HJHffZOZvgPed5Di3A7dPc4ySpA7xE7mSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVaSv0I2JhRGyLiJ9ExP6IeEdEnB8ROyPi2fJzUekbEfGFiBiNiCcj4pKW46wt/Z+NiLWzVZQkaXLtXunfDXwvM98KXATsB24BdmXmcmBXWQd4N7C8PNYDXwKIiPOB24DLgEuB2yZOFJKkuTFl6EfEecC7gM0AmflfmfkysAa4r3S7D7i+LK8BvpJNjwALI+JC4BpgZ2YeycyjwE5gdUerkSSdUmTmqTtEXAxsAp6meZW/B7gZGMvMhaVPAEczc2FEPADckZk/KNt2AZ8ARoAzM/PTpf3vgVcz87MnPN96mv9DYHBw8M+2bt162sUdPnKMl1497d1P28rF53X0eMePH+fcc8/t6DG7xVrmn36pA6xlwpVXXrknM4cn2zbQxv4DwCXARzNzd0Tczf9O5QCQmRkRpz57tCkzN9E8yTA8PJwjIyOnfax7tmznzr3tlNhZB24a6ejxGo0GM/k9zCfWMv/0Sx1gLe1oZ07/IHAwM3eX9W00TwIvlWkbys/DZfsYsLRl/yWl7WTtkqQ5MmXoZ+aLwAsR8ZbStIrmVM8OYOIOnLXA9rK8A/hguYvncuBYZh4CHgSujohF5Q3cq0ubJGmOtDv38VFgS0ScATwHfJjmCeP+iFgH/BR4f+n7XeBaYBT4delLZh6JiE8BPyz9PpmZRzpShSSpLW2FfmY+AUz2psCqSfomsOEkx7kXuHc6A5QkdY6fyJWkihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SapI26EfEQsi4vGIeKCsL4uI3RExGhFfj4gzSvvry/po2T7UcoxbS/szEXFNp4uRJJ3adK70bwb2t6x/BrgrM98MHAXWlfZ1wNHSflfpR0SsAG4E3gasBr4YEQtmNnxJ0nS0FfoRsQS4DvhyWQ/gKmBb6XIfcH1ZXlPWKdtXlf5rgK2Z+dvMfB4YBS7tRBGSpPYMtNnv88DHgTeU9TcCL2fmeFk/CCwuy4uBFwAyczwijpX+i4FHWo7Zus/vRcR6YD3A4OAgjUaj3Vr+n8GzYOPK8ak7dthMxjyZ48ePd/yY3WIt80+/1AHW0o4pQz8i3gMczsw9ETHS8RGcIDM3AZsAhoeHc2Tk9J/yni3buXNvu+e1zjlw00hHj9doNJjJ72E+sZb5p1/qAGtpRzuJeAXw3oi4FjgT+EPgbmBhRAyUq/0lwFjpPwYsBQ5GxABwHvDLlvYJrftIkubAlHP6mXlrZi7JzCGab8Q+lJk3AQ8DN5Rua4HtZXlHWadsfygzs7TfWO7uWQYsBx7tWCWSpCnNZO7jE8DWiPg08DiwubRvBr4aEaPAEZonCjJzX0TcDzwNjAMbMvO1GTy/JGmaphX6mdkAGmX5OSa5+yYzfwO87yT73w7cPt1BSpI6w0/kSlJFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVWTK0I+IpRHxcEQ8HRH7IuLm0n5+ROyMiGfLz0WlPSLiCxExGhFPRsQlLcdaW/o/GxFrZ68sSdJk2rnSHwc2ZuYK4HJgQ0SsAG4BdmXmcmBXWQd4N7C8PNYDX4LmSQK4DbgMuBS4beJEIUmaG1OGfmYeyswfleX/BPYDi4E1wH2l233A9WV5DfCVbHoEWBgRFwLXADsz80hmHgV2Aqs7Wo0k6ZSmNacfEUPA24HdwGBmHiqbXgQGy/Ji4IWW3Q6WtpO1S5LmyEC7HSPiXOCbwMcy81cR8fttmZkRkZ0YUESspzktxODgII1G47SPNXgWbFw53olhTctMxjyZ48ePd/yY3WIt80+/1AHW0o62Qj8iXkcz8Ldk5rdK80sRcWFmHirTN4dL+xiwtGX3JaVtDBg5ob1x4nNl5iZgE8Dw8HCOjIyc2KVt92zZzp172z6vdc7eVzp6uI0rX+POH0x9zAN3XNfR550NjUaDmfxN55N+qaVf6gBraUc7d+8EsBnYn5mfa9m0A5i4A2ctsL2l/YPlLp7LgWNlGuhB4OqIWFTewL26tEmS5kg7l8FXAB8A9kbEE6Xtb4E7gPsjYh3wU+D9Zdt3gWuBUeDXwIcBMvNIRHwK+GHp98nMPNKRKiRJbZky9DPzB0CcZPOqSfonsOEkx7oXuHc6A5QkdY6fyJWkihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIgPdHoA6Z+iW73TtuQ/ccV3XnltS+7zSl6SKGPqSVBFDX5IqYuhLUkUMfUmqiHfvqCPavXNo48pxPtTBu4y8a0iaHq/0Jakihr4kVcTQl6SKGPqSVBHfyJV6TKe/bmM6b677xnnvM/Sl07R37FhH70SS5oLTO5JUEUNfkioy59M7EbEauBtYAHw5M++Y6zGof3Tz66Q3ruzaU3dNt37fvpfQOXN6pR8RC4B/At4NrAD+MiJWzOUYJKlmc32lfykwmpnPAUTEVmAN8PQcj0NSD/FrPjonMnPuniziBmB1Zv5VWf8AcFlmfqSlz3pgfVl9C/DMDJ7yAuAXM9h/vuiXOsBa5qN+qQOsZcIfZeabJtsw727ZzMxNwKZOHCsiHsvM4U4cq5v6pQ6wlvmoX+oAa2nHXN+9MwYsbVlfUtokSXNgrkP/h8DyiFgWEWcANwI75ngMklStOZ3eyczxiPgI8CDNWzbvzcx9s/iUHZkmmgf6pQ6wlvmoX+oAa5nSnL6RK0nqLj+RK0kVMfQlqSJ9GfoRsToinomI0Yi4pdvjmY6IuDciDkfEUy1t50fEzoh4tvxc1M0xtiMilkbEwxHxdETsi4ibS3sv1nJmRDwaET8utfxjaV8WEbvL6+zr5eaEnhARCyLi8Yh4oKz3ZC0RcSAi9kbEExHxWGnrxdfYwojYFhE/iYj9EfGO2aqj70K/D77q4V+A1Se03QLsyszlwK6yPt+NAxszcwVwObCh/B16sZbfAldl5kXAxcDqiLgc+AxwV2a+GTgKrOviGKfrZmB/y3ov13JlZl7cck97L77G7ga+l5lvBS6i+beZnToys68ewDuAB1vWbwVu7fa4plnDEPBUy/ozwIVl+ULgmW6P8TRq2g78Ra/XApwN/Ai4jOanJQdK+/953c3nB83Px+wCrgIeAKKHazkAXHBCW0+9xoDzgOcpN9bMdh19d6UPLAZeaFk/WNp62WBmHirLLwKD3RzMdEXEEPB2YDc9WkuZDnkCOAzsBP4DeDkzx0uXXnqdfR74OPDfZf2N9G4tCXw/IvaUr3CB3nuNLQN+DvxzmXL7ckScwyzV0Y+h39eyedrvmftsI+Jc4JvAxzLzV63beqmWzHwtMy+meZV8KfDWLg/ptETEe4DDmbmn22PpkHdm5iU0p3M3RMS7Wjf2yGtsALgE+FJmvh14hROmcjpZRz+Gfj9+1cNLEXEhQPl5uMvjaUtEvI5m4G/JzG+V5p6sZUJmvgw8THMKZGFETHzAsVdeZ1cA742IA8BWmlM8d9ObtZCZY+XnYeDbNE/IvfYaOwgczMzdZX0bzZPArNTRj6Hfj1/1sANYW5bX0pwfn9ciIoDNwP7M/FzLpl6s5U0RsbAsn0XzvYn9NMP/htKtJ2rJzFszc0lmDtH8t/FQZt5ED9YSEedExBsmloGrgafosddYZr4IvBARbylNq2h+3fzs1NHtNzFm6Y2Ra4F/pznv+nfdHs80x/414BDwO5pXAOtozrnuAp4F/g04v9vjbKOOd9L87+iTwBPlcW2P1vKnwOOllqeAfyjtfww8CowC3wBe3+2xTrOuEeCBXq2ljPnH5bFv4t96j77GLgYeK6+xfwUWzVYdfg2DJFWkH6d3JEknYehLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0JekivwPft7XnMihsg0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aes_df.total_score.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aes_essays = aes_df.query(\"total_score > 1\")[\"essay\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanAES(dataList):\n",
    "    newList = []\n",
    "    for sent in dataList:\n",
    "        sent = re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)', '', sent)\n",
    "        sent = sent.replace(\"\\\\\",\"\")\n",
    "        sent = sent.replace(\"\\\\'\",\"\")\n",
    "        sent = sent.strip().strip(\"'\").strip('\"')\n",
    "\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        sent = sent.lower()#.decode('utf8', 'ignore')\n",
    "        newList.append(sent + '\\n')\n",
    "    \n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_aes = cleanAES(aes_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# spell = SpellChecker()\n",
    "\n",
    "split_aes = []\n",
    "for essay in clean_aes:\n",
    "    split_up = sent_tokenize(essay)\n",
    "    for sent in split_up:\n",
    "        words = word_tokenize(sent)\n",
    "#         misspelled = spell.unknown(words)\n",
    "#         new_items = [spell.correction(x) if x in misspelled else x for x in words]\n",
    "        if len(words) > 30:\n",
    "            continue\n",
    "        if len(words) < 4:\n",
    "            continue\n",
    "        split_aes.append(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/aes.txt\", 'w') as f:\n",
    "    f.writelines(split_aes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sophisticated Writing - Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gutenLinks = [\n",
    "    \"http://www.gutenberg.org/cache/epub/5827/pg5827.txt\", #Russell, The Problems of Philosophy\n",
    "    \"http://www.gutenberg.org/cache/epub/15718/pg15718.txt\", #Bleyer, How To Write Special Feature Articles\n",
    "    \"https://www.gutenberg.org/files/492/492-0.txt\", #Essays in the Art of Writing, by Robert Louis\n",
    "    \"https://www.gutenberg.org/files/37090/37090-0.txt\", #Our Knowledge of the External World as a Field for Scientific Method in Philosoph, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/42580/42580-8.txt\", #Expository Writing, by Mervin James Curl\n",
    "    \"http://www.gutenberg.org/cache/epub/2529/pg2529.txt\", #The Analysis of Mind, by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/38280/38280-0.txt\", #Modern Essays, by Various\n",
    "    \"https://www.gutenberg.org/files/205/205-0.txt\" #Walden, and On The Duty Of Civil Disobedience, by Henry David Thoreau\n",
    "    \"https://www.gutenberg.org/files/1022/1022-0.txt\" #Walking, by Henry David Thoreau\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filterSentences(sentList):\n",
    "    filteredList = []\n",
    "    for sent in sentList:\n",
    "        sent = sent.replace(\"\\\\\",\"\")\n",
    "        sent = sent.replace(\"\\\\'\",\"\")\n",
    "\n",
    "\n",
    "        if len(sent) < 40:\n",
    "            continue\n",
    "        if '^' in sent:\n",
    "            continue\n",
    "        if bool(re.search(r'\\d', sent)):\n",
    "            continue\n",
    "        if bool(re.search(r\"\\b[A-Z][A-Z]+\\b\", sent)):\n",
    "            continue\n",
    "        if bool(re.search(r'\\\"', sent)):\n",
    "            continue\n",
    "        if bool(re.search(r'_', sent)):\n",
    "            continue\n",
    "\n",
    "        sent = sent.strip()\n",
    "        sent = sent.lower()\n",
    "        sent = ' '.join(sent.split())\n",
    "        filteredList.append(sent + '\\n')\n",
    "\n",
    "    return filteredList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readGutenberg(url):\n",
    "    content = requests.get(url).content.decode('ascii', 'ignore')\n",
    "    content_list = sent_tokenize(content.replace('\\r\\n', ' '))\n",
    "    \n",
    "    filtered_list = filterSentences(content_list)\n",
    "    \n",
    "    return filtered_list[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allGuten = []\n",
    "for url in gutenLinks:\n",
    "    allGuten.append(readGutenberg(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allGuten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15693"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in allGuten])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allSophs = [y for x in allGuten for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/CS_230_Project/data/processed/allsophs.txt\", 'w') as f:\n",
    "    f.writelines(allSophs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
