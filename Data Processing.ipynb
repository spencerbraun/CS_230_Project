{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/spencerbraun/Documents/Stanford/CS 230 - Deep Learning/Project/\"\n",
    "short_answer_file = path+ \"asap-sas/train.tsv\"\n",
    "\n",
    "i=0\n",
    "\n",
    "dataList = []\n",
    "with open(short_answer_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"Id\"):\n",
    "            continue\n",
    "        splitLine = line.strip().split(\"\\t\")\n",
    "        scores = map(int, splitLine[2:4])\n",
    "        avgscore = sum(scores)/2\n",
    "        essay = sent_tokenize(splitLine[4])\n",
    "        dataList.append([essay, avgscore])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some additional information that we would need to replicate the experiment is how much vinegar should be placed in each identical container how or what tool to use to measure the mass of the four different samples and how much distilled water to use to rinse the four samples after taking them out of the vinegar'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(dataList[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, vocab_file, emb_file='', dim_emb=0):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.size, self.word2id, self.id2word = pickle.load(f)\n",
    "        self.dim_emb = dim_emb\n",
    "        self.embedding = np.random.random_sample(\n",
    "            (self.size, self.dim_emb)) - 0.5\n",
    "\n",
    "        if emb_file:\n",
    "            print( 'Loading word vectors from', emb_file)\n",
    "            with open(emb_file) as f:\n",
    "                for line in f:\n",
    "                    parts = line.split()\n",
    "                    word = parts[0]\n",
    "                    vec = np.array([float(x) for x in parts[1:]])\n",
    "                    if word in self.word2id:\n",
    "                        self.embedding[self.word2id[word]] = vec\n",
    "\n",
    "        for i in range(self.size):\n",
    "            self.embedding[i] /= LA.norm(self.embedding[i])\n",
    "\n",
    "def build_vocab(data, path, min_occur=5):\n",
    "    word2id = {'<pad>':0, '<go>':1, '<eos>':2, '<unk>':3}\n",
    "    id2word = ['<pad>', '<go>', '<eos>', '<unk>']\n",
    "\n",
    "    words = [word for sent in data for word in sent]\n",
    "    cnt = Counter(words)\n",
    "    for word in cnt:\n",
    "        if cnt[word] >= min_occur:\n",
    "            word2id[word] = len(word2id)\n",
    "            id2word.append(word)\n",
    "    vocab_size = len(word2id)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump((vocab_size, word2id, id2word), f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
